{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "(X_train,y_train),(X_test,y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from keras.datasets import boston_housing\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = boston_housing.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.5393920141694561"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm([[1,2,3],[4,5,6]])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "fig = plt.figure(figsize=(20,20))\n",
    "for i in range(6):\n",
    "    ax = fig.add_subplot(1,6,i+1,xticks=[],yticks=[])\n",
    "    ax.imshow(X_train[i],cmap=\"gray\")\n",
    "    ax.set_title(str(y_train[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype(\"float32\")/255\n",
    "X_test = X_test.astype(\"float32\")/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OnehotLabel\n",
    "import pandas as pd\n",
    "y_train_df = pd.get_dummies(y_train)\n",
    "y_train=np.array(pd.get_dummies(y_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Reshaping X_train\n",
    "X_train_matrix = X_train.reshape(60000,-1)\n",
    "X_train_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_34 (Dense)             (None, 20)                280       \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 301\n",
      "Trainable params: 301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Model in Lecture\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout,Dense\n",
    "model1 = Sequential()\n",
    "model1.add(Dense(20,input_dim=x_train.shape[1],activation=\"relu\"))\n",
    "#model1.add(Dense(6,activation=\"relu\"))\n",
    "#model1.add(Dense(1,activation=\"relu\"))\n",
    "#model1.add(Dense(10,activation=\"relu\"))\n",
    "model1.add(Dense(1))\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_flatten = X_test.reshape(X_test.shape[0],-1)\n",
    "y_test_one_hot = np.array(pd.get_dummies(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-0e22b5d85df5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mean_squared_error'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"adam\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m#model1.compile(loss=\"mean_squared_error\",optimizer=\"sgd\",metrics=[\"accuracy\"])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#score_1 = model1.evaluate(x_test,y_test)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#100*score_1[1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model1' is not defined"
     ]
    }
   ],
   "source": [
    "from keras import optimizers\n",
    "model1.compile(loss='mean_squared_error', optimizer=\"adam\")\n",
    "#model1.compile(loss=\"mean_squared_error\",optimizer=\"sgd\",metrics=[\"accuracy\"])\n",
    "#score_1 = model1.evaluate(x_test,y_test)\n",
    "#100*score_1[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 323 samples, validate on 81 samples\n",
      "Epoch 1/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 4791.8574\n",
      "Epoch 00001: val_loss improved from inf to 3051.34692, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 1s 2ms/step - loss: 4668.1861 - val_loss: 3051.3469\n",
      "Epoch 2/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 4045.8965\n",
      "Epoch 00002: val_loss improved from 3051.34692 to 2212.76758, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 147us/step - loss: 3647.7879 - val_loss: 2212.7676\n",
      "Epoch 3/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 3197.0300\n",
      "Epoch 00003: val_loss improved from 2212.76758 to 1598.29102, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 147us/step - loss: 2795.9917 - val_loss: 1598.2910\n",
      "Epoch 4/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 1609.3440\n",
      "Epoch 00004: val_loss improved from 1598.29102 to 1180.34485, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 141us/step - loss: 2155.3319 - val_loss: 1180.3448\n",
      "Epoch 5/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 1954.8530\n",
      "Epoch 00005: val_loss improved from 1180.34485 to 921.80499, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 150us/step - loss: 1732.9157 - val_loss: 921.8050\n",
      "Epoch 6/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 923.2890\n",
      "Epoch 00006: val_loss improved from 921.80499 to 778.58795, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 169us/step - loss: 1428.0752 - val_loss: 778.5880\n",
      "Epoch 7/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 1395.4740\n",
      "Epoch 00007: val_loss improved from 778.58795 to 704.34198, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 164us/step - loss: 1270.2735 - val_loss: 704.3420\n",
      "Epoch 8/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 1142.4644\n",
      "Epoch 00008: val_loss improved from 704.34198 to 662.66931, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 177us/step - loss: 1154.0669 - val_loss: 662.6693\n",
      "Epoch 9/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 1160.8351\n",
      "Epoch 00009: val_loss improved from 662.66931 to 628.54114, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 164us/step - loss: 1079.0089 - val_loss: 628.5411\n",
      "Epoch 10/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 766.4631\n",
      "Epoch 00010: val_loss improved from 628.54114 to 588.83868, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 161us/step - loss: 998.8765 - val_loss: 588.8387\n",
      "Epoch 11/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 760.4055\n",
      "Epoch 00011: val_loss improved from 588.83868 to 541.89740, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 183us/step - loss: 933.2702 - val_loss: 541.8974\n",
      "Epoch 12/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 837.7850\n",
      "Epoch 00012: val_loss improved from 541.89740 to 491.66455, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 148us/step - loss: 859.3001 - val_loss: 491.6646\n",
      "Epoch 13/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 769.5403\n",
      "Epoch 00013: val_loss improved from 491.66455 to 442.99542, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 164us/step - loss: 788.2473 - val_loss: 442.9954\n",
      "Epoch 14/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 663.8713\n",
      "Epoch 00014: val_loss improved from 442.99542 to 400.43347, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 157us/step - loss: 722.7848 - val_loss: 400.4335\n",
      "Epoch 15/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 486.1408\n",
      "Epoch 00015: val_loss improved from 400.43347 to 367.39136, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 142us/step - loss: 660.3022 - val_loss: 367.3914\n",
      "Epoch 16/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 898.0662\n",
      "Epoch 00016: val_loss improved from 367.39136 to 342.67850, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 134us/step - loss: 618.6799 - val_loss: 342.6785\n",
      "Epoch 17/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 865.5008\n",
      "Epoch 00017: val_loss improved from 342.67850 to 325.21179, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 149us/step - loss: 580.0987 - val_loss: 325.2118\n",
      "Epoch 18/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 427.6117\n",
      "Epoch 00018: val_loss improved from 325.21179 to 312.92505, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 164us/step - loss: 543.4479 - val_loss: 312.9250\n",
      "Epoch 19/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 518.7274\n",
      "Epoch 00019: val_loss improved from 312.92505 to 302.46942, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 145us/step - loss: 518.0127 - val_loss: 302.4694\n",
      "Epoch 20/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 459.1644\n",
      "Epoch 00020: val_loss improved from 302.46942 to 292.93158, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 160us/step - loss: 494.2470 - val_loss: 292.9316\n",
      "Epoch 21/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 507.6626\n",
      "Epoch 00021: val_loss improved from 292.93158 to 283.33163, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 153us/step - loss: 473.2641 - val_loss: 283.3316\n",
      "Epoch 22/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 483.1255\n",
      "Epoch 00022: val_loss improved from 283.33163 to 273.70947, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 159us/step - loss: 451.7622 - val_loss: 273.7095\n",
      "Epoch 23/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 506.4307\n",
      "Epoch 00023: val_loss improved from 273.70947 to 264.37469, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 141us/step - loss: 432.2099 - val_loss: 264.3747\n",
      "Epoch 24/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 412.3498\n",
      "Epoch 00024: val_loss improved from 264.37469 to 255.38663, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 153us/step - loss: 412.0118 - val_loss: 255.3866\n",
      "Epoch 25/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 502.8481\n",
      "Epoch 00025: val_loss improved from 255.38663 to 246.61307, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 155us/step - loss: 396.1040 - val_loss: 246.6131\n",
      "Epoch 26/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 466.4313\n",
      "Epoch 00026: val_loss improved from 246.61307 to 238.25842, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 134us/step - loss: 379.1531 - val_loss: 238.2584\n",
      "Epoch 27/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 340.0890\n",
      "Epoch 00027: val_loss improved from 238.25842 to 230.19905, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 145us/step - loss: 362.8948 - val_loss: 230.1991\n",
      "Epoch 28/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 429.9160\n",
      "Epoch 00028: val_loss improved from 230.19905 to 222.23216, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 161us/step - loss: 348.6748 - val_loss: 222.2322\n",
      "Epoch 29/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 350.9949\n",
      "Epoch 00029: val_loss improved from 222.23216 to 214.51720, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 220us/step - loss: 333.9874 - val_loss: 214.5172\n",
      "Epoch 30/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 374.4118\n",
      "Epoch 00030: val_loss improved from 214.51720 to 206.97545, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 164us/step - loss: 320.7088 - val_loss: 206.9754\n",
      "Epoch 31/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 305.0776\n",
      "Epoch 00031: val_loss improved from 206.97545 to 199.59911, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 158us/step - loss: 307.8605 - val_loss: 199.5991\n",
      "Epoch 32/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 242.1504\n",
      "Epoch 00032: val_loss improved from 199.59911 to 192.64288, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 126us/step - loss: 295.1103 - val_loss: 192.6429\n",
      "Epoch 33/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 269.8819\n",
      "Epoch 00033: val_loss improved from 192.64288 to 186.06665, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 129us/step - loss: 283.3483 - val_loss: 186.0667\n",
      "Epoch 34/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 265.6841\n",
      "Epoch 00034: val_loss improved from 186.06665 to 179.72681, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 125us/step - loss: 272.0634 - val_loss: 179.7268\n",
      "Epoch 35/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 282.1287\n",
      "Epoch 00035: val_loss improved from 179.72681 to 173.50716, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 122us/step - loss: 261.6580 - val_loss: 173.5072\n",
      "Epoch 36/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 215.3121\n",
      "Epoch 00036: val_loss improved from 173.50716 to 167.60486, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 134us/step - loss: 250.5568 - val_loss: 167.6049\n",
      "Epoch 37/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 280.1606\n",
      "Epoch 00037: val_loss improved from 167.60486 to 161.85107, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 142us/step - loss: 241.1704 - val_loss: 161.8511\n",
      "Epoch 38/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 240.9215\n",
      "Epoch 00038: val_loss improved from 161.85107 to 156.49452, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 145us/step - loss: 231.4868 - val_loss: 156.4945\n",
      "Epoch 39/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 227.2938\n",
      "Epoch 00039: val_loss improved from 156.49452 to 151.39018, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 157us/step - loss: 222.3162 - val_loss: 151.3902\n",
      "Epoch 40/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 159.1290\n",
      "Epoch 00040: val_loss improved from 151.39018 to 146.65811, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 135us/step - loss: 212.7104 - val_loss: 146.6581\n",
      "Epoch 41/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 203.4579\n",
      "Epoch 00041: val_loss improved from 146.65811 to 142.26514, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 169us/step - loss: 204.4674 - val_loss: 142.2651\n",
      "Epoch 42/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 163.8351\n",
      "Epoch 00042: val_loss improved from 142.26514 to 137.94058, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 148us/step - loss: 195.9639 - val_loss: 137.9406\n",
      "Epoch 43/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 242.5043\n",
      "Epoch 00043: val_loss improved from 137.94058 to 133.71428, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 162us/step - loss: 189.3092 - val_loss: 133.7143\n",
      "Epoch 44/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 145.7938\n",
      "Epoch 00044: val_loss improved from 133.71428 to 129.80185, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 188us/step - loss: 180.5908 - val_loss: 129.8018\n",
      "Epoch 45/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 162.2556\n",
      "Epoch 00045: val_loss improved from 129.80185 to 125.93993, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 190us/step - loss: 173.6102 - val_loss: 125.9399\n",
      "Epoch 46/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 192.6573\n",
      "Epoch 00046: val_loss improved from 125.93993 to 122.31021, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 159us/step - loss: 167.4164 - val_loss: 122.3102\n",
      "Epoch 47/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 144.4007\n",
      "Epoch 00047: val_loss improved from 122.31021 to 119.02117, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 126us/step - loss: 160.2786 - val_loss: 119.0212\n",
      "Epoch 48/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 176.4644\n",
      "Epoch 00048: val_loss improved from 119.02117 to 115.99638, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 151us/step - loss: 154.4327 - val_loss: 115.9964\n",
      "Epoch 49/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 144.9240\n",
      "Epoch 00049: val_loss improved from 115.99638 to 113.20856, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 142us/step - loss: 148.9045 - val_loss: 113.2086\n",
      "Epoch 50/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 138.7209\n",
      "Epoch 00050: val_loss improved from 113.20856 to 110.65704, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 171us/step - loss: 143.1276 - val_loss: 110.6570\n",
      "Epoch 51/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 143.7502\n",
      "Epoch 00051: val_loss improved from 110.65704 to 108.27233, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 208us/step - loss: 137.8948 - val_loss: 108.2723\n",
      "Epoch 52/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 114.8423\n",
      "Epoch 00052: val_loss improved from 108.27233 to 106.08103, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 180us/step - loss: 133.1531 - val_loss: 106.0810\n",
      "Epoch 53/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 146.9484\n",
      "Epoch 00053: val_loss improved from 106.08103 to 103.83990, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 196us/step - loss: 128.8425 - val_loss: 103.8399\n",
      "Epoch 54/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 143.9307\n",
      "Epoch 00054: val_loss improved from 103.83990 to 101.79274, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 193us/step - loss: 124.3949 - val_loss: 101.7927\n",
      "Epoch 55/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 141.4609\n",
      "Epoch 00055: val_loss improved from 101.79274 to 99.81054, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 159us/step - loss: 120.4838 - val_loss: 99.8105\n",
      "Epoch 56/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 106.8124\n",
      "Epoch 00056: val_loss improved from 99.81054 to 98.09985, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 168us/step - loss: 116.3023 - val_loss: 98.0999\n",
      "Epoch 57/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 112.4277\n",
      "Epoch 00057: val_loss improved from 98.09985 to 96.32892, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 175us/step - loss: 112.8075 - val_loss: 96.3289\n",
      "Epoch 58/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 117.4348\n",
      "Epoch 00058: val_loss improved from 96.32892 to 94.72807, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 222us/step - loss: 109.3245 - val_loss: 94.7281\n",
      "Epoch 59/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 114.9387\n",
      "Epoch 00059: val_loss improved from 94.72807 to 93.34276, saving model to ./best_weight_boston.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "323/323 [==============================] - 0s 166us/step - loss: 106.1189 - val_loss: 93.3428\n",
      "Epoch 60/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 124.9605\n",
      "Epoch 00060: val_loss improved from 93.34276 to 91.90302, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 151us/step - loss: 103.3511 - val_loss: 91.9030\n",
      "Epoch 61/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 100.8513\n",
      "Epoch 00061: val_loss improved from 91.90302 to 90.69523, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 153us/step - loss: 100.2759 - val_loss: 90.6952\n",
      "Epoch 62/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 80.3053\n",
      "Epoch 00062: val_loss improved from 90.69523 to 89.58968, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 192us/step - loss: 97.5928 - val_loss: 89.5897\n",
      "Epoch 63/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 101.5234\n",
      "Epoch 00063: val_loss improved from 89.58968 to 88.57913, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 204us/step - loss: 95.3815 - val_loss: 88.5791\n",
      "Epoch 64/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 87.6888\n",
      "Epoch 00064: val_loss improved from 88.57913 to 87.61768, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 179us/step - loss: 93.1069 - val_loss: 87.6177\n",
      "Epoch 65/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 74.6829\n",
      "Epoch 00065: val_loss improved from 87.61768 to 86.69964, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 181us/step - loss: 91.0175 - val_loss: 86.6996\n",
      "Epoch 66/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 77.2613\n",
      "Epoch 00066: val_loss improved from 86.69964 to 85.87247, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 149us/step - loss: 88.9570 - val_loss: 85.8725\n",
      "Epoch 67/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 81.3970\n",
      "Epoch 00067: val_loss improved from 85.87247 to 84.98103, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 137us/step - loss: 87.1439 - val_loss: 84.9810\n",
      "Epoch 68/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 93.1026\n",
      "Epoch 00068: val_loss improved from 84.98103 to 84.02757, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 126us/step - loss: 85.4147 - val_loss: 84.0276\n",
      "Epoch 69/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 68.5667\n",
      "Epoch 00069: val_loss improved from 84.02757 to 83.13053, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 149us/step - loss: 83.7054 - val_loss: 83.1305\n",
      "Epoch 70/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 108.8092\n",
      "Epoch 00070: val_loss improved from 83.13053 to 82.30360, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 128us/step - loss: 82.4188 - val_loss: 82.3036\n",
      "Epoch 71/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 70.3927\n",
      "Epoch 00071: val_loss improved from 82.30360 to 81.80060, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 172us/step - loss: 80.6085 - val_loss: 81.8006\n",
      "Epoch 72/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 78.0428\n",
      "Epoch 00072: val_loss improved from 81.80060 to 81.26911, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 196us/step - loss: 79.3131 - val_loss: 81.2691\n",
      "Epoch 73/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 75.4706\n",
      "Epoch 00073: val_loss improved from 81.26911 to 80.81602, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 180us/step - loss: 78.0628 - val_loss: 80.8160\n",
      "Epoch 74/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 79.6479\n",
      "Epoch 00074: val_loss improved from 80.81602 to 80.24850, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 173us/step - loss: 76.7351 - val_loss: 80.2485\n",
      "Epoch 75/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 67.2230\n",
      "Epoch 00075: val_loss improved from 80.24850 to 79.57196, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 151us/step - loss: 75.5623 - val_loss: 79.5720\n",
      "Epoch 76/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 65.4219\n",
      "Epoch 00076: val_loss improved from 79.57196 to 78.95841, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 160us/step - loss: 74.4398 - val_loss: 78.9584\n",
      "Epoch 77/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 75.6333\n",
      "Epoch 00077: val_loss improved from 78.95841 to 78.30044, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 134us/step - loss: 73.3860 - val_loss: 78.3004\n",
      "Epoch 78/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 70.2419\n",
      "Epoch 00078: val_loss improved from 78.30044 to 77.77390, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 138us/step - loss: 72.4804 - val_loss: 77.7739\n",
      "Epoch 79/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 74.6589\n",
      "Epoch 00079: val_loss improved from 77.77390 to 77.30625, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 147us/step - loss: 71.5358 - val_loss: 77.3063\n",
      "Epoch 80/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 76.8722\n",
      "Epoch 00080: val_loss improved from 77.30625 to 76.98649, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 177us/step - loss: 70.6340 - val_loss: 76.9865\n",
      "Epoch 81/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 72.7310\n",
      "Epoch 00081: val_loss improved from 76.98649 to 76.74413, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 230us/step - loss: 69.7973 - val_loss: 76.7441\n",
      "Epoch 82/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 68.8189\n",
      "Epoch 00082: val_loss improved from 76.74413 to 76.45657, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 202us/step - loss: 69.0852 - val_loss: 76.4566\n",
      "Epoch 83/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 58.8619\n",
      "Epoch 00083: val_loss improved from 76.45657 to 76.09095, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 171us/step - loss: 68.3307 - val_loss: 76.0910\n",
      "Epoch 84/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 68.0433\n",
      "Epoch 00084: val_loss improved from 76.09095 to 75.61866, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 156us/step - loss: 67.5936 - val_loss: 75.6187\n",
      "Epoch 85/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 50.0280\n",
      "Epoch 00085: val_loss improved from 75.61866 to 75.31020, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 163us/step - loss: 66.9539 - val_loss: 75.3102\n",
      "Epoch 86/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 81.4753\n",
      "Epoch 00086: val_loss improved from 75.31020 to 74.78423, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 146us/step - loss: 66.3161 - val_loss: 74.7842\n",
      "Epoch 87/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 53.1834\n",
      "Epoch 00087: val_loss improved from 74.78423 to 74.37919, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 157us/step - loss: 65.6997 - val_loss: 74.3792\n",
      "Epoch 88/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 57.0371\n",
      "Epoch 00088: val_loss improved from 74.37919 to 73.95006, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 188us/step - loss: 65.0743 - val_loss: 73.9501\n",
      "Epoch 89/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 60.1892\n",
      "Epoch 00089: val_loss improved from 73.95006 to 73.50053, saving model to ./best_weight_boston.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "323/323 [==============================] - 0s 178us/step - loss: 64.5601 - val_loss: 73.5005\n",
      "Epoch 90/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 61.4104\n",
      "Epoch 00090: val_loss improved from 73.50053 to 72.99984, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 152us/step - loss: 64.0103 - val_loss: 72.9998\n",
      "Epoch 91/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 61.8191\n",
      "Epoch 00091: val_loss improved from 72.99984 to 72.66866, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 167us/step - loss: 63.5331 - val_loss: 72.6687\n",
      "Epoch 92/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 47.8043\n",
      "Epoch 00092: val_loss improved from 72.66866 to 72.26913, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 156us/step - loss: 63.0550 - val_loss: 72.2691\n",
      "Epoch 93/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 56.5960\n",
      "Epoch 00093: val_loss improved from 72.26913 to 71.90356, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 158us/step - loss: 62.5918 - val_loss: 71.9036\n",
      "Epoch 94/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 55.6757\n",
      "Epoch 00094: val_loss improved from 71.90356 to 71.53704, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 150us/step - loss: 62.2199 - val_loss: 71.5370\n",
      "Epoch 95/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 57.0884\n",
      "Epoch 00095: val_loss improved from 71.53704 to 71.23779, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 137us/step - loss: 61.8055 - val_loss: 71.2378\n",
      "Epoch 96/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 59.5596\n",
      "Epoch 00096: val_loss improved from 71.23779 to 71.05775, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 151us/step - loss: 61.4048 - val_loss: 71.0577\n",
      "Epoch 97/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 76.1394\n",
      "Epoch 00097: val_loss improved from 71.05775 to 70.90553, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 192us/step - loss: 61.0301 - val_loss: 70.9055\n",
      "Epoch 98/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 38.6778\n",
      "Epoch 00098: val_loss improved from 70.90553 to 70.75729, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 243us/step - loss: 60.7294 - val_loss: 70.7573\n",
      "Epoch 99/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 77.9007\n",
      "Epoch 00099: val_loss improved from 70.75729 to 70.45116, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 181us/step - loss: 60.3693 - val_loss: 70.4512\n",
      "Epoch 100/100\n",
      "128/323 [==========>...................] - ETA: 0s - loss: 58.8941\n",
      "Epoch 00100: val_loss improved from 70.45116 to 70.21668, saving model to ./best_weight_boston.hdf5\n",
      "323/323 [==============================] - 0s 179us/step - loss: 60.0698 - val_loss: 70.2167\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "checkpointer = ModelCheckpoint(filepath=\"./best_weight_boston.hdf5\",verbose=1,\n",
    "                              save_best_only=True)\n",
    "hist = model1.fit(x_train,y_train,batch_size=128,epochs=100,validation_split=0.2,\n",
    "                 callbacks=[checkpointer],verbose=1,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-12e1aa78a262>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mscore_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mscore_1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model1' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "score_1 = model1.evaluate(x_test,y_test)\n",
    "100*score_1[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92.09"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "92.09"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(y, Y):\n",
    "    return -np.sum(np.sum(Y*np.log(y.T)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nn import NeuralNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 51.0% ... Training loss: 2.425 ... Validation loss: 2.425"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/Udacity_dl_nanodegree/first-neural-network/nn.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0ma3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0mz4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight_3_4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias_4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m         \u001b[0ma4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m         \u001b[0mz5\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight_4_5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias_5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Udacity_dl_nanodegree/first-neural-network/nn.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;31m# Note: in Python, you can define a function with a lambda expression,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m# as shown below.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdivide\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;31m# Replace 0 with your sigmoid calculation.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import sys\n",
    "\n",
    "####################\n",
    "### Set the hyperparameters in you myanswers.py file ###\n",
    "####################\n",
    "\n",
    "from nn import iterations, learning_rate,hidden_nodes, output_nodes\n",
    "\n",
    "delta = []\n",
    "N_i = X_train_matrix.shape[1]\n",
    "network = NeuralNetwork(N_i, hidden_nodes, output_nodes, learning_rate)\n",
    "\n",
    "losses = {'train':[], 'validation':[]}\n",
    "for ii in range(iterations):\n",
    "    # Go through a random batch of 128 records from the training data set\n",
    "    batch = np.random.choice(y_train_df.index, size=128)\n",
    "    X, y = X_train_matrix[batch], y_train[batch]\n",
    "    \n",
    "    network.train(X, y)\n",
    "    \n",
    "    \n",
    "    # Printing out the training progress\n",
    "    train_loss = MSE(network.run(X_train_matrix).T, y_train)/X_train_matrix.shape[0]\n",
    "    val_loss = MSE(network.run(X_test_flatten).T, y_test_one_hot)/X_test_flatten.shape[0]\n",
    "    sys.stdout.write(\"\\rProgress: {:2.1f}\".format(100 * ii/float(iterations)) \\\n",
    "                     + \"% ... Training loss: \" + str(train_loss)[:5] \\\n",
    "                     + \" ... Validation loss: \" + str(val_loss)[:5])\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    losses['train'].append(train_loss)\n",
    "    losses['validation'].append(val_loss)\n",
    "output=network.run((X_test_flatten))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'output' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-361168fea4bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;36m100\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'output' is not defined"
     ]
    }
   ],
   "source": [
    "100*(sum(np.argmax(output,axis=1) == y_test)/y_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 10)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nS72mGZvwNeUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9c63a4e2b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses['train'], label='Training loss')\n",
    "plt.plot(losses['validation'], label='Validation loss')\n",
    "plt.legend()\n",
    "_ = plt.ylim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print(y.shape)\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  7.78113223e-20,   2.06115362e-09,   9.99999998e-01],\n",
       "       [  4.29549702e-61,   1.23394576e-04,   9.99876605e-01],\n",
       "       [  1.12533283e-07,   1.67014200e-05,   9.99983186e-01],\n",
       "       [  8.80797078e-01,   1.19202922e-01,   2.24045327e-13]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b=np.array([[1,25,45],[-78,52,61],[75,80,91],[34,32,5]])\n",
    "c=np.exp(b)/np.sum(np.exp(b),axis=1,keepdims=True)\n",
    "np.argma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 2, 0])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(c,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.49342711720049e+19"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " 2.71828183e+00+  7.20048993e+10+   3.49342711e+19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 1)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.exp(b),axis=1,keepdims=True).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from learn_rate import NeuralNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 90.0% ... Training loss: 4.657 ... Validation loss: 4.639CPU times: user 2min 57s, sys: 42 s, total: 3min 39s\n",
      "Wall time: 1min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import sys\n",
    "\n",
    "####################\n",
    "### Set the hyperparameters in you myanswers.py file ###\n",
    "####################\n",
    "\n",
    "from learn_rate import iterations, learning_rate,hidden_nodes, output_nodes\n",
    "\n",
    "delta = []\n",
    "N_i = X_train_matrix.shape[1]\n",
    "network = NeuralNetwork(N_i, hidden_nodes, output_nodes, learning_rate)\n",
    "\n",
    "losses = {'train':[], 'validation':[]}\n",
    "for ii in range(iterations):\n",
    "    # Go through a random batch of 128 records from the training data set\n",
    "    batch = np.random.choice(y_train_df.index, size=128)\n",
    "    X, y = X_train_matrix[batch], y_train[batch]\n",
    "    \n",
    "    network.train(X, y)\n",
    "    \n",
    "    \n",
    "    # Printing out the training progress\n",
    "    train_loss = MSE(network.run(X_train_matrix).T, y_train)/X_train_matrix.shape[0]\n",
    "    val_loss = MSE(network.run(X_test_flatten).T, y_test_one_hot)/X_test_flatten.shape[0]\n",
    "    sys.stdout.write(\"\\rProgress: {:2.1f}\".format(100 * ii/float(iterations)) \\\n",
    "                     + \"% ... Training loss: \" + str(train_loss)[:5] \\\n",
    "                     + \" ... Validation loss: \" + str(val_loss)[:5])\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    losses['train'].append(train_loss)\n",
    "    losses['validation'].append(val_loss)\n",
    "output=network.run((X_test_flatten))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.23"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "100*(sum(np.argmax(output,axis=1) == y_test)/y_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\npEIo0EVEKsT/B3a3mw6bgxEjAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fdcb4757518>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses['train'], label='Training loss')\n",
    "plt.plot(losses['validation'], label='Validation loss')\n",
    "plt.legend()\n",
    "_ = plt.ylim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
