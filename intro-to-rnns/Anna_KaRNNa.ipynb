{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anna KaRNNa\n",
    "\n",
    "In this notebook, we'll build a character-wise RNN trained on Anna Karenina, one of my all-time favorite books. It'll be able to generate new text based on the text from the book.\n",
    "\n",
    "This network is based off of Andrej Karpathy's [post on RNNs](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) and [implementation in Torch](https://github.com/karpathy/char-rnn). Also, some information [here at r2rt](http://r2rt.com/recurrent-neural-networks-in-tensorflow-ii.html) and from [Sherjil Ozair](https://github.com/sherjilozair/char-rnn-tensorflow) on GitHub. Below is the general architecture of the character-wise RNN.\n",
    "\n",
    "<img src=\"assets/charseq.jpeg\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import namedtuple\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll load the text file and convert it into integers for our network to use. Here I'm creating a couple dictionaries to convert the characters to and from integers. Encoding the characters as integers makes it easier to use as input in the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('anna.txt', 'r') as f:\n",
    "    text=f.read()\n",
    "vocab = sorted(set(text))\n",
    "vocab_to_int = {c: i for i, c in enumerate(vocab)}\n",
    "int_to_vocab = dict(enumerate(vocab))\n",
    "encoded = np.array([vocab_to_int[c] for c in text], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretty cool way of encoding!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('anna.txt','r') as f:\n",
    "    text = f.read()\n",
    "vocab = sorted(set(text))\n",
    "vocab_to_int = {c:i for i,c in enumerate(vocab)}\n",
    "int_to_vocab = dict(enumerate(vocab))\n",
    "encoded = np.array([vocab_to_int[c] for c in text],dtype=np.int32)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([31, 64, 57, ..., 75, 13,  0], dtype=int32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check out the first 100 characters, make sure everything is peachy. According to the [American Book Review](http://americanbookreview.org/100bestlines.asp), this is the 6th best first line of a book ever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chapter 1\\n\\n\\nHappy families are all alike; every unhappy family is unhappy in its own\\nway.\\n\\nEverythin'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can see the characters encoded as integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([31, 64, 57, 72, 76, 61, 74,  1, 16,  0,  0,  0, 36, 57, 72, 72, 81,\n",
       "        1, 62, 57, 69, 65, 68, 65, 61, 75,  1, 57, 74, 61,  1, 57, 68, 68,\n",
       "        1, 57, 68, 65, 67, 61, 26,  1, 61, 78, 61, 74, 81,  1, 77, 70, 64,\n",
       "       57, 72, 72, 81,  1, 62, 57, 69, 65, 68, 81,  1, 65, 75,  1, 77, 70,\n",
       "       64, 57, 72, 72, 81,  1, 65, 70,  1, 65, 76, 75,  1, 71, 79, 70,  0,\n",
       "       79, 57, 81, 13,  0,  0, 33, 78, 61, 74, 81, 76, 64, 65, 70], dtype=int32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the network is working with individual characters, it's similar to a classification problem in which we are trying to predict the next character from the previous text.  Here's how many 'classes' our network has to pick from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making training mini-batches\n",
    "\n",
    "Here is where we'll make our mini-batches for training. Remember that we want our batches to be multiple sequences of some desired number of sequence steps. Considering a simple example, our batches would look like this:\n",
    "\n",
    "<img src=\"assets/sequence_batching@1x.png\" width=500px>\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "We start with our text encoded as integers in one long array in `encoded`. Let's create a function that will give us an iterator for our batches. I like using [generator functions](https://jeffknupp.com/blog/2013/04/07/improve-your-python-yield-and-generators-explained/) to do this. Then we can pass `encoded` into this function and get our batch generator.\n",
    "\n",
    "The first thing we need to do is discard some of the text so we only have completely full batches. Each batch contains $N \\times M$ characters, where $N$ is the batch size (the number of sequences) and $M$ is the number of steps. Then, to get the total number of batches, $K$, we can make from the array `arr`, you divide the length of `arr` by the number of characters per batch. Once you know the number of batches, you can get the total number of characters to keep from `arr`, $N * M * K$.\n",
    "\n",
    "After that, we need to split `arr` into $N$ sequences. You can do this using `arr.reshape(size)` where `size` is a tuple containing the dimensions sizes of the reshaped array. We know we want $N$ sequences (`batch_size` below), let's make that the size of the first dimension. For the second dimension, you can use `-1` as a placeholder in the size, it'll fill up the array with the appropriate data for you. After this, you should have an array that is $N \\times (M * K)$.\n",
    "\n",
    "Now that we have this array, we can iterate through it to get our batches. The idea is each batch is a $N \\times M$ window on the $N \\times (M * K)$ array. For each subsequent batch, the window moves over by `n_steps`. We also want to create both the input and target arrays. Remember that the targets are the inputs shifted over one character. \n",
    "\n",
    "The way I like to do this window is use `range` to take steps of size `n_steps` from $0$ to `arr.shape[1]`, the total number of steps in each sequence. That way, the integers you get from `range` always point to the start of a batch, and each window is `n_steps` wide.\n",
    "\n",
    "> **Exercise:** Write the code for creating batches in the function below. The exercises in this notebook _will not be easy_. I've provided a notebook with solutions alongside this notebook. If you get stuck, checkout the solutions. The most important thing is that you don't copy and paste the code into here, **type out the solution code yourself.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sequence(n):\n",
    "    for i in range(n):\n",
    "        yield i\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq = sequence(3)\n",
    "next(seq)\n",
    "next(seq)\n",
    "next(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([[1,2],[2,5],[7,8],[2,4],[4,5]])\n",
    "a.shape\n",
    "type(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batches(arr, batch_size, n_steps):\n",
    "    '''Create a generator that returns batches of size\n",
    "       batch_size x n_steps from arr.\n",
    "       \n",
    "       Arguments\n",
    "       ---------\n",
    "       arr: Array you want to make batches from\n",
    "       batch_size: Batch size, the number of sequences per batch\n",
    "       n_steps: Number of sequence steps per batch\n",
    "    '''\n",
    "    # Get the number of characters per batch and number of batches we can make\n",
    "    characters_per_batch = batch_size*n_steps\n",
    "    total_nums = len(arr)\n",
    "    n_batches = total_nums // characters_per_batch\n",
    "    print(n_batches)\n",
    "    # Keep only enough characters to make full batches\n",
    "    factor = n_batches*characters_per_batch\n",
    "    arr = arr[:factor]\n",
    "    #print((len(arr)/batch_size))\n",
    "    # Reshape into batch_size rows\n",
    "    arr = arr.reshape((batch_size,int(len(arr)/batch_size)))\n",
    "    \n",
    "    for n in range(0, arr.shape[1], n_steps):\n",
    "        # The features\n",
    "        print(n)\n",
    "        #print(n_steps)\n",
    "        x = arr[:,n:(n+n_steps)]\n",
    "        # The targets, shifted by one\n",
    "        y_temp = arr[:,n+1:(n+1+n_steps)]\n",
    "        # For the very last batch, y will be one character short at the end of \n",
    "        # the sequences which breaks things. To get around this, I'll make an \n",
    "        # array of the appropriate size first, of all zeros, then add the targets.\n",
    "        # This will introduce a small artifact in the last batch, but it won't matter.\n",
    "        y = np.zeros(x.shape, dtype=x.dtype)\n",
    "        y[:,:y_temp.shape[1]] = y_temp\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,10,2):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'll make my data sets and we can check out what's going on here. Here I'm going to use a batch size of 10 and 50 sequence steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3970\n",
      "0\n",
      "50\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "batches = get_batches(encoded, 10, 50)\n",
    "x1, y1 = next(batches)\n",
    "x,y = next(batches)\n",
    "x2,y2 = next(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      " [[31 64 57 72 76 61 74  1 16  0]\n",
      " [ 1 57 69  1 70 71 76  1 63 71]\n",
      " [78 65 70 13  0  0  3 53 61 75]\n",
      " [70  1 60 77 74 65 70 63  1 64]\n",
      " [ 1 65 76  1 65 75 11  1 75 65]\n",
      " [ 1 37 76  1 79 57 75  0 71 70]\n",
      " [64 61 70  1 59 71 69 61  1 62]\n",
      " [26  1 58 77 76  1 70 71 79  1]\n",
      " [76  1 65 75 70  7 76 13  1 48]\n",
      " [ 1 75 57 65 60  1 76 71  1 64]]\n",
      "\n",
      "y\n",
      " [[64 57 72 76 61 74  1 16  0  0]\n",
      " [57 69  1 70 71 76  1 63 71 65]\n",
      " [65 70 13  0  0  3 53 61 75 11]\n",
      " [ 1 60 77 74 65 70 63  1 64 65]\n",
      " [65 76  1 65 75 11  1 75 65 74]\n",
      " [37 76  1 79 57 75  0 71 70 68]\n",
      " [61 70  1 59 71 69 61  1 62 71]\n",
      " [ 1 58 77 76  1 70 71 79  1 75]\n",
      " [ 1 65 75 70  7 76 13  1 48 64]\n",
      " [75 57 65 60  1 76 71  1 64 61]]\n",
      "x\n",
      " [[64 57 72 72 81  1 62 57 69 65]\n",
      " [76  1 65 70  1 75 72 65 76 61]\n",
      " [26  1 76 64 65 74 76 81 12 61]\n",
      " [75 13  1 43 70 59 61  1 65 70]\n",
      " [71 75 75 65 70 63  1 64 65 69]\n",
      " [74  1 64 71 77 75 61  1 58 61]\n",
      " [ 1 74 71 71 69 13  0  0 40 61]\n",
      " [61 74 61 68 81  0 75 77 59 64]\n",
      " [76 64 61  1 68 57 70 60 71 79]\n",
      " [70 63 13  1  3 36 61  7 75  1]]\n",
      "\n",
      "y\n",
      " [[57 72 72 81  1 62 57 69 65 68]\n",
      " [ 1 65 70  1 75 72 65 76 61  1]\n",
      " [ 1 76 64 65 74 76 81 12 61 65]\n",
      " [13  1 43 70 59 61  1 65 70  1]\n",
      " [75 75 65 70 63  1 64 65 69 75]\n",
      " [ 1 64 71 77 75 61  1 58 61 62]\n",
      " [74 71 71 69 13  0  0 40 61 78]\n",
      " [74 61 68 81  0 75 77 59 64  1]\n",
      " [64 61  1 68 57 70 60 71 79 70]\n",
      " [63 13  1  3 36 61  7 75  1 76]]\n",
      "x\n",
      " [[63  1 79 57 75  1 65 70  1 59]\n",
      " [75 76 61 74  1 71 62  1 76 64]\n",
      " [76 64 61  1 74 61 75 76  1 65]\n",
      " [76 64 61  0 69 71 79 65 70 63]\n",
      " [79 61 70 76  1 71 77 76 13  0]\n",
      " [ 1 75 57 79  1 64 61 74  1 76]\n",
      " [71 76  1 65 70  1 76 64 61  1]\n",
      " [71 62  1 69 65 70 60 11  1 76]\n",
      " [ 1 71 79 70  0 76 64 74 71 57]\n",
      " [78 61  1 64 65 69 11  1 57 70]]\n",
      "\n",
      "y\n",
      " [[ 1 79 57 75  1 65 70  1 59 71]\n",
      " [76 61 74  1 71 62  1 76 64 61]\n",
      " [64 61  1 74 61 75 76  1 65 70]\n",
      " [64 61  0 69 71 79 65 70 63 11]\n",
      " [61 70 76  1 71 77 76 13  0  0]\n",
      " [75 57 79  1 64 61 74  1 76 61]\n",
      " [76  1 65 70  1 76 64 61  1 68]\n",
      " [62  1 69 65 70 60 11  1 76 71]\n",
      " [71 79 70  0 76 64 74 71 57 76]\n",
      " [61  1 64 65 69 11  1 57 70 60]]\n"
     ]
    }
   ],
   "source": [
    "print('x\\n', x1[:10, :10])\n",
    "print('\\ny\\n', y1[:10, :10])\n",
    "print('x\\n', x[:10, :10])\n",
    "print('\\ny\\n', y[:10, :10])\n",
    "print('x\\n', x2[:10, :10])\n",
    "print('\\ny\\n', y2[:10, :10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you implemented `get_batches` correctly, the above output should look something like \n",
    "```\n",
    "x\n",
    " [[55 63 69 22  6 76 45  5 16 35]\n",
    " [ 5 69  1  5 12 52  6  5 56 52]\n",
    " [48 29 12 61 35 35  8 64 76 78]\n",
    " [12  5 24 39 45 29 12 56  5 63]\n",
    " [ 5 29  6  5 29 78 28  5 78 29]\n",
    " [ 5 13  6  5 36 69 78 35 52 12]\n",
    " [63 76 12  5 18 52  1 76  5 58]\n",
    " [34  5 73 39  6  5 12 52 36  5]\n",
    " [ 6  5 29 78 12 79  6 61  5 59]\n",
    " [ 5 78 69 29 24  5  6 52  5 63]]\n",
    "\n",
    "y\n",
    " [[63 69 22  6 76 45  5 16 35 35]\n",
    " [69  1  5 12 52  6  5 56 52 29]\n",
    " [29 12 61 35 35  8 64 76 78 28]\n",
    " [ 5 24 39 45 29 12 56  5 63 29]\n",
    " [29  6  5 29 78 28  5 78 29 45]\n",
    " [13  6  5 36 69 78 35 52 12 43]\n",
    " [76 12  5 18 52  1 76  5 58 52]\n",
    " [ 5 73 39  6  5 12 52 36  5 78]\n",
    " [ 5 29 78 12 79  6 61  5 59 63]\n",
    " [78 69 29 24  5  6 52  5 63 76]]\n",
    " ```\n",
    " although the exact numbers will be different. Check to make sure the data is shifted over one step for `y`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model\n",
    "\n",
    "Below is where you'll build the network. We'll break it up into parts so it's easier to reason about each bit. Then we can connect them up into the whole network.\n",
    "\n",
    "<img src=\"assets/charRNN.png\" width=500px>\n",
    "\n",
    "\n",
    "### Inputs\n",
    "\n",
    "First off we'll create our input placeholders. As usual we need placeholders for the training data and the targets. We'll also create a placeholder for dropout layers called `keep_prob`. This will be a scalar, that is a 0-D tensor. To make a scalar, you create a placeholder without giving it a size.\n",
    "\n",
    "> **Exercise:** Create the input placeholders in the function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_inputs(batch_size, num_steps):\n",
    "    ''' Define placeholders for inputs, targets, and dropout \n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        batch_size: Batch size, number of sequences per batch\n",
    "        num_steps: Number of sequence steps in a batch\n",
    "        \n",
    "    '''\n",
    "    # Declare placeholders we'll feed into the graph\n",
    "    inputs = tf.placeholder(shape=(batch_size,num_steps),dtype = tf.int32, name = \"inputs\")\n",
    "    targets = tf.placeholder(shape=(batch_size,num_steps),dtype = tf.int32, name = \"targets\")\n",
    "    \n",
    "    # Keep probability placeholder for drop out layers\n",
    "    keep_prob = tf.placeholder(dtype = tf.float32, name = \"keep_prob\")\n",
    "    \n",
    "    return inputs, targets, keep_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Cell\n",
    "\n",
    "Here we will create the LSTM cell we'll use in the hidden layer. We'll use this cell as a building block for the RNN. So we aren't actually defining the RNN here, just the type of cell we'll use in the hidden layer.\n",
    "\n",
    "We first create a basic LSTM cell with\n",
    "\n",
    "```python\n",
    "lstm = tf.contrib.rnn.BasicLSTMCell(num_units)\n",
    "```\n",
    "\n",
    "where `num_units` is the number of units in the hidden layers in the cell. Then we can add dropout by wrapping it with \n",
    "\n",
    "```python\n",
    "tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "```\n",
    "You pass in a cell and it will automatically add dropout to the inputs or outputs. Finally, we can stack up the LSTM cells into layers with [`tf.contrib.rnn.MultiRNNCell`](https://www.tensorflow.org/versions/r1.0/api_docs/python/tf/contrib/rnn/MultiRNNCell). With this, you pass in a list of cells and it will send the output of one cell into the next cell. Previously with TensorFlow 1.0, you could do this\n",
    "\n",
    "```python\n",
    "tf.contrib.rnn.MultiRNNCell([cell]*num_layers)\n",
    "```\n",
    "\n",
    "This might look a little weird if you know Python well because this will create a list of the same `cell` object. However, TensorFlow 1.0 will create different weight matrices for all `cell` objects. But, starting with TensorFlow 1.1 you actually need to create new cell objects in the list. To get it to work in TensorFlow 1.1, it should look like\n",
    "\n",
    "```python\n",
    "def build_cell(num_units, keep_prob):\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(num_units)\n",
    "    drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "    \n",
    "    return drop\n",
    "    \n",
    "tf.contrib.rnn.MultiRNNCell([build_cell(num_units, keep_prob) for _ in range(num_layers)])\n",
    "```\n",
    "\n",
    "Even though this is actually multiple LSTM cells stacked on each other, you can treat the multiple layers as one cell.\n",
    "\n",
    "We also need to create an initial cell state of all zeros. This can be done like so\n",
    "\n",
    "```python\n",
    "initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "```\n",
    "\n",
    "Below, we implement the `build_lstm` function to create these LSTM cells and the initial state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_lstm(lstm_size, num_layers, batch_size, keep_prob):\n",
    "    ''' Build LSTM cell.\n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        keep_prob: Scalar tensor (tf.placeholder) for the dropout keep probability\n",
    "        lstm_size: Size of the hidden layers in the LSTM cells\n",
    "        num_layers: Number of LSTM layers\n",
    "        batch_size: Batch size\n",
    "\n",
    "    '''\n",
    "    ### Build the LSTM Cell\n",
    "    # Use a basic LSTM cell\n",
    "    def build_cell(lstm_size,keep_prob):\n",
    "        lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "    \n",
    "    # Add dropout to the cell outputs\n",
    "        drop = tf.contrib.rnn.DropoutWrapper(lstm ,output_keep_prob=keep_prob)\n",
    "        return drop\n",
    "    \n",
    "    # Stack up multiple LSTM layers, for deep learning\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([build_cell(lstm_size,keep_prob) for _ in range(num_layers)])\n",
    "    initial_state = cell.zero_state(batch_size,tf.float32)\n",
    "    \n",
    "    return cell, initial_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Output\n",
    "\n",
    "Here we'll create the output layer. We need to connect the output of the RNN cells to a full connected layer with a softmax output. The softmax output gives us a probability distribution we can use to predict the next character, so we want this layer to have size $C$, the number of classes/characters we have in our text.\n",
    "\n",
    "If our input has batch size $N$, number of steps $M$, and the hidden layer has $L$ hidden units, then the output is a 3D tensor with size $N \\times M \\times L$. The output of each LSTM cell has size $L$, we have $M$ of them, one for each sequence step, and we have $N$ sequences. So the total size is $N \\times M \\times L$. \n",
    "\n",
    "We are using the same fully connected layer, the same weights, for each of the outputs. Then, to make things easier, we should reshape the outputs into a 2D tensor with shape $(M * N) \\times L$. That is, one row for each sequence and step, where the values of each row are the output from the LSTM cells. We get the LSTM output as a list, `lstm_output`. First we need to concatenate this whole list into one array with [`tf.concat`](https://www.tensorflow.org/api_docs/python/tf/concat). Then, reshape it (with `tf.reshape`) to size $(M * N) \\times L$.\n",
    "\n",
    "One we have the outputs reshaped, we can do the matrix multiplication with the weights. We need to wrap the weight and bias variables in a variable scope with `tf.variable_scope(scope_name)` because there are weights being created in the LSTM cells. TensorFlow will throw an error if the weights created here have the same names as the weights created in the LSTM cells, which they will be default. To avoid this, we wrap the variables in a variable scope so we can give them unique names.\n",
    "\n",
    "> **Exercise:** Implement the output layer in the function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_output(lstm_output, in_size, out_size):\n",
    "    ''' Build a softmax layer, return the softmax output and logits.\n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        \n",
    "        lstm_output: List of output tensors from the LSTM layer\n",
    "        in_size: Size of the input tensor, for example, size of the LSTM cells\n",
    "        out_size: Size of this softmax layer\n",
    "    \n",
    "    '''\n",
    "\n",
    "    # Reshape output so it's a bunch of rows, one row for each step for each sequence.\n",
    "    # Concatenate lstm_output over axis 1 (the columns)\n",
    "    seq_output = tf.concat(lstm_output,axis=1)    # right\n",
    "    # Reshape seq_output to a 2D tensor with lstm_size columns\n",
    "    x = tf.reshape(seq_output, [-1,in_size])\n",
    "    \n",
    "    # Connect the RNN outputs to a softmax layer\n",
    "    with tf.variable_scope('softmax'):\n",
    "        # Create the weight and bias variables here\n",
    "        softmax_w = tf.Variable(tf.truncated_normal((in_size,out_size),stddev=0.1))\n",
    "        softmax_b = tf.Variable(tf.zeros(out_size))\n",
    "    \n",
    "    # Since output is a bunch of rows of RNN cell outputs, logits will be a bunch\n",
    "    # of rows of logit outputs, one for each step and sequence\n",
    "    logits = tf.add(tf.matmul(x,softmax_w),softmax_b)\n",
    "    \n",
    "    # Use softmax to get the probabilities for predicted characters\n",
    "    out = tf.nn.softmax(logits=logits,name=\"predictions\")\n",
    "    \n",
    "    return out, logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loss\n",
    "\n",
    "Next up is the training loss. We get the logits and targets and calculate the softmax cross-entropy loss. First we need to one-hot encode the targets, we're getting them as encoded characters. Then, reshape the one-hot targets so it's a 2D tensor with size $(M*N) \\times C$ where $C$ is the number of classes/characters we have. Remember that we reshaped the LSTM outputs and ran them through a fully connected layer with $C$ units. So our logits will also have size $(M*N) \\times C$.\n",
    "\n",
    "Then we run the logits and targets through `tf.nn.softmax_cross_entropy_with_logits` and find the mean to get the loss.\n",
    "\n",
    ">**Exercise:** Implement the loss calculation in the function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_loss(logits, targets, lstm_size, num_classes):\n",
    "    ''' Calculate the loss from the logits and the targets.\n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        logits: Logits from final fully connected layer\n",
    "        targets: Targets for supervised learning\n",
    "        lstm_size: Number of LSTM hidden units\n",
    "        num_classes: Number of classes in targets\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    # One-hot encode targets and reshape to match logits, one row per sequence per step\n",
    "    y_one_hot = tf.one_hot(targets,num_classes) \n",
    "    y_reshaped =  tf.reshape(y_one_hot,logits.get_shape())\n",
    "    \n",
    "    # Softmax cross entropy loss\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits,labels=y_reshaped)\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer\n",
    "\n",
    "Here we build the optimizer. Normal RNNs have have issues gradients exploding and disappearing. LSTMs fix the disappearance problem, but the gradients can still grow without bound. To fix this, we can clip the gradients above some threshold. That is, if a gradient is larger than that threshold, we set it to the threshold. This will ensure the gradients never grow overly large. Then we use an AdamOptimizer for the learning step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_optimizer(loss, learning_rate, grad_clip):\n",
    "    ''' Build optmizer for training, using gradient clipping.\n",
    "    \n",
    "        Arguments:\n",
    "        loss: Network loss\n",
    "        learning_rate: Learning rate for optimizer\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # Optimizer for training, using gradient clipping to control exploding gradients\n",
    "    tvars = tf.trainable_variables()\n",
    "    grads, _ = tf.clip_by_global_norm(tf.gradients(loss, tvars), grad_clip)\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate)\n",
    "    optimizer = train_op.apply_gradients(zip(grads, tvars))\n",
    "    \n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_optimizer(loss,learning_rate,grad_clip):\n",
    "    tvars = tf.trainable_variables()\n",
    "    grads,_ = tf.clip_by_global_norm(tf.gradients(loss,tvars),grad_clip)\n",
    "    trian_op = tf.train.AdamOptimizer(learning_rate)\n",
    "    optimizer = train_op.apply_gradients(zip(grads,tvars))\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the network\n",
    "\n",
    "Now we can put all the pieces together and build a class for the network. To actually run data through the LSTM cells, we will use [`tf.nn.dynamic_rnn`](https://www.tensorflow.org/versions/r1.0/api_docs/python/tf/nn/dynamic_rnn). This function will pass the hidden and cell states across LSTM cells appropriately for us. It returns the outputs for each LSTM cell at each step for each sequence in the mini-batch. It also gives us the final LSTM state. We want to save this state as `final_state` so we can pass it to the first LSTM cell in the the next mini-batch run. For `tf.nn.dynamic_rnn`, we pass in the cell and initial state we get from `build_lstm`, as well as our input sequences. Also, we need to one-hot encode the inputs before going into the RNN. \n",
    "\n",
    "> **Exercise:** Use the functions you've implemented previously and `tf.nn.dynamic_rnn` to build the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CharRNN:\n",
    "    \n",
    "    def __init__(self, num_classes, batch_size=64, num_steps=50, \n",
    "                       lstm_size=128, num_layers=2, learning_rate=0.001, \n",
    "                       grad_clip=5, sampling=False):\n",
    "    \n",
    "        # When we're using this network for sampling later, we'll be passing in\n",
    "        # one character at a time, so providing an option for that\n",
    "        if sampling == True:\n",
    "            batch_size, num_steps = 1, 1\n",
    "        else:\n",
    "            batch_size, num_steps = batch_size, num_steps\n",
    "\n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        # Build the input placeholder tensors\n",
    "        self.inputs, self.targets, self.keep_prob =  build_inputs(batch_size, num_steps)\n",
    "\n",
    "        # Build the LSTM cell\n",
    "        cell, self.initial_state = build_lstm(lstm_size, num_layers, batch_size, self.keep_prob)\n",
    "\n",
    "        ### Run the data through the RNN layers\n",
    "        # First, one-hot encode the input tokens\n",
    "        x_one_hot = tf.one_hot(self.inputs,num_classes)\n",
    "        \n",
    "        # Run each sequence step through the RNN with tf.nn.dynamic_rnn \n",
    "        outputs, state = tf.nn.dynamic_rnn(cell,x_one_hot,initial_state=self.initial_state)\n",
    "        self.final_state = state\n",
    "        \n",
    "        # Get softmax predictions and logits\n",
    "        self.prediction, self.logits = build_output(outputs, lstm_size, num_classes)\n",
    "        \n",
    "        # Loss and optimizer (with gradient clipping)\n",
    "        self.loss =  build_loss(self.logits, self.targets, lstm_size, num_classes)\n",
    "        self.optimizer = build_optimizer(self.loss, learning_rate, grad_clip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "Here are the hyperparameters for the network.\n",
    "\n",
    "* `batch_size` - Number of sequences running through the network in one pass.\n",
    "* `num_steps` - Number of characters in the sequence the network is trained on. Larger is better typically, the network will learn more long range dependencies. But it takes longer to train. 100 is typically a good number here.\n",
    "* `lstm_size` - The number of units in the hidden layers.\n",
    "* `num_layers` - Number of hidden LSTM layers to use\n",
    "* `learning_rate` - Learning rate for training\n",
    "* `keep_prob` - The dropout keep probability when training. If you're network is overfitting, try decreasing this.\n",
    "\n",
    "Here's some good advice from Andrej Karpathy on training the network. I'm going to copy it in here for your benefit, but also link to [where it originally came from](https://github.com/karpathy/char-rnn#tips-and-tricks).\n",
    "\n",
    "> ## Tips and Tricks\n",
    "\n",
    ">### Monitoring Validation Loss vs. Training Loss\n",
    ">If you're somewhat new to Machine Learning or Neural Networks it can take a bit of expertise to get good models. The most important quantity to keep track of is the difference between your training loss (printed during training) and the validation loss (printed once in a while when the RNN is run on the validation data (by default every 1000 iterations)). In particular:\n",
    "\n",
    "> - If your training loss is much lower than validation loss then this means the network might be **overfitting**. Solutions to this are to decrease your network size, or to increase dropout. For example you could try dropout of 0.5 and so on.\n",
    "> - If your training/validation loss are about equal then your model is **underfitting**. Increase the size of your model (either number of layers or the raw number of neurons per layer)\n",
    "\n",
    "> ### Approximate number of parameters\n",
    "\n",
    "> The two most important parameters that control the model are `lstm_size` and `num_layers`. I would advise that you always use `num_layers` of either 2/3. The `lstm_size` can be adjusted based on how much data you have. The two important quantities to keep track of here are:\n",
    "\n",
    "> - The number of parameters in your model. This is printed when you start training.\n",
    "> - The size of your dataset. 1MB file is approximately 1 million characters.\n",
    "\n",
    ">These two should be about the same order of magnitude. It's a little tricky to tell. Here are some examples:\n",
    "\n",
    "> - I have a 100MB dataset and I'm using the default parameter settings (which currently print 150K parameters). My data size is significantly larger (100 mil >> 0.15 mil), so I expect to heavily underfit. I am thinking I can comfortably afford to make `lstm_size` larger.\n",
    "> - I have a 10MB dataset and running a 10 million parameter model. I'm slightly nervous and I'm carefully monitoring my validation loss. If it's larger than my training loss then I may want to try to increase dropout a bit and see if that helps the validation loss.\n",
    "\n",
    "> ### Best models strategy\n",
    "\n",
    ">The winning strategy to obtaining very good models (if you have the compute time) is to always err on making the network larger (as large as you're willing to wait for it to compute) and then try different dropout values (between 0,1). Whatever model has the best validation performance (the loss, written in the checkpoint filename, low is good) is the one you should use in the end.\n",
    "\n",
    ">It is very common in deep learning to run many different models with many different hyperparameter settings, and in the end take whatever checkpoint gave the best validation performance.\n",
    "\n",
    ">By the way, the size of your training and validation splits are also parameters. Make sure you have a decent amount of data in your validation set or otherwise the validation performance will be noisy and not very informative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 10         # Sequences per batch\n",
    "num_steps = 50          # Number of sequence steps per batch\n",
    "lstm_size = 128         # Size of hidden layers in LSTMs\n",
    "num_layers = 2          # Number of LSTM layers\n",
    "learning_rate = 0.01    # Learning rate\n",
    "keep_prob = 0.5         # Dropout keep probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time for training\n",
    "\n",
    "This is typical training code, passing inputs and targets into the network, then running the optimizer. Here we also get back the final LSTM state for the mini-batch. Then, we pass that state back into the network so the next batch can continue the state from the previous batch. And every so often (set by `save_every_n`) I save a checkpoint.\n",
    "\n",
    "Here I'm saving checkpoints with the format\n",
    "\n",
    "`i{iteration number}_l{# hidden layer units}.ckpt`\n",
    "\n",
    "> **Exercise:** Set the hyperparameters above to train the network. Watch the training loss, it should be consistently dropping. Also, I highly advise running this on a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3970\n",
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "800\n",
      "850\n",
      "900\n",
      "950\n",
      "1000\n",
      "1050\n",
      "1100\n",
      "1150\n",
      "1200\n",
      "1250\n",
      "1300\n",
      "1350\n",
      "1400\n",
      "1450\n",
      "1500\n",
      "1550\n",
      "1600\n",
      "1650\n",
      "1700\n",
      "1750\n",
      "1800\n",
      "1850\n",
      "1900\n",
      "1950\n",
      "2000\n",
      "2050\n",
      "2100\n",
      "2150\n",
      "2200\n",
      "2250\n",
      "2300\n",
      "2350\n",
      "2400\n",
      "2450\n",
      "Epoch: 1/20...  Training Step: 50...  Training loss: 3.1231...  0.1041 sec/batch\n",
      "2500\n",
      "2550\n",
      "2600\n",
      "2650\n",
      "2700\n",
      "2750\n",
      "2800\n",
      "2850\n",
      "2900\n",
      "2950\n",
      "3000\n",
      "3050\n",
      "3100\n",
      "3150\n",
      "3200\n",
      "3250\n",
      "3300\n",
      "3350\n",
      "3400\n",
      "3450\n",
      "3500\n",
      "3550\n",
      "3600\n",
      "3650\n",
      "3700\n",
      "3750\n",
      "3800\n",
      "3850\n",
      "3900\n",
      "3950\n",
      "4000\n",
      "4050\n",
      "4100\n",
      "4150\n",
      "4200\n",
      "4250\n",
      "4300\n",
      "4350\n",
      "4400\n",
      "4450\n",
      "4500\n",
      "4550\n",
      "4600\n",
      "4650\n",
      "4700\n",
      "4750\n",
      "4800\n",
      "4850\n",
      "4900\n",
      "4950\n",
      "Epoch: 1/20...  Training Step: 100...  Training loss: 2.6921...  0.0841 sec/batch\n",
      "5000\n",
      "5050\n",
      "5100\n",
      "5150\n",
      "5200\n",
      "5250\n",
      "5300\n",
      "5350\n",
      "5400\n",
      "5450\n",
      "5500\n",
      "5550\n",
      "5600\n",
      "5650\n",
      "5700\n",
      "5750\n",
      "5800\n",
      "5850\n",
      "5900\n",
      "5950\n",
      "6000\n",
      "6050\n",
      "6100\n",
      "6150\n",
      "6200\n",
      "6250\n",
      "6300\n",
      "6350\n",
      "6400\n",
      "6450\n",
      "6500\n",
      "6550\n",
      "6600\n",
      "6650\n",
      "6700\n",
      "6750\n",
      "6800\n",
      "6850\n",
      "6900\n",
      "6950\n",
      "7000\n",
      "7050\n",
      "7100\n",
      "7150\n",
      "7200\n",
      "7250\n",
      "7300\n",
      "7350\n",
      "7400\n",
      "7450\n",
      "Epoch: 1/20...  Training Step: 150...  Training loss: 2.4518...  0.0917 sec/batch\n",
      "7500\n",
      "7550\n",
      "7600\n",
      "7650\n",
      "7700\n",
      "7750\n",
      "7800\n",
      "7850\n",
      "7900\n",
      "7950\n",
      "8000\n",
      "8050\n",
      "8100\n",
      "8150\n",
      "8200\n",
      "8250\n",
      "8300\n",
      "8350\n",
      "8400\n",
      "8450\n",
      "8500\n",
      "8550\n",
      "8600\n",
      "8650\n",
      "8700\n",
      "8750\n",
      "8800\n",
      "8850\n",
      "8900\n",
      "8950\n",
      "9000\n",
      "9050\n",
      "9100\n",
      "9150\n",
      "9200\n",
      "9250\n",
      "9300\n",
      "9350\n",
      "9400\n",
      "9450\n",
      "9500\n",
      "9550\n",
      "9600\n",
      "9650\n",
      "9700\n",
      "9750\n",
      "9800\n",
      "9850\n",
      "9900\n",
      "9950\n",
      "Epoch: 1/20...  Training Step: 200...  Training loss: 2.4605...  0.1162 sec/batch\n",
      "10000\n",
      "10050\n",
      "10100\n",
      "10150\n",
      "10200\n",
      "10250\n",
      "10300\n",
      "10350\n",
      "10400\n",
      "10450\n",
      "10500\n",
      "10550\n",
      "10600\n",
      "10650\n",
      "10700\n",
      "10750\n",
      "10800\n",
      "10850\n",
      "10900\n",
      "10950\n",
      "11000\n",
      "11050\n",
      "11100\n",
      "11150\n",
      "11200\n",
      "11250\n",
      "11300\n",
      "11350\n",
      "11400\n",
      "11450\n",
      "11500\n",
      "11550\n",
      "11600\n",
      "11650\n",
      "11700\n",
      "11750\n",
      "11800\n",
      "11850\n",
      "11900\n",
      "11950\n",
      "12000\n",
      "12050\n",
      "12100\n",
      "12150\n",
      "12200\n",
      "12250\n",
      "12300\n",
      "12350\n",
      "12400\n",
      "12450\n",
      "Epoch: 1/20...  Training Step: 250...  Training loss: 2.3669...  0.0735 sec/batch\n",
      "12500\n",
      "12550\n",
      "12600\n",
      "12650\n",
      "12700\n",
      "12750\n",
      "12800\n",
      "12850\n",
      "12900\n",
      "12950\n",
      "13000\n",
      "13050\n",
      "13100\n",
      "13150\n",
      "13200\n",
      "13250\n",
      "13300\n",
      "13350\n",
      "13400\n",
      "13450\n",
      "13500\n",
      "13550\n",
      "13600\n",
      "13650\n",
      "13700\n",
      "13750\n",
      "13800\n",
      "13850\n",
      "13900\n",
      "13950\n",
      "14000\n",
      "14050\n",
      "14100\n",
      "14150\n",
      "14200\n",
      "14250\n",
      "14300\n",
      "14350\n",
      "14400\n",
      "14450\n",
      "14500\n",
      "14550\n",
      "14600\n",
      "14650\n",
      "14700\n",
      "14750\n",
      "14800\n",
      "14850\n",
      "14900\n",
      "14950\n",
      "Epoch: 1/20...  Training Step: 300...  Training loss: 2.2751...  0.0931 sec/batch\n",
      "15000\n",
      "15050\n",
      "15100\n",
      "15150\n",
      "15200\n",
      "15250\n",
      "15300\n",
      "15350\n",
      "15400\n",
      "15450\n",
      "15500\n",
      "15550\n",
      "15600\n",
      "15650\n",
      "15700\n",
      "15750\n",
      "15800\n",
      "15850\n",
      "15900\n",
      "15950\n",
      "16000\n",
      "16050\n",
      "16100\n",
      "16150\n",
      "16200\n",
      "16250\n",
      "16300\n",
      "16350\n",
      "16400\n",
      "16450\n",
      "16500\n",
      "16550\n",
      "16600\n",
      "16650\n",
      "16700\n",
      "16750\n",
      "16800\n",
      "16850\n",
      "16900\n",
      "16950\n",
      "17000\n",
      "17050\n",
      "17100\n",
      "17150\n",
      "17200\n",
      "17250\n",
      "17300\n",
      "17350\n",
      "17400\n",
      "17450\n",
      "Epoch: 1/20...  Training Step: 350...  Training loss: 2.1498...  0.0807 sec/batch\n",
      "17500\n",
      "17550\n",
      "17600\n",
      "17650\n",
      "17700\n",
      "17750\n",
      "17800\n",
      "17850\n",
      "17900\n",
      "17950\n",
      "18000\n",
      "18050\n",
      "18100\n",
      "18150\n",
      "18200\n",
      "18250\n",
      "18300\n",
      "18350\n",
      "18400\n",
      "18450\n",
      "18500\n",
      "18550\n",
      "18600\n",
      "18650\n",
      "18700\n",
      "18750\n",
      "18800\n",
      "18850\n",
      "18900\n",
      "18950\n",
      "19000\n",
      "19050\n",
      "19100\n",
      "19150\n",
      "19200\n",
      "19250\n",
      "19300\n",
      "19350\n",
      "19400\n",
      "19450\n",
      "19500\n",
      "19550\n",
      "19600\n",
      "19650\n",
      "19700\n",
      "19750\n",
      "19800\n",
      "19850\n",
      "19900\n",
      "19950\n",
      "Epoch: 1/20...  Training Step: 400...  Training loss: 2.2111...  0.0878 sec/batch\n",
      "20000\n",
      "20050\n",
      "20100\n",
      "20150\n",
      "20200\n",
      "20250\n",
      "20300\n",
      "20350\n",
      "20400\n",
      "20450\n",
      "20500\n",
      "20550\n",
      "20600\n",
      "20650\n",
      "20700\n",
      "20750\n",
      "20800\n",
      "20850\n",
      "20900\n",
      "20950\n",
      "21000\n",
      "21050\n",
      "21100\n",
      "21150\n",
      "21200\n",
      "21250\n",
      "21300\n",
      "21350\n",
      "21400\n",
      "21450\n",
      "21500\n",
      "21550\n",
      "21600\n",
      "21650\n",
      "21700\n",
      "21750\n",
      "21800\n",
      "21850\n",
      "21900\n",
      "21950\n",
      "22000\n",
      "22050\n",
      "22100\n",
      "22150\n",
      "22200\n",
      "22250\n",
      "22300\n",
      "22350\n",
      "22400\n",
      "22450\n",
      "Epoch: 1/20...  Training Step: 450...  Training loss: 2.2012...  0.0724 sec/batch\n",
      "22500\n",
      "22550\n",
      "22600\n",
      "22650\n",
      "22700\n",
      "22750\n",
      "22800\n",
      "22850\n",
      "22900\n",
      "22950\n",
      "23000\n",
      "23050\n",
      "23100\n",
      "23150\n",
      "23200\n",
      "23250\n",
      "23300\n",
      "23350\n",
      "23400\n",
      "23450\n",
      "23500\n",
      "23550\n",
      "23600\n",
      "23650\n",
      "23700\n",
      "23750\n",
      "23800\n",
      "23850\n",
      "23900\n",
      "23950\n",
      "24000\n",
      "24050\n",
      "24100\n",
      "24150\n",
      "24200\n",
      "24250\n",
      "24300\n",
      "24350\n",
      "24400\n",
      "24450\n",
      "24500\n",
      "24550\n",
      "24600\n",
      "24650\n",
      "24700\n",
      "24750\n",
      "24800\n",
      "24850\n",
      "24900\n",
      "24950\n",
      "Epoch: 1/20...  Training Step: 500...  Training loss: 2.2531...  0.0749 sec/batch\n",
      "25000\n",
      "25050\n",
      "25100\n",
      "25150\n",
      "25200\n",
      "25250\n",
      "25300\n",
      "25350\n",
      "25400\n",
      "25450\n",
      "25500\n",
      "25550\n",
      "25600\n",
      "25650\n",
      "25700\n",
      "25750\n",
      "25800\n",
      "25850\n",
      "25900\n",
      "25950\n",
      "26000\n",
      "26050\n",
      "26100\n",
      "26150\n",
      "26200\n",
      "26250\n",
      "26300\n",
      "26350\n",
      "26400\n",
      "26450\n",
      "26500\n",
      "26550\n",
      "26600\n",
      "26650\n",
      "26700\n",
      "26750\n",
      "26800\n",
      "26850\n",
      "26900\n",
      "26950\n",
      "27000\n",
      "27050\n",
      "27100\n",
      "27150\n",
      "27200\n",
      "27250\n",
      "27300\n",
      "27350\n",
      "27400\n",
      "27450\n",
      "Epoch: 1/20...  Training Step: 550...  Training loss: 1.9846...  0.0809 sec/batch\n",
      "27500\n",
      "27550\n",
      "27600\n",
      "27650\n",
      "27700\n",
      "27750\n",
      "27800\n",
      "27850\n",
      "27900\n",
      "27950\n",
      "28000\n",
      "28050\n",
      "28100\n",
      "28150\n",
      "28200\n",
      "28250\n",
      "28300\n",
      "28350\n",
      "28400\n",
      "28450\n",
      "28500\n",
      "28550\n",
      "28600\n",
      "28650\n",
      "28700\n",
      "28750\n",
      "28800\n",
      "28850\n",
      "28900\n",
      "28950\n",
      "29000\n",
      "29050\n",
      "29100\n",
      "29150\n",
      "29200\n",
      "29250\n",
      "29300\n",
      "29350\n",
      "29400\n",
      "29450\n",
      "29500\n",
      "29550\n",
      "29600\n",
      "29650\n",
      "29700\n",
      "29750\n",
      "29800\n",
      "29850\n",
      "29900\n",
      "29950\n",
      "Epoch: 1/20...  Training Step: 600...  Training loss: 1.9085...  0.0734 sec/batch\n",
      "30000\n",
      "30050\n",
      "30100\n",
      "30150\n",
      "30200\n",
      "30250\n",
      "30300\n",
      "30350\n",
      "30400\n",
      "30450\n",
      "30500\n",
      "30550\n",
      "30600\n",
      "30650\n",
      "30700\n",
      "30750\n",
      "30800\n",
      "30850\n",
      "30900\n",
      "30950\n",
      "31000\n",
      "31050\n",
      "31100\n",
      "31150\n",
      "31200\n",
      "31250\n",
      "31300\n",
      "31350\n",
      "31400\n",
      "31450\n",
      "31500\n",
      "31550\n",
      "31600\n",
      "31650\n",
      "31700\n",
      "31750\n",
      "31800\n",
      "31850\n",
      "31900\n",
      "31950\n",
      "32000\n",
      "32050\n",
      "32100\n",
      "32150\n",
      "32200\n",
      "32250\n",
      "32300\n",
      "32350\n",
      "32400\n",
      "32450\n",
      "Epoch: 1/20...  Training Step: 650...  Training loss: 1.9089...  0.0716 sec/batch\n",
      "32500\n",
      "32550\n",
      "32600\n",
      "32650\n",
      "32700\n",
      "32750\n",
      "32800\n",
      "32850\n",
      "32900\n",
      "32950\n",
      "33000\n",
      "33050\n",
      "33100\n",
      "33150\n",
      "33200\n",
      "33250\n",
      "33300\n",
      "33350\n",
      "33400\n",
      "33450\n",
      "33500\n",
      "33550\n",
      "33600\n",
      "33650\n",
      "33700\n",
      "33750\n",
      "33800\n",
      "33850\n",
      "33900\n",
      "33950\n",
      "34000\n",
      "34050\n",
      "34100\n",
      "34150\n",
      "34200\n",
      "34250\n",
      "34300\n",
      "34350\n",
      "34400\n",
      "34450\n",
      "34500\n",
      "34550\n",
      "34600\n",
      "34650\n",
      "34700\n",
      "34750\n",
      "34800\n",
      "34850\n",
      "34900\n",
      "34950\n",
      "Epoch: 1/20...  Training Step: 700...  Training loss: 1.8798...  0.0740 sec/batch\n",
      "35000\n",
      "35050\n",
      "35100\n",
      "35150\n",
      "35200\n",
      "35250\n",
      "35300\n",
      "35350\n",
      "35400\n",
      "35450\n",
      "35500\n",
      "35550\n",
      "35600\n",
      "35650\n",
      "35700\n",
      "35750\n",
      "35800\n",
      "35850\n",
      "35900\n",
      "35950\n",
      "36000\n",
      "36050\n",
      "36100\n",
      "36150\n",
      "36200\n",
      "36250\n",
      "36300\n",
      "36350\n",
      "36400\n",
      "36450\n",
      "36500\n",
      "36550\n",
      "36600\n",
      "36650\n",
      "36700\n",
      "36750\n",
      "36800\n",
      "36850\n",
      "36900\n",
      "36950\n",
      "37000\n",
      "37050\n",
      "37100\n",
      "37150\n",
      "37200\n",
      "37250\n",
      "37300\n",
      "37350\n",
      "37400\n",
      "37450\n",
      "Epoch: 1/20...  Training Step: 750...  Training loss: 2.0261...  0.0838 sec/batch\n",
      "37500\n",
      "37550\n",
      "37600\n",
      "37650\n",
      "37700\n",
      "37750\n",
      "37800\n",
      "37850\n",
      "37900\n",
      "37950\n",
      "38000\n",
      "38050\n",
      "38100\n",
      "38150\n",
      "38200\n",
      "38250\n",
      "38300\n",
      "38350\n",
      "38400\n",
      "38450\n",
      "38500\n",
      "38550\n",
      "38600\n",
      "38650\n",
      "38700\n",
      "38750\n",
      "38800\n",
      "38850\n",
      "38900\n",
      "38950\n",
      "39000\n",
      "39050\n",
      "39100\n",
      "39150\n",
      "39200\n",
      "39250\n",
      "39300\n",
      "39350\n",
      "39400\n",
      "39450\n",
      "39500\n",
      "39550\n",
      "39600\n",
      "39650\n",
      "39700\n",
      "39750\n",
      "39800\n",
      "39850\n",
      "39900\n",
      "39950\n",
      "Epoch: 1/20...  Training Step: 800...  Training loss: 1.9925...  0.0746 sec/batch\n",
      "40000\n",
      "40050\n",
      "40100\n",
      "40150\n",
      "40200\n",
      "40250\n",
      "40300\n",
      "40350\n",
      "40400\n",
      "40450\n",
      "40500\n",
      "40550\n",
      "40600\n",
      "40650\n",
      "40700\n",
      "40750\n",
      "40800\n",
      "40850\n",
      "40900\n",
      "40950\n",
      "41000\n",
      "41050\n",
      "41100\n",
      "41150\n",
      "41200\n",
      "41250\n",
      "41300\n",
      "41350\n",
      "41400\n",
      "41450\n",
      "41500\n",
      "41550\n",
      "41600\n",
      "41650\n",
      "41700\n",
      "41750\n",
      "41800\n",
      "41850\n",
      "41900\n",
      "41950\n",
      "42000\n",
      "42050\n",
      "42100\n",
      "42150\n",
      "42200\n",
      "42250\n",
      "42300\n",
      "42350\n",
      "42400\n",
      "42450\n",
      "Epoch: 1/20...  Training Step: 850...  Training loss: 2.1400...  0.0869 sec/batch\n",
      "42500\n",
      "42550\n",
      "42600\n",
      "42650\n",
      "42700\n",
      "42750\n",
      "42800\n",
      "42850\n",
      "42900\n",
      "42950\n",
      "43000\n",
      "43050\n",
      "43100\n",
      "43150\n",
      "43200\n",
      "43250\n",
      "43300\n",
      "43350\n",
      "43400\n",
      "43450\n",
      "43500\n",
      "43550\n",
      "43600\n",
      "43650\n",
      "43700\n",
      "43750\n",
      "43800\n",
      "43850\n",
      "43900\n",
      "43950\n",
      "44000\n",
      "44050\n",
      "44100\n",
      "44150\n",
      "44200\n",
      "44250\n",
      "44300\n",
      "44350\n",
      "44400\n",
      "44450\n",
      "44500\n",
      "44550\n",
      "44600\n",
      "44650\n",
      "44700\n",
      "44750\n",
      "44800\n",
      "44850\n",
      "44900\n",
      "44950\n",
      "Epoch: 1/20...  Training Step: 900...  Training loss: 1.9442...  0.0775 sec/batch\n",
      "45000\n",
      "45050\n",
      "45100\n",
      "45150\n",
      "45200\n",
      "45250\n",
      "45300\n",
      "45350\n",
      "45400\n",
      "45450\n",
      "45500\n",
      "45550\n",
      "45600\n",
      "45650\n",
      "45700\n",
      "45750\n",
      "45800\n",
      "45850\n",
      "45900\n",
      "45950\n",
      "46000\n",
      "46050\n",
      "46100\n",
      "46150\n",
      "46200\n",
      "46250\n",
      "46300\n",
      "46350\n",
      "46400\n",
      "46450\n",
      "46500\n",
      "46550\n",
      "46600\n",
      "46650\n",
      "46700\n",
      "46750\n",
      "46800\n",
      "46850\n",
      "46900\n",
      "46950\n",
      "47000\n",
      "47050\n",
      "47100\n",
      "47150\n",
      "47200\n",
      "47250\n",
      "47300\n",
      "47350\n",
      "47400\n",
      "47450\n",
      "Epoch: 1/20...  Training Step: 950...  Training loss: 1.9596...  0.0966 sec/batch\n",
      "47500\n",
      "47550\n",
      "47600\n",
      "47650\n",
      "47700\n",
      "47750\n",
      "47800\n",
      "47850\n",
      "47900\n",
      "47950\n",
      "48000\n",
      "48050\n",
      "48100\n",
      "48150\n",
      "48200\n",
      "48250\n",
      "48300\n",
      "48350\n",
      "48400\n",
      "48450\n",
      "48500\n",
      "48550\n",
      "48600\n",
      "48650\n",
      "48700\n",
      "48750\n",
      "48800\n",
      "48850\n",
      "48900\n",
      "48950\n",
      "49000\n",
      "49050\n",
      "49100\n",
      "49150\n",
      "49200\n",
      "49250\n",
      "49300\n",
      "49350\n",
      "49400\n",
      "49450\n",
      "49500\n",
      "49550\n",
      "49600\n",
      "49650\n",
      "49700\n",
      "49750\n",
      "49800\n",
      "49850\n",
      "49900\n",
      "49950\n",
      "Epoch: 1/20...  Training Step: 1000...  Training loss: 1.8963...  0.1004 sec/batch\n",
      "50000\n",
      "50050\n",
      "50100\n",
      "50150\n",
      "50200\n",
      "50250\n",
      "50300\n",
      "50350\n",
      "50400\n",
      "50450\n",
      "50500\n",
      "50550\n",
      "50600\n",
      "50650\n",
      "50700\n",
      "50750\n",
      "50800\n",
      "50850\n",
      "50900\n",
      "50950\n",
      "51000\n",
      "51050\n",
      "51100\n",
      "51150\n",
      "51200\n",
      "51250\n",
      "51300\n",
      "51350\n",
      "51400\n",
      "51450\n",
      "51500\n",
      "51550\n",
      "51600\n",
      "51650\n",
      "51700\n",
      "51750\n",
      "51800\n",
      "51850\n",
      "51900\n",
      "51950\n",
      "52000\n",
      "52050\n",
      "52100\n",
      "52150\n",
      "52200\n",
      "52250\n",
      "52300\n",
      "52350\n",
      "52400\n",
      "52450\n",
      "Epoch: 1/20...  Training Step: 1050...  Training loss: 1.9121...  0.0786 sec/batch\n",
      "52500\n",
      "52550\n",
      "52600\n",
      "52650\n",
      "52700\n",
      "52750\n",
      "52800\n",
      "52850\n",
      "52900\n",
      "52950\n",
      "53000\n",
      "53050\n",
      "53100\n",
      "53150\n",
      "53200\n",
      "53250\n",
      "53300\n",
      "53350\n",
      "53400\n",
      "53450\n",
      "53500\n",
      "53550\n",
      "53600\n",
      "53650\n",
      "53700\n",
      "53750\n",
      "53800\n",
      "53850\n",
      "53900\n",
      "53950\n",
      "54000\n",
      "54050\n",
      "54100\n",
      "54150\n",
      "54200\n",
      "54250\n",
      "54300\n",
      "54350\n",
      "54400\n",
      "54450\n",
      "54500\n",
      "54550\n",
      "54600\n",
      "54650\n",
      "54700\n",
      "54750\n",
      "54800\n",
      "54850\n",
      "54900\n",
      "54950\n",
      "Epoch: 1/20...  Training Step: 1100...  Training loss: 1.9234...  0.1003 sec/batch\n",
      "55000\n",
      "55050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55100\n",
      "55150\n",
      "55200\n",
      "55250\n",
      "55300\n",
      "55350\n",
      "55400\n",
      "55450\n",
      "55500\n",
      "55550\n",
      "55600\n",
      "55650\n",
      "55700\n",
      "55750\n",
      "55800\n",
      "55850\n",
      "55900\n",
      "55950\n",
      "56000\n",
      "56050\n",
      "56100\n",
      "56150\n",
      "56200\n",
      "56250\n",
      "56300\n",
      "56350\n",
      "56400\n",
      "56450\n",
      "56500\n",
      "56550\n",
      "56600\n",
      "56650\n",
      "56700\n",
      "56750\n",
      "56800\n",
      "56850\n",
      "56900\n",
      "56950\n",
      "57000\n",
      "57050\n",
      "57100\n",
      "57150\n",
      "57200\n",
      "57250\n",
      "57300\n",
      "57350\n",
      "57400\n",
      "57450\n",
      "Epoch: 1/20...  Training Step: 1150...  Training loss: 1.9376...  0.0815 sec/batch\n",
      "57500\n",
      "57550\n",
      "57600\n",
      "57650\n",
      "57700\n",
      "57750\n",
      "57800\n",
      "57850\n",
      "57900\n",
      "57950\n",
      "58000\n",
      "58050\n",
      "58100\n",
      "58150\n",
      "58200\n",
      "58250\n",
      "58300\n",
      "58350\n",
      "58400\n",
      "58450\n",
      "58500\n",
      "58550\n",
      "58600\n",
      "58650\n",
      "58700\n",
      "58750\n",
      "58800\n",
      "58850\n",
      "58900\n",
      "58950\n",
      "59000\n",
      "59050\n",
      "59100\n",
      "59150\n",
      "59200\n",
      "59250\n",
      "59300\n",
      "59350\n",
      "59400\n",
      "59450\n",
      "59500\n",
      "59550\n",
      "59600\n",
      "59650\n",
      "59700\n",
      "59750\n",
      "59800\n",
      "59850\n",
      "59900\n",
      "59950\n",
      "Epoch: 1/20...  Training Step: 1200...  Training loss: 1.9200...  0.1014 sec/batch\n",
      "60000\n",
      "60050\n",
      "60100\n",
      "60150\n",
      "60200\n",
      "60250\n",
      "60300\n",
      "60350\n",
      "60400\n",
      "60450\n",
      "60500\n",
      "60550\n",
      "60600\n",
      "60650\n",
      "60700\n",
      "60750\n",
      "60800\n",
      "60850\n",
      "60900\n",
      "60950\n",
      "61000\n",
      "61050\n",
      "61100\n",
      "61150\n",
      "61200\n",
      "61250\n",
      "61300\n",
      "61350\n",
      "61400\n",
      "61450\n",
      "61500\n",
      "61550\n",
      "61600\n",
      "61650\n",
      "61700\n",
      "61750\n",
      "61800\n",
      "61850\n",
      "61900\n",
      "61950\n",
      "62000\n",
      "62050\n",
      "62100\n",
      "62150\n",
      "62200\n",
      "62250\n",
      "62300\n",
      "62350\n",
      "62400\n",
      "62450\n",
      "Epoch: 1/20...  Training Step: 1250...  Training loss: 1.7636...  0.1145 sec/batch\n",
      "62500\n",
      "62550\n",
      "62600\n",
      "62650\n",
      "62700\n",
      "62750\n",
      "62800\n",
      "62850\n",
      "62900\n",
      "62950\n",
      "63000\n",
      "63050\n",
      "63100\n",
      "63150\n",
      "63200\n",
      "63250\n",
      "63300\n",
      "63350\n",
      "63400\n",
      "63450\n",
      "63500\n",
      "63550\n",
      "63600\n",
      "63650\n",
      "63700\n",
      "63750\n",
      "63800\n",
      "63850\n",
      "63900\n",
      "63950\n",
      "64000\n",
      "64050\n",
      "64100\n",
      "64150\n",
      "64200\n",
      "64250\n",
      "64300\n",
      "64350\n",
      "64400\n",
      "64450\n",
      "64500\n",
      "64550\n",
      "64600\n",
      "64650\n",
      "64700\n",
      "64750\n",
      "64800\n",
      "64850\n",
      "64900\n",
      "64950\n",
      "Epoch: 1/20...  Training Step: 1300...  Training loss: 1.8708...  0.0736 sec/batch\n",
      "65000\n",
      "65050\n",
      "65100\n",
      "65150\n",
      "65200\n",
      "65250\n",
      "65300\n",
      "65350\n",
      "65400\n",
      "65450\n",
      "65500\n",
      "65550\n",
      "65600\n",
      "65650\n",
      "65700\n",
      "65750\n",
      "65800\n",
      "65850\n",
      "65900\n",
      "65950\n",
      "66000\n",
      "66050\n",
      "66100\n",
      "66150\n",
      "66200\n",
      "66250\n",
      "66300\n",
      "66350\n",
      "66400\n",
      "66450\n",
      "66500\n",
      "66550\n",
      "66600\n",
      "66650\n",
      "66700\n",
      "66750\n",
      "66800\n",
      "66850\n",
      "66900\n",
      "66950\n",
      "67000\n",
      "67050\n",
      "67100\n",
      "67150\n",
      "67200\n",
      "67250\n",
      "67300\n",
      "67350\n",
      "67400\n",
      "67450\n",
      "Epoch: 1/20...  Training Step: 1350...  Training loss: 1.7584...  0.1160 sec/batch\n",
      "67500\n",
      "67550\n",
      "67600\n",
      "67650\n",
      "67700\n",
      "67750\n",
      "67800\n",
      "67850\n",
      "67900\n",
      "67950\n",
      "68000\n",
      "68050\n",
      "68100\n",
      "68150\n",
      "68200\n",
      "68250\n",
      "68300\n",
      "68350\n",
      "68400\n",
      "68450\n",
      "68500\n",
      "68550\n",
      "68600\n",
      "68650\n",
      "68700\n",
      "68750\n",
      "68800\n",
      "68850\n",
      "68900\n",
      "68950\n",
      "69000\n",
      "69050\n",
      "69100\n",
      "69150\n",
      "69200\n",
      "69250\n",
      "69300\n",
      "69350\n",
      "69400\n",
      "69450\n",
      "69500\n",
      "69550\n",
      "69600\n",
      "69650\n",
      "69700\n",
      "69750\n",
      "69800\n",
      "69850\n",
      "69900\n",
      "69950\n",
      "Epoch: 1/20...  Training Step: 1400...  Training loss: 1.9793...  0.1141 sec/batch\n",
      "70000\n",
      "70050\n",
      "70100\n",
      "70150\n",
      "70200\n",
      "70250\n",
      "70300\n",
      "70350\n",
      "70400\n",
      "70450\n",
      "70500\n",
      "70550\n",
      "70600\n",
      "70650\n",
      "70700\n",
      "70750\n",
      "70800\n",
      "70850\n",
      "70900\n",
      "70950\n",
      "71000\n",
      "71050\n",
      "71100\n",
      "71150\n",
      "71200\n",
      "71250\n",
      "71300\n",
      "71350\n",
      "71400\n",
      "71450\n",
      "71500\n",
      "71550\n",
      "71600\n",
      "71650\n",
      "71700\n",
      "71750\n",
      "71800\n",
      "71850\n",
      "71900\n",
      "71950\n",
      "72000\n",
      "72050\n",
      "72100\n",
      "72150\n",
      "72200\n",
      "72250\n",
      "72300\n",
      "72350\n",
      "72400\n",
      "72450\n",
      "Epoch: 1/20...  Training Step: 1450...  Training loss: 1.8208...  0.0781 sec/batch\n",
      "72500\n",
      "72550\n",
      "72600\n",
      "72650\n",
      "72700\n",
      "72750\n",
      "72800\n",
      "72850\n",
      "72900\n",
      "72950\n",
      "73000\n",
      "73050\n",
      "73100\n",
      "73150\n",
      "73200\n",
      "73250\n",
      "73300\n",
      "73350\n",
      "73400\n",
      "73450\n",
      "73500\n",
      "73550\n",
      "73600\n",
      "73650\n",
      "73700\n",
      "73750\n",
      "73800\n",
      "73850\n",
      "73900\n",
      "73950\n",
      "74000\n",
      "74050\n",
      "74100\n",
      "74150\n",
      "74200\n",
      "74250\n",
      "74300\n",
      "74350\n",
      "74400\n",
      "74450\n",
      "74500\n",
      "74550\n",
      "74600\n",
      "74650\n",
      "74700\n",
      "74750\n",
      "74800\n",
      "74850\n",
      "74900\n",
      "74950\n",
      "Epoch: 1/20...  Training Step: 1500...  Training loss: 1.9555...  0.0954 sec/batch\n",
      "75000\n",
      "75050\n",
      "75100\n",
      "75150\n",
      "75200\n",
      "75250\n",
      "75300\n",
      "75350\n",
      "75400\n",
      "75450\n",
      "75500\n",
      "75550\n",
      "75600\n",
      "75650\n",
      "75700\n",
      "75750\n",
      "75800\n",
      "75850\n",
      "75900\n",
      "75950\n",
      "76000\n",
      "76050\n",
      "76100\n",
      "76150\n",
      "76200\n",
      "76250\n",
      "76300\n",
      "76350\n",
      "76400\n",
      "76450\n",
      "76500\n",
      "76550\n",
      "76600\n",
      "76650\n",
      "76700\n",
      "76750\n",
      "76800\n",
      "76850\n",
      "76900\n",
      "76950\n",
      "77000\n",
      "77050\n",
      "77100\n",
      "77150\n",
      "77200\n",
      "77250\n",
      "77300\n",
      "77350\n",
      "77400\n",
      "77450\n",
      "Epoch: 1/20...  Training Step: 1550...  Training loss: 1.7872...  0.0734 sec/batch\n",
      "77500\n",
      "77550\n",
      "77600\n",
      "77650\n",
      "77700\n",
      "77750\n",
      "77800\n",
      "77850\n",
      "77900\n",
      "77950\n",
      "78000\n",
      "78050\n",
      "78100\n",
      "78150\n",
      "78200\n",
      "78250\n",
      "78300\n",
      "78350\n",
      "78400\n",
      "78450\n",
      "78500\n",
      "78550\n",
      "78600\n",
      "78650\n",
      "78700\n",
      "78750\n",
      "78800\n",
      "78850\n",
      "78900\n",
      "78950\n",
      "79000\n",
      "79050\n",
      "79100\n",
      "79150\n",
      "79200\n",
      "79250\n",
      "79300\n",
      "79350\n",
      "79400\n",
      "79450\n",
      "79500\n",
      "79550\n",
      "79600\n",
      "79650\n",
      "79700\n",
      "79750\n",
      "79800\n",
      "79850\n",
      "79900\n",
      "79950\n",
      "Epoch: 1/20...  Training Step: 1600...  Training loss: 1.8779...  0.0763 sec/batch\n",
      "80000\n",
      "80050\n",
      "80100\n",
      "80150\n",
      "80200\n",
      "80250\n",
      "80300\n",
      "80350\n",
      "80400\n",
      "80450\n",
      "80500\n",
      "80550\n",
      "80600\n",
      "80650\n",
      "80700\n",
      "80750\n",
      "80800\n",
      "80850\n",
      "80900\n",
      "80950\n",
      "81000\n",
      "81050\n",
      "81100\n",
      "81150\n",
      "81200\n",
      "81250\n",
      "81300\n",
      "81350\n",
      "81400\n",
      "81450\n",
      "81500\n",
      "81550\n",
      "81600\n",
      "81650\n",
      "81700\n",
      "81750\n",
      "81800\n",
      "81850\n",
      "81900\n",
      "81950\n",
      "82000\n",
      "82050\n",
      "82100\n",
      "82150\n",
      "82200\n",
      "82250\n",
      "82300\n",
      "82350\n",
      "82400\n",
      "82450\n",
      "Epoch: 1/20...  Training Step: 1650...  Training loss: 1.7835...  0.0757 sec/batch\n",
      "82500\n",
      "82550\n",
      "82600\n",
      "82650\n",
      "82700\n",
      "82750\n",
      "82800\n",
      "82850\n",
      "82900\n",
      "82950\n",
      "83000\n",
      "83050\n",
      "83100\n",
      "83150\n",
      "83200\n",
      "83250\n",
      "83300\n",
      "83350\n",
      "83400\n",
      "83450\n",
      "83500\n",
      "83550\n",
      "83600\n",
      "83650\n",
      "83700\n",
      "83750\n",
      "83800\n",
      "83850\n",
      "83900\n",
      "83950\n",
      "84000\n",
      "84050\n",
      "84100\n",
      "84150\n",
      "84200\n",
      "84250\n",
      "84300\n",
      "84350\n",
      "84400\n",
      "84450\n",
      "84500\n",
      "84550\n",
      "84600\n",
      "84650\n",
      "84700\n",
      "84750\n",
      "84800\n",
      "84850\n",
      "84900\n",
      "84950\n",
      "Epoch: 1/20...  Training Step: 1700...  Training loss: 1.8394...  0.0742 sec/batch\n",
      "85000\n",
      "85050\n",
      "85100\n",
      "85150\n",
      "85200\n",
      "85250\n",
      "85300\n",
      "85350\n",
      "85400\n",
      "85450\n",
      "85500\n",
      "85550\n",
      "85600\n",
      "85650\n",
      "85700\n",
      "85750\n",
      "85800\n",
      "85850\n",
      "85900\n",
      "85950\n",
      "86000\n",
      "86050\n",
      "86100\n",
      "86150\n",
      "86200\n",
      "86250\n",
      "86300\n",
      "86350\n",
      "86400\n",
      "86450\n",
      "86500\n",
      "86550\n",
      "86600\n",
      "86650\n",
      "86700\n",
      "86750\n",
      "86800\n",
      "86850\n",
      "86900\n",
      "86950\n",
      "87000\n",
      "87050\n",
      "87100\n",
      "87150\n",
      "87200\n",
      "87250\n",
      "87300\n",
      "87350\n",
      "87400\n",
      "87450\n",
      "Epoch: 1/20...  Training Step: 1750...  Training loss: 1.8183...  0.0810 sec/batch\n",
      "87500\n",
      "87550\n",
      "87600\n",
      "87650\n",
      "87700\n",
      "87750\n",
      "87800\n",
      "87850\n",
      "87900\n",
      "87950\n",
      "88000\n",
      "88050\n",
      "88100\n",
      "88150\n",
      "88200\n",
      "88250\n",
      "88300\n",
      "88350\n",
      "88400\n",
      "88450\n",
      "88500\n",
      "88550\n",
      "88600\n",
      "88650\n",
      "88700\n",
      "88750\n",
      "88800\n",
      "88850\n",
      "88900\n",
      "88950\n",
      "89000\n",
      "89050\n",
      "89100\n",
      "89150\n",
      "89200\n",
      "89250\n",
      "89300\n",
      "89350\n",
      "89400\n",
      "89450\n",
      "89500\n",
      "89550\n",
      "89600\n",
      "89650\n",
      "89700\n",
      "89750\n",
      "89800\n",
      "89850\n",
      "89900\n",
      "89950\n",
      "Epoch: 1/20...  Training Step: 1800...  Training loss: 1.8659...  0.0745 sec/batch\n",
      "90000\n",
      "90050\n",
      "90100\n",
      "90150\n",
      "90200\n",
      "90250\n",
      "90300\n",
      "90350\n",
      "90400\n",
      "90450\n",
      "90500\n",
      "90550\n",
      "90600\n",
      "90650\n",
      "90700\n",
      "90750\n",
      "90800\n",
      "90850\n",
      "90900\n",
      "90950\n",
      "91000\n",
      "91050\n",
      "91100\n",
      "91150\n",
      "91200\n",
      "91250\n",
      "91300\n",
      "91350\n",
      "91400\n",
      "91450\n",
      "91500\n",
      "91550\n",
      "91600\n",
      "91650\n",
      "91700\n",
      "91750\n",
      "91800\n",
      "91850\n",
      "91900\n",
      "91950\n",
      "92000\n",
      "92050\n",
      "92100\n",
      "92150\n",
      "92200\n",
      "92250\n",
      "92300\n",
      "92350\n",
      "92400\n",
      "92450\n",
      "Epoch: 1/20...  Training Step: 1850...  Training loss: 1.9045...  0.1077 sec/batch\n",
      "92500\n",
      "92550\n",
      "92600\n",
      "92650\n",
      "92700\n",
      "92750\n",
      "92800\n",
      "92850\n",
      "92900\n",
      "92950\n",
      "93000\n",
      "93050\n",
      "93100\n",
      "93150\n",
      "93200\n",
      "93250\n",
      "93300\n",
      "93350\n",
      "93400\n",
      "93450\n",
      "93500\n",
      "93550\n",
      "93600\n",
      "93650\n",
      "93700\n",
      "93750\n",
      "93800\n",
      "93850\n",
      "93900\n",
      "93950\n",
      "94000\n",
      "94050\n",
      "94100\n",
      "94150\n",
      "94200\n",
      "94250\n",
      "94300\n",
      "94350\n",
      "94400\n",
      "94450\n",
      "94500\n",
      "94550\n",
      "94600\n",
      "94650\n",
      "94700\n",
      "94750\n",
      "94800\n",
      "94850\n",
      "94900\n",
      "94950\n",
      "Epoch: 1/20...  Training Step: 1900...  Training loss: 1.7359...  0.0936 sec/batch\n",
      "95000\n",
      "95050\n",
      "95100\n",
      "95150\n",
      "95200\n",
      "95250\n",
      "95300\n",
      "95350\n",
      "95400\n",
      "95450\n",
      "95500\n",
      "95550\n",
      "95600\n",
      "95650\n",
      "95700\n",
      "95750\n",
      "95800\n",
      "95850\n",
      "95900\n",
      "95950\n",
      "96000\n",
      "96050\n",
      "96100\n",
      "96150\n",
      "96200\n",
      "96250\n",
      "96300\n",
      "96350\n",
      "96400\n",
      "96450\n",
      "96500\n",
      "96550\n",
      "96600\n",
      "96650\n",
      "96700\n",
      "96750\n",
      "96800\n",
      "96850\n",
      "96900\n",
      "96950\n",
      "97000\n",
      "97050\n",
      "97100\n",
      "97150\n",
      "97200\n",
      "97250\n",
      "97300\n",
      "97350\n",
      "97400\n",
      "97450\n",
      "Epoch: 1/20...  Training Step: 1950...  Training loss: 1.7171...  0.0803 sec/batch\n",
      "97500\n",
      "97550\n",
      "97600\n",
      "97650\n",
      "97700\n",
      "97750\n",
      "97800\n",
      "97850\n",
      "97900\n",
      "97950\n",
      "98000\n",
      "98050\n",
      "98100\n",
      "98150\n",
      "98200\n",
      "98250\n",
      "98300\n",
      "98350\n",
      "98400\n",
      "98450\n",
      "98500\n",
      "98550\n",
      "98600\n",
      "98650\n",
      "98700\n",
      "98750\n",
      "98800\n",
      "98850\n",
      "98900\n",
      "98950\n",
      "99000\n",
      "99050\n",
      "99100\n",
      "99150\n",
      "99200\n",
      "99250\n",
      "99300\n",
      "99350\n",
      "99400\n",
      "99450\n",
      "99500\n",
      "99550\n",
      "99600\n",
      "99650\n",
      "99700\n",
      "99750\n",
      "99800\n",
      "99850\n",
      "99900\n",
      "99950\n",
      "Epoch: 1/20...  Training Step: 2000...  Training loss: 1.8285...  0.0823 sec/batch\n",
      "100000\n",
      "100050\n",
      "100100\n",
      "100150\n",
      "100200\n",
      "100250\n",
      "100300\n",
      "100350\n",
      "100400\n",
      "100450\n",
      "100500\n",
      "100550\n",
      "100600\n",
      "100650\n",
      "100700\n",
      "100750\n",
      "100800\n",
      "100850\n",
      "100900\n",
      "100950\n",
      "101000\n",
      "101050\n",
      "101100\n",
      "101150\n",
      "101200\n",
      "101250\n",
      "101300\n",
      "101350\n",
      "101400\n",
      "101450\n",
      "101500\n",
      "101550\n",
      "101600\n",
      "101650\n",
      "101700\n",
      "101750\n",
      "101800\n",
      "101850\n",
      "101900\n",
      "101950\n",
      "102000\n",
      "102050\n",
      "102100\n",
      "102150\n",
      "102200\n",
      "102250\n",
      "102300\n",
      "102350\n",
      "102400\n",
      "102450\n",
      "Epoch: 1/20...  Training Step: 2050...  Training loss: 1.9319...  0.0844 sec/batch\n",
      "102500\n",
      "102550\n",
      "102600\n",
      "102650\n",
      "102700\n",
      "102750\n",
      "102800\n",
      "102850\n",
      "102900\n",
      "102950\n",
      "103000\n",
      "103050\n",
      "103100\n",
      "103150\n",
      "103200\n",
      "103250\n",
      "103300\n",
      "103350\n",
      "103400\n",
      "103450\n",
      "103500\n",
      "103550\n",
      "103600\n",
      "103650\n",
      "103700\n",
      "103750\n",
      "103800\n",
      "103850\n",
      "103900\n",
      "103950\n",
      "104000\n",
      "104050\n",
      "104100\n",
      "104150\n",
      "104200\n",
      "104250\n",
      "104300\n",
      "104350\n",
      "104400\n",
      "104450\n",
      "104500\n",
      "104550\n",
      "104600\n",
      "104650\n",
      "104700\n",
      "104750\n",
      "104800\n",
      "104850\n",
      "104900\n",
      "104950\n",
      "Epoch: 1/20...  Training Step: 2100...  Training loss: 1.7517...  0.1011 sec/batch\n",
      "105000\n",
      "105050\n",
      "105100\n",
      "105150\n",
      "105200\n",
      "105250\n",
      "105300\n",
      "105350\n",
      "105400\n",
      "105450\n",
      "105500\n",
      "105550\n",
      "105600\n",
      "105650\n",
      "105700\n",
      "105750\n",
      "105800\n",
      "105850\n",
      "105900\n",
      "105950\n",
      "106000\n",
      "106050\n",
      "106100\n",
      "106150\n",
      "106200\n",
      "106250\n",
      "106300\n",
      "106350\n",
      "106400\n",
      "106450\n",
      "106500\n",
      "106550\n",
      "106600\n",
      "106650\n",
      "106700\n",
      "106750\n",
      "106800\n",
      "106850\n",
      "106900\n",
      "106950\n",
      "107000\n",
      "107050\n",
      "107100\n",
      "107150\n",
      "107200\n",
      "107250\n",
      "107300\n",
      "107350\n",
      "107400\n",
      "107450\n",
      "Epoch: 1/20...  Training Step: 2150...  Training loss: 1.7780...  0.1042 sec/batch\n",
      "107500\n",
      "107550\n",
      "107600\n",
      "107650\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107700\n",
      "107750\n",
      "107800\n",
      "107850\n",
      "107900\n",
      "107950\n",
      "108000\n",
      "108050\n",
      "108100\n",
      "108150\n",
      "108200\n",
      "108250\n",
      "108300\n",
      "108350\n",
      "108400\n",
      "108450\n",
      "108500\n",
      "108550\n",
      "108600\n",
      "108650\n",
      "108700\n",
      "108750\n",
      "108800\n",
      "108850\n",
      "108900\n",
      "108950\n",
      "109000\n",
      "109050\n",
      "109100\n",
      "109150\n",
      "109200\n",
      "109250\n",
      "109300\n",
      "109350\n",
      "109400\n",
      "109450\n",
      "109500\n",
      "109550\n",
      "109600\n",
      "109650\n",
      "109700\n",
      "109750\n",
      "109800\n",
      "109850\n",
      "109900\n",
      "109950\n",
      "Epoch: 1/20...  Training Step: 2200...  Training loss: 1.7151...  0.1089 sec/batch\n",
      "110000\n",
      "110050\n",
      "110100\n",
      "110150\n",
      "110200\n",
      "110250\n",
      "110300\n",
      "110350\n",
      "110400\n",
      "110450\n",
      "110500\n",
      "110550\n",
      "110600\n",
      "110650\n",
      "110700\n",
      "110750\n",
      "110800\n",
      "110850\n",
      "110900\n",
      "110950\n",
      "111000\n",
      "111050\n",
      "111100\n",
      "111150\n",
      "111200\n",
      "111250\n",
      "111300\n",
      "111350\n",
      "111400\n",
      "111450\n",
      "111500\n",
      "111550\n",
      "111600\n",
      "111650\n",
      "111700\n",
      "111750\n",
      "111800\n",
      "111850\n",
      "111900\n",
      "111950\n",
      "112000\n",
      "112050\n",
      "112100\n",
      "112150\n",
      "112200\n",
      "112250\n",
      "112300\n",
      "112350\n",
      "112400\n",
      "112450\n",
      "Epoch: 1/20...  Training Step: 2250...  Training loss: 1.7354...  0.1145 sec/batch\n",
      "112500\n",
      "112550\n",
      "112600\n",
      "112650\n",
      "112700\n",
      "112750\n",
      "112800\n",
      "112850\n",
      "112900\n",
      "112950\n",
      "113000\n",
      "113050\n",
      "113100\n",
      "113150\n",
      "113200\n",
      "113250\n",
      "113300\n",
      "113350\n",
      "113400\n",
      "113450\n",
      "113500\n",
      "113550\n",
      "113600\n",
      "113650\n",
      "113700\n",
      "113750\n",
      "113800\n",
      "113850\n",
      "113900\n",
      "113950\n",
      "114000\n",
      "114050\n",
      "114100\n",
      "114150\n",
      "114200\n",
      "114250\n",
      "114300\n",
      "114350\n",
      "114400\n",
      "114450\n",
      "114500\n",
      "114550\n",
      "114600\n",
      "114650\n",
      "114700\n",
      "114750\n",
      "114800\n",
      "114850\n",
      "114900\n",
      "114950\n",
      "Epoch: 1/20...  Training Step: 2300...  Training loss: 1.6863...  0.0910 sec/batch\n",
      "115000\n",
      "115050\n",
      "115100\n",
      "115150\n",
      "115200\n",
      "115250\n",
      "115300\n",
      "115350\n",
      "115400\n",
      "115450\n",
      "115500\n",
      "115550\n",
      "115600\n",
      "115650\n",
      "115700\n",
      "115750\n",
      "115800\n",
      "115850\n",
      "115900\n",
      "115950\n",
      "116000\n",
      "116050\n",
      "116100\n",
      "116150\n",
      "116200\n",
      "116250\n",
      "116300\n",
      "116350\n",
      "116400\n",
      "116450\n",
      "116500\n",
      "116550\n",
      "116600\n",
      "116650\n",
      "116700\n",
      "116750\n",
      "116800\n",
      "116850\n",
      "116900\n",
      "116950\n",
      "117000\n",
      "117050\n",
      "117100\n",
      "117150\n",
      "117200\n",
      "117250\n",
      "117300\n",
      "117350\n",
      "117400\n",
      "117450\n",
      "Epoch: 1/20...  Training Step: 2350...  Training loss: 1.8872...  0.0968 sec/batch\n",
      "117500\n",
      "117550\n",
      "117600\n",
      "117650\n",
      "117700\n",
      "117750\n",
      "117800\n",
      "117850\n",
      "117900\n",
      "117950\n",
      "118000\n",
      "118050\n",
      "118100\n",
      "118150\n",
      "118200\n",
      "118250\n",
      "118300\n",
      "118350\n",
      "118400\n",
      "118450\n",
      "118500\n",
      "118550\n",
      "118600\n",
      "118650\n",
      "118700\n",
      "118750\n",
      "118800\n",
      "118850\n",
      "118900\n",
      "118950\n",
      "119000\n",
      "119050\n",
      "119100\n",
      "119150\n",
      "119200\n",
      "119250\n",
      "119300\n",
      "119350\n",
      "119400\n",
      "119450\n",
      "119500\n",
      "119550\n",
      "119600\n",
      "119650\n",
      "119700\n",
      "119750\n",
      "119800\n",
      "119850\n",
      "119900\n",
      "119950\n",
      "Epoch: 1/20...  Training Step: 2400...  Training loss: 1.9456...  0.0812 sec/batch\n",
      "120000\n",
      "120050\n",
      "120100\n",
      "120150\n",
      "120200\n",
      "120250\n",
      "120300\n",
      "120350\n",
      "120400\n",
      "120450\n",
      "120500\n",
      "120550\n",
      "120600\n",
      "120650\n",
      "120700\n",
      "120750\n",
      "120800\n",
      "120850\n",
      "120900\n",
      "120950\n",
      "121000\n",
      "121050\n",
      "121100\n",
      "121150\n",
      "121200\n",
      "121250\n",
      "121300\n",
      "121350\n",
      "121400\n",
      "121450\n",
      "121500\n",
      "121550\n",
      "121600\n",
      "121650\n",
      "121700\n",
      "121750\n",
      "121800\n",
      "121850\n",
      "121900\n",
      "121950\n",
      "122000\n",
      "122050\n",
      "122100\n",
      "122150\n",
      "122200\n",
      "122250\n",
      "122300\n",
      "122350\n",
      "122400\n",
      "122450\n",
      "Epoch: 1/20...  Training Step: 2450...  Training loss: 1.9690...  0.0778 sec/batch\n",
      "122500\n",
      "122550\n",
      "122600\n",
      "122650\n",
      "122700\n",
      "122750\n",
      "122800\n",
      "122850\n",
      "122900\n",
      "122950\n",
      "123000\n",
      "123050\n",
      "123100\n",
      "123150\n",
      "123200\n",
      "123250\n",
      "123300\n",
      "123350\n",
      "123400\n",
      "123450\n",
      "123500\n",
      "123550\n",
      "123600\n",
      "123650\n",
      "123700\n",
      "123750\n",
      "123800\n",
      "123850\n",
      "123900\n",
      "123950\n",
      "124000\n",
      "124050\n",
      "124100\n",
      "124150\n",
      "124200\n",
      "124250\n",
      "124300\n",
      "124350\n",
      "124400\n",
      "124450\n",
      "124500\n",
      "124550\n",
      "124600\n",
      "124650\n",
      "124700\n",
      "124750\n",
      "124800\n",
      "124850\n",
      "124900\n",
      "124950\n",
      "Epoch: 1/20...  Training Step: 2500...  Training loss: 1.7318...  0.0829 sec/batch\n",
      "125000\n",
      "125050\n",
      "125100\n",
      "125150\n",
      "125200\n",
      "125250\n",
      "125300\n",
      "125350\n",
      "125400\n",
      "125450\n",
      "125500\n",
      "125550\n",
      "125600\n",
      "125650\n",
      "125700\n",
      "125750\n",
      "125800\n",
      "125850\n",
      "125900\n",
      "125950\n",
      "126000\n",
      "126050\n",
      "126100\n",
      "126150\n",
      "126200\n",
      "126250\n",
      "126300\n",
      "126350\n",
      "126400\n",
      "126450\n",
      "126500\n",
      "126550\n",
      "126600\n",
      "126650\n",
      "126700\n",
      "126750\n",
      "126800\n",
      "126850\n",
      "126900\n",
      "126950\n",
      "127000\n",
      "127050\n",
      "127100\n",
      "127150\n",
      "127200\n",
      "127250\n",
      "127300\n",
      "127350\n",
      "127400\n",
      "127450\n",
      "Epoch: 1/20...  Training Step: 2550...  Training loss: 1.8174...  0.0912 sec/batch\n",
      "127500\n",
      "127550\n",
      "127600\n",
      "127650\n",
      "127700\n",
      "127750\n",
      "127800\n",
      "127850\n",
      "127900\n",
      "127950\n",
      "128000\n",
      "128050\n",
      "128100\n",
      "128150\n",
      "128200\n",
      "128250\n",
      "128300\n",
      "128350\n",
      "128400\n",
      "128450\n",
      "128500\n",
      "128550\n",
      "128600\n",
      "128650\n",
      "128700\n",
      "128750\n",
      "128800\n",
      "128850\n",
      "128900\n",
      "128950\n",
      "129000\n",
      "129050\n",
      "129100\n",
      "129150\n",
      "129200\n",
      "129250\n",
      "129300\n",
      "129350\n",
      "129400\n",
      "129450\n",
      "129500\n",
      "129550\n",
      "129600\n",
      "129650\n",
      "129700\n",
      "129750\n",
      "129800\n",
      "129850\n",
      "129900\n",
      "129950\n",
      "Epoch: 1/20...  Training Step: 2600...  Training loss: 1.6984...  0.1040 sec/batch\n",
      "130000\n",
      "130050\n",
      "130100\n",
      "130150\n",
      "130200\n",
      "130250\n",
      "130300\n",
      "130350\n",
      "130400\n",
      "130450\n",
      "130500\n",
      "130550\n",
      "130600\n",
      "130650\n",
      "130700\n",
      "130750\n",
      "130800\n",
      "130850\n",
      "130900\n",
      "130950\n",
      "131000\n",
      "131050\n",
      "131100\n",
      "131150\n",
      "131200\n",
      "131250\n",
      "131300\n",
      "131350\n",
      "131400\n",
      "131450\n",
      "131500\n",
      "131550\n",
      "131600\n",
      "131650\n",
      "131700\n",
      "131750\n",
      "131800\n",
      "131850\n",
      "131900\n",
      "131950\n",
      "132000\n",
      "132050\n",
      "132100\n",
      "132150\n",
      "132200\n",
      "132250\n",
      "132300\n",
      "132350\n",
      "132400\n",
      "132450\n",
      "Epoch: 1/20...  Training Step: 2650...  Training loss: 1.9089...  0.1270 sec/batch\n",
      "132500\n",
      "132550\n",
      "132600\n",
      "132650\n",
      "132700\n",
      "132750\n",
      "132800\n",
      "132850\n",
      "132900\n",
      "132950\n",
      "133000\n",
      "133050\n",
      "133100\n",
      "133150\n",
      "133200\n",
      "133250\n",
      "133300\n",
      "133350\n",
      "133400\n",
      "133450\n",
      "133500\n",
      "133550\n",
      "133600\n",
      "133650\n",
      "133700\n",
      "133750\n",
      "133800\n",
      "133850\n",
      "133900\n",
      "133950\n",
      "134000\n",
      "134050\n",
      "134100\n",
      "134150\n",
      "134200\n",
      "134250\n",
      "134300\n",
      "134350\n",
      "134400\n",
      "134450\n",
      "134500\n",
      "134550\n",
      "134600\n",
      "134650\n",
      "134700\n",
      "134750\n",
      "134800\n",
      "134850\n",
      "134900\n",
      "134950\n",
      "Epoch: 1/20...  Training Step: 2700...  Training loss: 1.8535...  0.0951 sec/batch\n",
      "135000\n",
      "135050\n",
      "135100\n",
      "135150\n",
      "135200\n",
      "135250\n",
      "135300\n",
      "135350\n",
      "135400\n",
      "135450\n",
      "135500\n",
      "135550\n",
      "135600\n",
      "135650\n",
      "135700\n",
      "135750\n",
      "135800\n",
      "135850\n",
      "135900\n",
      "135950\n",
      "136000\n",
      "136050\n",
      "136100\n",
      "136150\n",
      "136200\n",
      "136250\n",
      "136300\n",
      "136350\n",
      "136400\n",
      "136450\n",
      "136500\n",
      "136550\n",
      "136600\n",
      "136650\n",
      "136700\n",
      "136750\n",
      "136800\n",
      "136850\n",
      "136900\n",
      "136950\n",
      "137000\n",
      "137050\n",
      "137100\n",
      "137150\n",
      "137200\n",
      "137250\n",
      "137300\n",
      "137350\n",
      "137400\n",
      "137450\n",
      "Epoch: 1/20...  Training Step: 2750...  Training loss: 1.6700...  0.1276 sec/batch\n",
      "137500\n",
      "137550\n",
      "137600\n",
      "137650\n",
      "137700\n",
      "137750\n",
      "137800\n",
      "137850\n",
      "137900\n",
      "137950\n",
      "138000\n",
      "138050\n",
      "138100\n",
      "138150\n",
      "138200\n",
      "138250\n",
      "138300\n",
      "138350\n",
      "138400\n",
      "138450\n",
      "138500\n",
      "138550\n",
      "138600\n",
      "138650\n",
      "138700\n",
      "138750\n",
      "138800\n",
      "138850\n",
      "138900\n",
      "138950\n",
      "139000\n",
      "139050\n",
      "139100\n",
      "139150\n",
      "139200\n",
      "139250\n",
      "139300\n",
      "139350\n",
      "139400\n",
      "139450\n",
      "139500\n",
      "139550\n",
      "139600\n",
      "139650\n",
      "139700\n",
      "139750\n",
      "139800\n",
      "139850\n",
      "139900\n",
      "139950\n",
      "Epoch: 1/20...  Training Step: 2800...  Training loss: 1.6527...  0.1524 sec/batch\n",
      "140000\n",
      "140050\n",
      "140100\n",
      "140150\n",
      "140200\n",
      "140250\n",
      "140300\n",
      "140350\n",
      "140400\n",
      "140450\n",
      "140500\n",
      "140550\n",
      "140600\n",
      "140650\n",
      "140700\n",
      "140750\n",
      "140800\n",
      "140850\n",
      "140900\n",
      "140950\n",
      "141000\n",
      "141050\n",
      "141100\n",
      "141150\n",
      "141200\n",
      "141250\n",
      "141300\n",
      "141350\n",
      "141400\n",
      "141450\n",
      "141500\n",
      "141550\n",
      "141600\n",
      "141650\n",
      "141700\n",
      "141750\n",
      "141800\n",
      "141850\n",
      "141900\n",
      "141950\n",
      "142000\n",
      "142050\n",
      "142100\n",
      "142150\n",
      "142200\n",
      "142250\n",
      "142300\n",
      "142350\n",
      "142400\n",
      "142450\n",
      "Epoch: 1/20...  Training Step: 2850...  Training loss: 1.8718...  0.0912 sec/batch\n",
      "142500\n",
      "142550\n",
      "142600\n",
      "142650\n",
      "142700\n",
      "142750\n",
      "142800\n",
      "142850\n",
      "142900\n",
      "142950\n",
      "143000\n",
      "143050\n",
      "143100\n",
      "143150\n",
      "143200\n",
      "143250\n",
      "143300\n",
      "143350\n",
      "143400\n",
      "143450\n",
      "143500\n",
      "143550\n",
      "143600\n",
      "143650\n",
      "143700\n",
      "143750\n",
      "143800\n",
      "143850\n",
      "143900\n",
      "143950\n",
      "144000\n",
      "144050\n",
      "144100\n",
      "144150\n",
      "144200\n",
      "144250\n",
      "144300\n",
      "144350\n",
      "144400\n",
      "144450\n",
      "144500\n",
      "144550\n",
      "144600\n",
      "144650\n",
      "144700\n",
      "144750\n",
      "144800\n",
      "144850\n",
      "144900\n",
      "144950\n",
      "Epoch: 1/20...  Training Step: 2900...  Training loss: 1.5927...  0.1045 sec/batch\n",
      "145000\n",
      "145050\n",
      "145100\n",
      "145150\n",
      "145200\n",
      "145250\n",
      "145300\n",
      "145350\n",
      "145400\n",
      "145450\n",
      "145500\n",
      "145550\n",
      "145600\n",
      "145650\n",
      "145700\n",
      "145750\n",
      "145800\n",
      "145850\n",
      "145900\n",
      "145950\n",
      "146000\n",
      "146050\n",
      "146100\n",
      "146150\n",
      "146200\n",
      "146250\n",
      "146300\n",
      "146350\n",
      "146400\n",
      "146450\n",
      "146500\n",
      "146550\n",
      "146600\n",
      "146650\n",
      "146700\n",
      "146750\n",
      "146800\n",
      "146850\n",
      "146900\n",
      "146950\n",
      "147000\n",
      "147050\n",
      "147100\n",
      "147150\n",
      "147200\n",
      "147250\n",
      "147300\n",
      "147350\n",
      "147400\n",
      "147450\n",
      "Epoch: 1/20...  Training Step: 2950...  Training loss: 1.5595...  0.1311 sec/batch\n",
      "147500\n",
      "147550\n",
      "147600\n",
      "147650\n",
      "147700\n",
      "147750\n",
      "147800\n",
      "147850\n",
      "147900\n",
      "147950\n",
      "148000\n",
      "148050\n",
      "148100\n",
      "148150\n",
      "148200\n",
      "148250\n",
      "148300\n",
      "148350\n",
      "148400\n",
      "148450\n",
      "148500\n",
      "148550\n",
      "148600\n",
      "148650\n",
      "148700\n",
      "148750\n",
      "148800\n",
      "148850\n",
      "148900\n",
      "148950\n",
      "149000\n",
      "149050\n",
      "149100\n",
      "149150\n",
      "149200\n",
      "149250\n",
      "149300\n",
      "149350\n",
      "149400\n",
      "149450\n",
      "149500\n",
      "149550\n",
      "149600\n",
      "149650\n",
      "149700\n",
      "149750\n",
      "149800\n",
      "149850\n",
      "149900\n",
      "149950\n",
      "Epoch: 1/20...  Training Step: 3000...  Training loss: 1.7599...  0.1019 sec/batch\n",
      "150000\n",
      "150050\n",
      "150100\n",
      "150150\n",
      "150200\n",
      "150250\n",
      "150300\n",
      "150350\n",
      "150400\n",
      "150450\n",
      "150500\n",
      "150550\n",
      "150600\n",
      "150650\n",
      "150700\n",
      "150750\n",
      "150800\n",
      "150850\n",
      "150900\n",
      "150950\n",
      "151000\n",
      "151050\n",
      "151100\n",
      "151150\n",
      "151200\n",
      "151250\n",
      "151300\n",
      "151350\n",
      "151400\n",
      "151450\n",
      "151500\n",
      "151550\n",
      "151600\n",
      "151650\n",
      "151700\n",
      "151750\n",
      "151800\n",
      "151850\n",
      "151900\n",
      "151950\n",
      "152000\n",
      "152050\n",
      "152100\n",
      "152150\n",
      "152200\n",
      "152250\n",
      "152300\n",
      "152350\n",
      "152400\n",
      "152450\n",
      "Epoch: 1/20...  Training Step: 3050...  Training loss: 1.8687...  0.1159 sec/batch\n",
      "152500\n",
      "152550\n",
      "152600\n",
      "152650\n",
      "152700\n",
      "152750\n",
      "152800\n",
      "152850\n",
      "152900\n",
      "152950\n",
      "153000\n",
      "153050\n",
      "153100\n",
      "153150\n",
      "153200\n",
      "153250\n",
      "153300\n",
      "153350\n",
      "153400\n",
      "153450\n",
      "153500\n",
      "153550\n",
      "153600\n",
      "153650\n",
      "153700\n",
      "153750\n",
      "153800\n",
      "153850\n",
      "153900\n",
      "153950\n",
      "154000\n",
      "154050\n",
      "154100\n",
      "154150\n",
      "154200\n",
      "154250\n",
      "154300\n",
      "154350\n",
      "154400\n",
      "154450\n",
      "154500\n",
      "154550\n",
      "154600\n",
      "154650\n",
      "154700\n",
      "154750\n",
      "154800\n",
      "154850\n",
      "154900\n",
      "154950\n",
      "Epoch: 1/20...  Training Step: 3100...  Training loss: 1.6838...  0.0817 sec/batch\n",
      "155000\n",
      "155050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "155100\n",
      "155150\n",
      "155200\n",
      "155250\n",
      "155300\n",
      "155350\n",
      "155400\n",
      "155450\n",
      "155500\n",
      "155550\n",
      "155600\n",
      "155650\n",
      "155700\n",
      "155750\n",
      "155800\n",
      "155850\n",
      "155900\n",
      "155950\n",
      "156000\n",
      "156050\n",
      "156100\n",
      "156150\n",
      "156200\n",
      "156250\n",
      "156300\n",
      "156350\n",
      "156400\n",
      "156450\n",
      "156500\n",
      "156550\n",
      "156600\n",
      "156650\n",
      "156700\n",
      "156750\n",
      "156800\n",
      "156850\n",
      "156900\n",
      "156950\n",
      "157000\n",
      "157050\n",
      "157100\n",
      "157150\n",
      "157200\n",
      "157250\n",
      "157300\n",
      "157350\n",
      "157400\n",
      "157450\n",
      "Epoch: 1/20...  Training Step: 3150...  Training loss: 1.6362...  0.0764 sec/batch\n",
      "157500\n",
      "157550\n",
      "157600\n",
      "157650\n",
      "157700\n",
      "157750\n",
      "157800\n",
      "157850\n",
      "157900\n",
      "157950\n",
      "158000\n",
      "158050\n",
      "158100\n",
      "158150\n",
      "158200\n",
      "158250\n",
      "158300\n",
      "158350\n",
      "158400\n",
      "158450\n",
      "158500\n",
      "158550\n",
      "158600\n",
      "158650\n",
      "158700\n",
      "158750\n",
      "158800\n",
      "158850\n",
      "158900\n",
      "158950\n",
      "159000\n",
      "159050\n",
      "159100\n",
      "159150\n",
      "159200\n",
      "159250\n",
      "159300\n",
      "159350\n",
      "159400\n",
      "159450\n",
      "159500\n",
      "159550\n",
      "159600\n",
      "159650\n",
      "159700\n",
      "159750\n",
      "159800\n",
      "159850\n",
      "159900\n",
      "159950\n",
      "Epoch: 1/20...  Training Step: 3200...  Training loss: 1.8684...  0.0798 sec/batch\n",
      "160000\n",
      "160050\n",
      "160100\n",
      "160150\n",
      "160200\n",
      "160250\n",
      "160300\n",
      "160350\n",
      "160400\n",
      "160450\n",
      "160500\n",
      "160550\n",
      "160600\n",
      "160650\n",
      "160700\n",
      "160750\n",
      "160800\n",
      "160850\n",
      "160900\n",
      "160950\n",
      "161000\n",
      "161050\n",
      "161100\n",
      "161150\n",
      "161200\n",
      "161250\n",
      "161300\n",
      "161350\n",
      "161400\n",
      "161450\n",
      "161500\n",
      "161550\n",
      "161600\n",
      "161650\n",
      "161700\n",
      "161750\n",
      "161800\n",
      "161850\n",
      "161900\n",
      "161950\n",
      "162000\n",
      "162050\n",
      "162100\n",
      "162150\n",
      "162200\n",
      "162250\n",
      "162300\n",
      "162350\n",
      "162400\n",
      "162450\n",
      "Epoch: 1/20...  Training Step: 3250...  Training loss: 1.7678...  0.0796 sec/batch\n",
      "162500\n",
      "162550\n",
      "162600\n",
      "162650\n",
      "162700\n",
      "162750\n",
      "162800\n",
      "162850\n",
      "162900\n",
      "162950\n",
      "163000\n",
      "163050\n",
      "163100\n",
      "163150\n",
      "163200\n",
      "163250\n",
      "163300\n",
      "163350\n",
      "163400\n",
      "163450\n",
      "163500\n",
      "163550\n",
      "163600\n",
      "163650\n",
      "163700\n",
      "163750\n",
      "163800\n",
      "163850\n",
      "163900\n",
      "163950\n",
      "164000\n",
      "164050\n",
      "164100\n",
      "164150\n",
      "164200\n",
      "164250\n",
      "164300\n",
      "164350\n",
      "164400\n",
      "164450\n",
      "164500\n",
      "164550\n",
      "164600\n",
      "164650\n",
      "164700\n",
      "164750\n",
      "164800\n",
      "164850\n",
      "164900\n",
      "164950\n",
      "Epoch: 1/20...  Training Step: 3300...  Training loss: 1.8096...  0.1013 sec/batch\n",
      "165000\n",
      "165050\n",
      "165100\n",
      "165150\n",
      "165200\n",
      "165250\n",
      "165300\n",
      "165350\n",
      "165400\n",
      "165450\n",
      "165500\n",
      "165550\n",
      "165600\n",
      "165650\n",
      "165700\n",
      "165750\n",
      "165800\n",
      "165850\n",
      "165900\n",
      "165950\n",
      "166000\n",
      "166050\n",
      "166100\n",
      "166150\n",
      "166200\n",
      "166250\n",
      "166300\n",
      "166350\n",
      "166400\n",
      "166450\n",
      "166500\n",
      "166550\n",
      "166600\n",
      "166650\n",
      "166700\n",
      "166750\n",
      "166800\n",
      "166850\n",
      "166900\n",
      "166950\n",
      "167000\n",
      "167050\n",
      "167100\n",
      "167150\n",
      "167200\n",
      "167250\n",
      "167300\n",
      "167350\n",
      "167400\n",
      "167450\n",
      "Epoch: 1/20...  Training Step: 3350...  Training loss: 1.5769...  0.1115 sec/batch\n",
      "167500\n",
      "167550\n",
      "167600\n",
      "167650\n",
      "167700\n",
      "167750\n",
      "167800\n",
      "167850\n",
      "167900\n",
      "167950\n",
      "168000\n",
      "168050\n",
      "168100\n",
      "168150\n",
      "168200\n",
      "168250\n",
      "168300\n",
      "168350\n",
      "168400\n",
      "168450\n",
      "168500\n",
      "168550\n",
      "168600\n",
      "168650\n",
      "168700\n",
      "168750\n",
      "168800\n",
      "168850\n",
      "168900\n",
      "168950\n",
      "169000\n",
      "169050\n",
      "169100\n",
      "169150\n",
      "169200\n",
      "169250\n",
      "169300\n",
      "169350\n",
      "169400\n",
      "169450\n",
      "169500\n",
      "169550\n",
      "169600\n",
      "169650\n",
      "169700\n",
      "169750\n",
      "169800\n",
      "169850\n",
      "169900\n",
      "169950\n",
      "Epoch: 1/20...  Training Step: 3400...  Training loss: 1.6918...  0.1095 sec/batch\n",
      "170000\n",
      "170050\n",
      "170100\n",
      "170150\n",
      "170200\n",
      "170250\n",
      "170300\n",
      "170350\n",
      "170400\n",
      "170450\n",
      "170500\n",
      "170550\n",
      "170600\n",
      "170650\n",
      "170700\n",
      "170750\n",
      "170800\n",
      "170850\n",
      "170900\n",
      "170950\n",
      "171000\n",
      "171050\n",
      "171100\n",
      "171150\n",
      "171200\n",
      "171250\n",
      "171300\n",
      "171350\n",
      "171400\n",
      "171450\n",
      "171500\n",
      "171550\n",
      "171600\n",
      "171650\n",
      "171700\n",
      "171750\n",
      "171800\n",
      "171850\n",
      "171900\n",
      "171950\n",
      "172000\n",
      "172050\n",
      "172100\n",
      "172150\n",
      "172200\n",
      "172250\n",
      "172300\n",
      "172350\n",
      "172400\n",
      "172450\n",
      "Epoch: 1/20...  Training Step: 3450...  Training loss: 1.7160...  0.1431 sec/batch\n",
      "172500\n",
      "172550\n",
      "172600\n",
      "172650\n",
      "172700\n",
      "172750\n",
      "172800\n",
      "172850\n",
      "172900\n",
      "172950\n",
      "173000\n",
      "173050\n",
      "173100\n",
      "173150\n",
      "173200\n",
      "173250\n",
      "173300\n",
      "173350\n",
      "173400\n",
      "173450\n",
      "173500\n",
      "173550\n",
      "173600\n",
      "173650\n",
      "173700\n",
      "173750\n",
      "173800\n",
      "173850\n",
      "173900\n",
      "173950\n",
      "174000\n",
      "174050\n",
      "174100\n",
      "174150\n",
      "174200\n",
      "174250\n",
      "174300\n",
      "174350\n",
      "174400\n",
      "174450\n",
      "174500\n",
      "174550\n",
      "174600\n",
      "174650\n",
      "174700\n",
      "174750\n",
      "174800\n",
      "174850\n",
      "174900\n",
      "174950\n",
      "Epoch: 1/20...  Training Step: 3500...  Training loss: 1.6675...  0.1160 sec/batch\n",
      "175000\n",
      "175050\n",
      "175100\n",
      "175150\n",
      "175200\n",
      "175250\n",
      "175300\n",
      "175350\n",
      "175400\n",
      "175450\n",
      "175500\n",
      "175550\n",
      "175600\n",
      "175650\n",
      "175700\n",
      "175750\n",
      "175800\n",
      "175850\n",
      "175900\n",
      "175950\n",
      "176000\n",
      "176050\n",
      "176100\n",
      "176150\n",
      "176200\n",
      "176250\n",
      "176300\n",
      "176350\n",
      "176400\n",
      "176450\n",
      "176500\n",
      "176550\n",
      "176600\n",
      "176650\n",
      "176700\n",
      "176750\n",
      "176800\n",
      "176850\n",
      "176900\n",
      "176950\n",
      "177000\n",
      "177050\n",
      "177100\n",
      "177150\n",
      "177200\n",
      "177250\n",
      "177300\n",
      "177350\n",
      "177400\n",
      "177450\n",
      "Epoch: 1/20...  Training Step: 3550...  Training loss: 1.8113...  0.1102 sec/batch\n",
      "177500\n",
      "177550\n",
      "177600\n",
      "177650\n",
      "177700\n",
      "177750\n",
      "177800\n",
      "177850\n",
      "177900\n",
      "177950\n",
      "178000\n",
      "178050\n",
      "178100\n",
      "178150\n",
      "178200\n",
      "178250\n",
      "178300\n",
      "178350\n",
      "178400\n",
      "178450\n",
      "178500\n",
      "178550\n",
      "178600\n",
      "178650\n",
      "178700\n",
      "178750\n",
      "178800\n",
      "178850\n",
      "178900\n",
      "178950\n",
      "179000\n",
      "179050\n",
      "179100\n",
      "179150\n",
      "179200\n",
      "179250\n",
      "179300\n",
      "179350\n",
      "179400\n",
      "179450\n",
      "179500\n",
      "179550\n",
      "179600\n",
      "179650\n",
      "179700\n",
      "179750\n",
      "179800\n",
      "179850\n",
      "179900\n",
      "179950\n",
      "Epoch: 1/20...  Training Step: 3600...  Training loss: 1.7137...  0.0813 sec/batch\n",
      "180000\n",
      "180050\n",
      "180100\n",
      "180150\n",
      "180200\n",
      "180250\n",
      "180300\n",
      "180350\n",
      "180400\n",
      "180450\n",
      "180500\n",
      "180550\n",
      "180600\n",
      "180650\n",
      "180700\n",
      "180750\n",
      "180800\n",
      "180850\n",
      "180900\n",
      "180950\n",
      "181000\n",
      "181050\n",
      "181100\n",
      "181150\n",
      "181200\n",
      "181250\n",
      "181300\n",
      "181350\n",
      "181400\n",
      "181450\n",
      "181500\n",
      "181550\n",
      "181600\n",
      "181650\n",
      "181700\n",
      "181750\n",
      "181800\n",
      "181850\n",
      "181900\n",
      "181950\n",
      "182000\n",
      "182050\n",
      "182100\n",
      "182150\n",
      "182200\n",
      "182250\n",
      "182300\n",
      "182350\n",
      "182400\n",
      "182450\n",
      "Epoch: 1/20...  Training Step: 3650...  Training loss: 1.9261...  0.0907 sec/batch\n",
      "182500\n",
      "182550\n",
      "182600\n",
      "182650\n",
      "182700\n",
      "182750\n",
      "182800\n",
      "182850\n",
      "182900\n",
      "182950\n",
      "183000\n",
      "183050\n",
      "183100\n",
      "183150\n",
      "183200\n",
      "183250\n",
      "183300\n",
      "183350\n",
      "183400\n",
      "183450\n",
      "183500\n",
      "183550\n",
      "183600\n",
      "183650\n",
      "183700\n",
      "183750\n",
      "183800\n",
      "183850\n",
      "183900\n",
      "183950\n",
      "184000\n",
      "184050\n",
      "184100\n",
      "184150\n",
      "184200\n",
      "184250\n",
      "184300\n",
      "184350\n",
      "184400\n",
      "184450\n",
      "184500\n",
      "184550\n",
      "184600\n",
      "184650\n",
      "184700\n",
      "184750\n",
      "184800\n",
      "184850\n",
      "184900\n",
      "184950\n",
      "Epoch: 1/20...  Training Step: 3700...  Training loss: 1.6759...  0.0917 sec/batch\n",
      "185000\n",
      "185050\n",
      "185100\n",
      "185150\n",
      "185200\n",
      "185250\n",
      "185300\n",
      "185350\n",
      "185400\n",
      "185450\n",
      "185500\n",
      "185550\n",
      "185600\n",
      "185650\n",
      "185700\n",
      "185750\n",
      "185800\n",
      "185850\n",
      "185900\n",
      "185950\n",
      "186000\n",
      "186050\n",
      "186100\n",
      "186150\n",
      "186200\n",
      "186250\n",
      "186300\n",
      "186350\n",
      "186400\n",
      "186450\n",
      "186500\n",
      "186550\n",
      "186600\n",
      "186650\n",
      "186700\n",
      "186750\n",
      "186800\n",
      "186850\n",
      "186900\n",
      "186950\n",
      "187000\n",
      "187050\n",
      "187100\n",
      "187150\n",
      "187200\n",
      "187250\n",
      "187300\n",
      "187350\n",
      "187400\n",
      "187450\n",
      "Epoch: 1/20...  Training Step: 3750...  Training loss: 1.8746...  0.1701 sec/batch\n",
      "187500\n",
      "187550\n",
      "187600\n",
      "187650\n",
      "187700\n",
      "187750\n",
      "187800\n",
      "187850\n",
      "187900\n",
      "187950\n",
      "188000\n",
      "188050\n",
      "188100\n",
      "188150\n",
      "188200\n",
      "188250\n",
      "188300\n",
      "188350\n",
      "188400\n",
      "188450\n",
      "188500\n",
      "188550\n",
      "188600\n",
      "188650\n",
      "188700\n",
      "188750\n",
      "188800\n",
      "188850\n",
      "188900\n",
      "188950\n",
      "189000\n",
      "189050\n",
      "189100\n",
      "189150\n",
      "189200\n",
      "189250\n",
      "189300\n",
      "189350\n",
      "189400\n",
      "189450\n",
      "189500\n",
      "189550\n",
      "189600\n",
      "189650\n",
      "189700\n",
      "189750\n",
      "189800\n",
      "189850\n",
      "189900\n",
      "189950\n",
      "Epoch: 1/20...  Training Step: 3800...  Training loss: 1.7142...  0.0773 sec/batch\n",
      "190000\n",
      "190050\n",
      "190100\n",
      "190150\n",
      "190200\n",
      "190250\n",
      "190300\n",
      "190350\n",
      "190400\n",
      "190450\n",
      "190500\n",
      "190550\n",
      "190600\n",
      "190650\n",
      "190700\n",
      "190750\n",
      "190800\n",
      "190850\n",
      "190900\n",
      "190950\n",
      "191000\n",
      "191050\n",
      "191100\n",
      "191150\n",
      "191200\n",
      "191250\n",
      "191300\n",
      "191350\n",
      "191400\n",
      "191450\n",
      "191500\n",
      "191550\n",
      "191600\n",
      "191650\n",
      "191700\n",
      "191750\n",
      "191800\n",
      "191850\n",
      "191900\n",
      "191950\n",
      "192000\n",
      "192050\n",
      "192100\n",
      "192150\n",
      "192200\n",
      "192250\n",
      "192300\n",
      "192350\n",
      "192400\n",
      "192450\n",
      "Epoch: 1/20...  Training Step: 3850...  Training loss: 1.7164...  0.0825 sec/batch\n",
      "192500\n",
      "192550\n",
      "192600\n",
      "192650\n",
      "192700\n",
      "192750\n",
      "192800\n",
      "192850\n",
      "192900\n",
      "192950\n",
      "193000\n",
      "193050\n",
      "193100\n",
      "193150\n",
      "193200\n",
      "193250\n",
      "193300\n",
      "193350\n",
      "193400\n",
      "193450\n",
      "193500\n",
      "193550\n",
      "193600\n",
      "193650\n",
      "193700\n",
      "193750\n",
      "193800\n",
      "193850\n",
      "193900\n",
      "193950\n",
      "194000\n",
      "194050\n",
      "194100\n",
      "194150\n",
      "194200\n",
      "194250\n",
      "194300\n",
      "194350\n",
      "194400\n",
      "194450\n",
      "194500\n",
      "194550\n",
      "194600\n",
      "194650\n",
      "194700\n",
      "194750\n",
      "194800\n",
      "194850\n",
      "194900\n",
      "194950\n",
      "Epoch: 1/20...  Training Step: 3900...  Training loss: 1.8935...  0.1479 sec/batch\n",
      "195000\n",
      "195050\n",
      "195100\n",
      "195150\n",
      "195200\n",
      "195250\n",
      "195300\n",
      "195350\n",
      "195400\n",
      "195450\n",
      "195500\n",
      "195550\n",
      "195600\n",
      "195650\n",
      "195700\n",
      "195750\n",
      "195800\n",
      "195850\n",
      "195900\n",
      "195950\n",
      "196000\n",
      "196050\n",
      "196100\n",
      "196150\n",
      "196200\n",
      "196250\n",
      "196300\n",
      "196350\n",
      "196400\n",
      "196450\n",
      "196500\n",
      "196550\n",
      "196600\n",
      "196650\n",
      "196700\n",
      "196750\n",
      "196800\n",
      "196850\n",
      "196900\n",
      "196950\n",
      "197000\n",
      "197050\n",
      "197100\n",
      "197150\n",
      "197200\n",
      "197250\n",
      "197300\n",
      "197350\n",
      "197400\n",
      "197450\n",
      "Epoch: 1/20...  Training Step: 3950...  Training loss: 1.7384...  0.0849 sec/batch\n",
      "197500\n",
      "197550\n",
      "197600\n",
      "197650\n",
      "197700\n",
      "197750\n",
      "197800\n",
      "197850\n",
      "197900\n",
      "197950\n",
      "198000\n",
      "198050\n",
      "198100\n",
      "198150\n",
      "198200\n",
      "198250\n",
      "198300\n",
      "198350\n",
      "198400\n",
      "198450\n",
      "3970\n",
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "800\n",
      "850\n",
      "900\n",
      "950\n",
      "1000\n",
      "1050\n",
      "1100\n",
      "1150\n",
      "1200\n",
      "1250\n",
      "1300\n",
      "1350\n",
      "1400\n",
      "1450\n",
      "Epoch: 2/20...  Training Step: 4000...  Training loss: 1.7299...  0.0903 sec/batch\n",
      "1500\n",
      "1550\n",
      "1600\n",
      "1650\n",
      "1700\n",
      "1750\n",
      "1800\n",
      "1850\n",
      "1900\n",
      "1950\n",
      "2000\n",
      "2050\n",
      "2100\n",
      "2150\n",
      "2200\n",
      "2250\n",
      "2300\n",
      "2350\n",
      "2400\n",
      "2450\n",
      "2500\n",
      "2550\n",
      "2600\n",
      "2650\n",
      "2700\n",
      "2750\n",
      "2800\n",
      "2850\n",
      "2900\n",
      "2950\n",
      "3000\n",
      "3050\n",
      "3100\n",
      "3150\n",
      "3200\n",
      "3250\n",
      "3300\n",
      "3350\n",
      "3400\n",
      "3450\n",
      "3500\n",
      "3550\n",
      "3600\n",
      "3650\n",
      "3700\n",
      "3750\n",
      "3800\n",
      "3850\n",
      "3900\n",
      "3950\n",
      "Epoch: 2/20...  Training Step: 4050...  Training loss: 1.6795...  0.1380 sec/batch\n",
      "4000\n",
      "4050\n",
      "4100\n",
      "4150\n",
      "4200\n",
      "4250\n",
      "4300\n",
      "4350\n",
      "4400\n",
      "4450\n",
      "4500\n",
      "4550\n",
      "4600\n",
      "4650\n",
      "4700\n",
      "4750\n",
      "4800\n",
      "4850\n",
      "4900\n",
      "4950\n",
      "5000\n",
      "5050\n",
      "5100\n",
      "5150\n",
      "5200\n",
      "5250\n",
      "5300\n",
      "5350\n",
      "5400\n",
      "5450\n",
      "5500\n",
      "5550\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5600\n",
      "5650\n",
      "5700\n",
      "5750\n",
      "5800\n",
      "5850\n",
      "5900\n",
      "5950\n",
      "6000\n",
      "6050\n",
      "6100\n",
      "6150\n",
      "6200\n",
      "6250\n",
      "6300\n",
      "6350\n",
      "6400\n",
      "6450\n",
      "Epoch: 2/20...  Training Step: 4100...  Training loss: 1.6749...  0.0841 sec/batch\n",
      "6500\n",
      "6550\n",
      "6600\n",
      "6650\n",
      "6700\n",
      "6750\n",
      "6800\n",
      "6850\n",
      "6900\n",
      "6950\n",
      "7000\n",
      "7050\n",
      "7100\n",
      "7150\n",
      "7200\n",
      "7250\n",
      "7300\n",
      "7350\n",
      "7400\n",
      "7450\n",
      "7500\n",
      "7550\n",
      "7600\n",
      "7650\n",
      "7700\n",
      "7750\n",
      "7800\n",
      "7850\n",
      "7900\n",
      "7950\n",
      "8000\n",
      "8050\n",
      "8100\n",
      "8150\n",
      "8200\n",
      "8250\n",
      "8300\n",
      "8350\n",
      "8400\n",
      "8450\n",
      "8500\n",
      "8550\n",
      "8600\n",
      "8650\n",
      "8700\n",
      "8750\n",
      "8800\n",
      "8850\n",
      "8900\n",
      "8950\n",
      "Epoch: 2/20...  Training Step: 4150...  Training loss: 1.6096...  0.0865 sec/batch\n",
      "9000\n",
      "9050\n",
      "9100\n",
      "9150\n",
      "9200\n",
      "9250\n",
      "9300\n",
      "9350\n",
      "9400\n",
      "9450\n",
      "9500\n",
      "9550\n",
      "9600\n",
      "9650\n",
      "9700\n",
      "9750\n",
      "9800\n",
      "9850\n",
      "9900\n",
      "9950\n",
      "10000\n",
      "10050\n",
      "10100\n",
      "10150\n",
      "10200\n",
      "10250\n",
      "10300\n",
      "10350\n",
      "10400\n",
      "10450\n",
      "10500\n",
      "10550\n",
      "10600\n",
      "10650\n",
      "10700\n",
      "10750\n",
      "10800\n",
      "10850\n",
      "10900\n",
      "10950\n",
      "11000\n",
      "11050\n",
      "11100\n",
      "11150\n",
      "11200\n",
      "11250\n",
      "11300\n",
      "11350\n",
      "11400\n",
      "11450\n",
      "Epoch: 2/20...  Training Step: 4200...  Training loss: 1.6843...  0.0967 sec/batch\n",
      "11500\n",
      "11550\n",
      "11600\n",
      "11650\n",
      "11700\n",
      "11750\n",
      "11800\n",
      "11850\n",
      "11900\n",
      "11950\n",
      "12000\n",
      "12050\n",
      "12100\n",
      "12150\n",
      "12200\n",
      "12250\n",
      "12300\n",
      "12350\n",
      "12400\n",
      "12450\n",
      "12500\n",
      "12550\n",
      "12600\n",
      "12650\n",
      "12700\n",
      "12750\n",
      "12800\n",
      "12850\n",
      "12900\n",
      "12950\n",
      "13000\n",
      "13050\n",
      "13100\n",
      "13150\n",
      "13200\n",
      "13250\n",
      "13300\n",
      "13350\n",
      "13400\n",
      "13450\n",
      "13500\n",
      "13550\n",
      "13600\n",
      "13650\n",
      "13700\n",
      "13750\n",
      "13800\n",
      "13850\n",
      "13900\n",
      "13950\n",
      "Epoch: 2/20...  Training Step: 4250...  Training loss: 1.7660...  0.0939 sec/batch\n",
      "14000\n",
      "14050\n",
      "14100\n",
      "14150\n",
      "14200\n",
      "14250\n",
      "14300\n",
      "14350\n",
      "14400\n",
      "14450\n",
      "14500\n",
      "14550\n",
      "14600\n",
      "14650\n",
      "14700\n",
      "14750\n",
      "14800\n",
      "14850\n",
      "14900\n",
      "14950\n",
      "15000\n",
      "15050\n",
      "15100\n",
      "15150\n",
      "15200\n",
      "15250\n",
      "15300\n",
      "15350\n",
      "15400\n",
      "15450\n",
      "15500\n",
      "15550\n",
      "15600\n",
      "15650\n",
      "15700\n",
      "15750\n",
      "15800\n",
      "15850\n",
      "15900\n",
      "15950\n",
      "16000\n",
      "16050\n",
      "16100\n",
      "16150\n",
      "16200\n",
      "16250\n",
      "16300\n",
      "16350\n",
      "16400\n",
      "16450\n",
      "Epoch: 2/20...  Training Step: 4300...  Training loss: 1.7626...  0.0856 sec/batch\n",
      "16500\n",
      "16550\n",
      "16600\n",
      "16650\n",
      "16700\n",
      "16750\n",
      "16800\n",
      "16850\n",
      "16900\n",
      "16950\n",
      "17000\n",
      "17050\n",
      "17100\n",
      "17150\n",
      "17200\n",
      "17250\n",
      "17300\n",
      "17350\n",
      "17400\n",
      "17450\n",
      "17500\n",
      "17550\n",
      "17600\n",
      "17650\n",
      "17700\n",
      "17750\n",
      "17800\n",
      "17850\n",
      "17900\n",
      "17950\n",
      "18000\n",
      "18050\n",
      "18100\n",
      "18150\n",
      "18200\n",
      "18250\n",
      "18300\n",
      "18350\n",
      "18400\n",
      "18450\n",
      "18500\n",
      "18550\n",
      "18600\n",
      "18650\n",
      "18700\n",
      "18750\n",
      "18800\n",
      "18850\n",
      "18900\n",
      "18950\n",
      "Epoch: 2/20...  Training Step: 4350...  Training loss: 1.7245...  0.0806 sec/batch\n",
      "19000\n",
      "19050\n",
      "19100\n",
      "19150\n",
      "19200\n",
      "19250\n",
      "19300\n",
      "19350\n",
      "19400\n",
      "19450\n",
      "19500\n",
      "19550\n",
      "19600\n",
      "19650\n",
      "19700\n",
      "19750\n",
      "19800\n",
      "19850\n",
      "19900\n",
      "19950\n",
      "20000\n",
      "20050\n",
      "20100\n",
      "20150\n",
      "20200\n",
      "20250\n",
      "20300\n",
      "20350\n",
      "20400\n",
      "20450\n",
      "20500\n",
      "20550\n",
      "20600\n",
      "20650\n",
      "20700\n",
      "20750\n",
      "20800\n",
      "20850\n",
      "20900\n",
      "20950\n",
      "21000\n",
      "21050\n",
      "21100\n",
      "21150\n",
      "21200\n",
      "21250\n",
      "21300\n",
      "21350\n",
      "21400\n",
      "21450\n",
      "Epoch: 2/20...  Training Step: 4400...  Training loss: 1.6109...  0.0863 sec/batch\n",
      "21500\n",
      "21550\n",
      "21600\n",
      "21650\n",
      "21700\n",
      "21750\n",
      "21800\n",
      "21850\n",
      "21900\n",
      "21950\n",
      "22000\n",
      "22050\n",
      "22100\n",
      "22150\n",
      "22200\n",
      "22250\n",
      "22300\n",
      "22350\n",
      "22400\n",
      "22450\n",
      "22500\n",
      "22550\n",
      "22600\n",
      "22650\n",
      "22700\n",
      "22750\n",
      "22800\n",
      "22850\n",
      "22900\n",
      "22950\n",
      "23000\n",
      "23050\n",
      "23100\n",
      "23150\n",
      "23200\n",
      "23250\n",
      "23300\n",
      "23350\n",
      "23400\n",
      "23450\n",
      "23500\n",
      "23550\n",
      "23600\n",
      "23650\n",
      "23700\n",
      "23750\n",
      "23800\n",
      "23850\n",
      "23900\n",
      "23950\n",
      "Epoch: 2/20...  Training Step: 4450...  Training loss: 1.7141...  0.0881 sec/batch\n",
      "24000\n",
      "24050\n",
      "24100\n",
      "24150\n",
      "24200\n",
      "24250\n",
      "24300\n",
      "24350\n",
      "24400\n",
      "24450\n",
      "24500\n",
      "24550\n",
      "24600\n",
      "24650\n",
      "24700\n",
      "24750\n",
      "24800\n",
      "24850\n",
      "24900\n",
      "24950\n",
      "25000\n",
      "25050\n",
      "25100\n",
      "25150\n",
      "25200\n",
      "25250\n",
      "25300\n",
      "25350\n",
      "25400\n",
      "25450\n",
      "25500\n",
      "25550\n",
      "25600\n",
      "25650\n",
      "25700\n",
      "25750\n",
      "25800\n",
      "25850\n",
      "25900\n",
      "25950\n",
      "26000\n",
      "26050\n",
      "26100\n",
      "26150\n",
      "26200\n",
      "26250\n",
      "26300\n",
      "26350\n",
      "26400\n",
      "26450\n",
      "Epoch: 2/20...  Training Step: 4500...  Training loss: 1.6879...  0.0789 sec/batch\n",
      "26500\n",
      "26550\n",
      "26600\n",
      "26650\n",
      "26700\n",
      "26750\n",
      "26800\n",
      "26850\n",
      "26900\n",
      "26950\n",
      "27000\n",
      "27050\n",
      "27100\n",
      "27150\n",
      "27200\n",
      "27250\n",
      "27300\n",
      "27350\n",
      "27400\n",
      "27450\n",
      "27500\n",
      "27550\n",
      "27600\n",
      "27650\n",
      "27700\n",
      "27750\n",
      "27800\n",
      "27850\n",
      "27900\n",
      "27950\n",
      "28000\n",
      "28050\n",
      "28100\n",
      "28150\n",
      "28200\n",
      "28250\n",
      "28300\n",
      "28350\n",
      "28400\n",
      "28450\n",
      "28500\n",
      "28550\n",
      "28600\n",
      "28650\n",
      "28700\n",
      "28750\n",
      "28800\n",
      "28850\n",
      "28900\n",
      "28950\n",
      "Epoch: 2/20...  Training Step: 4550...  Training loss: 1.8859...  0.0805 sec/batch\n",
      "29000\n",
      "29050\n",
      "29100\n",
      "29150\n",
      "29200\n",
      "29250\n",
      "29300\n",
      "29350\n",
      "29400\n",
      "29450\n",
      "29500\n",
      "29550\n",
      "29600\n",
      "29650\n",
      "29700\n",
      "29750\n",
      "29800\n",
      "29850\n",
      "29900\n",
      "29950\n",
      "30000\n",
      "30050\n",
      "30100\n",
      "30150\n",
      "30200\n",
      "30250\n",
      "30300\n",
      "30350\n",
      "30400\n",
      "30450\n",
      "30500\n",
      "30550\n",
      "30600\n",
      "30650\n",
      "30700\n",
      "30750\n",
      "30800\n",
      "30850\n",
      "30900\n",
      "30950\n",
      "31000\n",
      "31050\n",
      "31100\n",
      "31150\n",
      "31200\n",
      "31250\n",
      "31300\n",
      "31350\n",
      "31400\n",
      "31450\n",
      "Epoch: 2/20...  Training Step: 4600...  Training loss: 1.7501...  0.0974 sec/batch\n",
      "31500\n",
      "31550\n",
      "31600\n",
      "31650\n",
      "31700\n",
      "31750\n",
      "31800\n",
      "31850\n",
      "31900\n",
      "31950\n",
      "32000\n",
      "32050\n",
      "32100\n",
      "32150\n",
      "32200\n",
      "32250\n",
      "32300\n",
      "32350\n",
      "32400\n",
      "32450\n",
      "32500\n",
      "32550\n",
      "32600\n",
      "32650\n",
      "32700\n",
      "32750\n",
      "32800\n",
      "32850\n",
      "32900\n",
      "32950\n",
      "33000\n",
      "33050\n",
      "33100\n",
      "33150\n",
      "33200\n",
      "33250\n",
      "33300\n",
      "33350\n",
      "33400\n",
      "33450\n",
      "33500\n",
      "33550\n",
      "33600\n",
      "33650\n",
      "33700\n",
      "33750\n",
      "33800\n",
      "33850\n",
      "33900\n",
      "33950\n",
      "Epoch: 2/20...  Training Step: 4650...  Training loss: 1.8730...  0.0805 sec/batch\n",
      "34000\n",
      "34050\n",
      "34100\n",
      "34150\n",
      "34200\n",
      "34250\n",
      "34300\n",
      "34350\n",
      "34400\n",
      "34450\n",
      "34500\n",
      "34550\n",
      "34600\n",
      "34650\n",
      "34700\n",
      "34750\n",
      "34800\n",
      "34850\n",
      "34900\n",
      "34950\n",
      "35000\n",
      "35050\n",
      "35100\n",
      "35150\n",
      "35200\n",
      "35250\n",
      "35300\n",
      "35350\n",
      "35400\n",
      "35450\n",
      "35500\n",
      "35550\n",
      "35600\n",
      "35650\n",
      "35700\n",
      "35750\n",
      "35800\n",
      "35850\n",
      "35900\n",
      "35950\n",
      "36000\n",
      "36050\n",
      "36100\n",
      "36150\n",
      "36200\n",
      "36250\n",
      "36300\n",
      "36350\n",
      "36400\n",
      "36450\n",
      "Epoch: 2/20...  Training Step: 4700...  Training loss: 1.6771...  0.0875 sec/batch\n",
      "36500\n",
      "36550\n",
      "36600\n",
      "36650\n",
      "36700\n",
      "36750\n",
      "36800\n",
      "36850\n",
      "36900\n",
      "36950\n",
      "37000\n",
      "37050\n",
      "37100\n",
      "37150\n",
      "37200\n",
      "37250\n",
      "37300\n",
      "37350\n",
      "37400\n",
      "37450\n",
      "37500\n",
      "37550\n",
      "37600\n",
      "37650\n",
      "37700\n",
      "37750\n",
      "37800\n",
      "37850\n",
      "37900\n",
      "37950\n",
      "38000\n",
      "38050\n",
      "38100\n",
      "38150\n",
      "38200\n",
      "38250\n",
      "38300\n",
      "38350\n",
      "38400\n",
      "38450\n",
      "38500\n",
      "38550\n",
      "38600\n",
      "38650\n",
      "38700\n",
      "38750\n",
      "38800\n",
      "38850\n",
      "38900\n",
      "38950\n",
      "Epoch: 2/20...  Training Step: 4750...  Training loss: 1.7967...  0.0939 sec/batch\n",
      "39000\n",
      "39050\n",
      "39100\n",
      "39150\n",
      "39200\n",
      "39250\n",
      "39300\n",
      "39350\n",
      "39400\n",
      "39450\n",
      "39500\n",
      "39550\n",
      "39600\n",
      "39650\n",
      "39700\n",
      "39750\n",
      "39800\n",
      "39850\n",
      "39900\n",
      "39950\n",
      "40000\n",
      "40050\n",
      "40100\n",
      "40150\n",
      "40200\n",
      "40250\n",
      "40300\n",
      "40350\n",
      "40400\n",
      "40450\n",
      "40500\n",
      "40550\n",
      "40600\n",
      "40650\n",
      "40700\n",
      "40750\n",
      "40800\n",
      "40850\n",
      "40900\n",
      "40950\n",
      "41000\n",
      "41050\n",
      "41100\n",
      "41150\n",
      "41200\n",
      "41250\n",
      "41300\n",
      "41350\n",
      "41400\n",
      "41450\n",
      "Epoch: 2/20...  Training Step: 4800...  Training loss: 1.5906...  0.0920 sec/batch\n",
      "41500\n",
      "41550\n",
      "41600\n",
      "41650\n",
      "41700\n",
      "41750\n",
      "41800\n",
      "41850\n",
      "41900\n",
      "41950\n",
      "42000\n",
      "42050\n",
      "42100\n",
      "42150\n",
      "42200\n",
      "42250\n",
      "42300\n",
      "42350\n",
      "42400\n",
      "42450\n",
      "42500\n",
      "42550\n",
      "42600\n",
      "42650\n",
      "42700\n",
      "42750\n",
      "42800\n",
      "42850\n",
      "42900\n",
      "42950\n",
      "43000\n",
      "43050\n",
      "43100\n",
      "43150\n",
      "43200\n",
      "43250\n",
      "43300\n",
      "43350\n",
      "43400\n",
      "43450\n",
      "43500\n",
      "43550\n",
      "43600\n",
      "43650\n",
      "43700\n",
      "43750\n",
      "43800\n",
      "43850\n",
      "43900\n",
      "43950\n",
      "Epoch: 2/20...  Training Step: 4850...  Training loss: 1.6354...  0.0824 sec/batch\n",
      "44000\n",
      "44050\n",
      "44100\n",
      "44150\n",
      "44200\n",
      "44250\n",
      "44300\n",
      "44350\n",
      "44400\n",
      "44450\n",
      "44500\n",
      "44550\n",
      "44600\n",
      "44650\n",
      "44700\n",
      "44750\n",
      "44800\n",
      "44850\n",
      "44900\n",
      "44950\n",
      "45000\n",
      "45050\n",
      "45100\n",
      "45150\n",
      "45200\n",
      "45250\n",
      "45300\n",
      "45350\n",
      "45400\n",
      "45450\n",
      "45500\n",
      "45550\n",
      "45600\n",
      "45650\n",
      "45700\n",
      "45750\n",
      "45800\n",
      "45850\n",
      "45900\n",
      "45950\n",
      "46000\n",
      "46050\n",
      "46100\n",
      "46150\n",
      "46200\n",
      "46250\n",
      "46300\n",
      "46350\n",
      "46400\n",
      "46450\n",
      "Epoch: 2/20...  Training Step: 4900...  Training loss: 1.7213...  0.0844 sec/batch\n",
      "46500\n",
      "46550\n",
      "46600\n",
      "46650\n",
      "46700\n",
      "46750\n",
      "46800\n",
      "46850\n",
      "46900\n",
      "46950\n",
      "47000\n",
      "47050\n",
      "47100\n",
      "47150\n",
      "47200\n",
      "47250\n",
      "47300\n",
      "47350\n",
      "47400\n",
      "47450\n",
      "47500\n",
      "47550\n",
      "47600\n",
      "47650\n",
      "47700\n",
      "47750\n",
      "47800\n",
      "47850\n",
      "47900\n",
      "47950\n",
      "48000\n",
      "48050\n",
      "48100\n",
      "48150\n",
      "48200\n",
      "48250\n",
      "48300\n",
      "48350\n",
      "48400\n",
      "48450\n",
      "48500\n",
      "48550\n",
      "48600\n",
      "48650\n",
      "48700\n",
      "48750\n",
      "48800\n",
      "48850\n",
      "48900\n",
      "48950\n",
      "Epoch: 2/20...  Training Step: 4950...  Training loss: 1.6198...  0.0961 sec/batch\n",
      "49000\n",
      "49050\n",
      "49100\n",
      "49150\n",
      "49200\n",
      "49250\n",
      "49300\n",
      "49350\n",
      "49400\n",
      "49450\n",
      "49500\n",
      "49550\n",
      "49600\n",
      "49650\n",
      "49700\n",
      "49750\n",
      "49800\n",
      "49850\n",
      "49900\n",
      "49950\n",
      "50000\n",
      "50050\n",
      "50100\n",
      "50150\n",
      "50200\n",
      "50250\n",
      "50300\n",
      "50350\n",
      "50400\n",
      "50450\n",
      "50500\n",
      "50550\n",
      "50600\n",
      "50650\n",
      "50700\n",
      "50750\n",
      "50800\n",
      "50850\n",
      "50900\n",
      "50950\n",
      "51000\n",
      "51050\n",
      "51100\n",
      "51150\n",
      "51200\n",
      "51250\n",
      "51300\n",
      "51350\n",
      "51400\n",
      "51450\n",
      "Epoch: 2/20...  Training Step: 5000...  Training loss: 1.7227...  0.0854 sec/batch\n",
      "51500\n",
      "51550\n",
      "51600\n",
      "51650\n",
      "51700\n",
      "51750\n",
      "51800\n",
      "51850\n",
      "51900\n",
      "51950\n",
      "52000\n",
      "52050\n",
      "52100\n",
      "52150\n",
      "52200\n",
      "52250\n",
      "52300\n",
      "52350\n",
      "52400\n",
      "52450\n",
      "52500\n",
      "52550\n",
      "52600\n",
      "52650\n",
      "52700\n",
      "52750\n",
      "52800\n",
      "52850\n",
      "52900\n",
      "52950\n",
      "53000\n",
      "53050\n",
      "53100\n",
      "53150\n",
      "53200\n",
      "53250\n",
      "53300\n",
      "53350\n",
      "53400\n",
      "53450\n",
      "53500\n",
      "53550\n",
      "53600\n",
      "53650\n",
      "53700\n",
      "53750\n",
      "53800\n",
      "53850\n",
      "53900\n",
      "53950\n",
      "Epoch: 2/20...  Training Step: 5050...  Training loss: 1.5685...  0.0835 sec/batch\n",
      "54000\n",
      "54050\n",
      "54100\n",
      "54150\n",
      "54200\n",
      "54250\n",
      "54300\n",
      "54350\n",
      "54400\n",
      "54450\n",
      "54500\n",
      "54550\n",
      "54600\n",
      "54650\n",
      "54700\n",
      "54750\n",
      "54800\n",
      "54850\n",
      "54900\n",
      "54950\n",
      "55000\n",
      "55050\n",
      "55100\n",
      "55150\n",
      "55200\n",
      "55250\n",
      "55300\n",
      "55350\n",
      "55400\n",
      "55450\n",
      "55500\n",
      "55550\n",
      "55600\n",
      "55650\n",
      "55700\n",
      "55750\n",
      "55800\n",
      "55850\n",
      "55900\n",
      "55950\n",
      "56000\n",
      "56050\n",
      "56100\n",
      "56150\n",
      "56200\n",
      "56250\n",
      "56300\n",
      "56350\n",
      "56400\n",
      "56450\n",
      "Epoch: 2/20...  Training Step: 5100...  Training loss: 1.6080...  0.1006 sec/batch\n",
      "56500\n",
      "56550\n",
      "56600\n",
      "56650\n",
      "56700\n",
      "56750\n",
      "56800\n",
      "56850\n",
      "56900\n",
      "56950\n",
      "57000\n",
      "57050\n",
      "57100\n",
      "57150\n",
      "57200\n",
      "57250\n",
      "57300\n",
      "57350\n",
      "57400\n",
      "57450\n",
      "57500\n",
      "57550\n",
      "57600\n",
      "57650\n",
      "57700\n",
      "57750\n",
      "57800\n",
      "57850\n",
      "57900\n",
      "57950\n",
      "58000\n",
      "58050\n",
      "58100\n",
      "58150\n",
      "58200\n",
      "58250\n",
      "58300\n",
      "58350\n",
      "58400\n",
      "58450\n",
      "58500\n",
      "58550\n",
      "58600\n",
      "58650\n",
      "58700\n",
      "58750\n",
      "58800\n",
      "58850\n",
      "58900\n",
      "58950\n",
      "Epoch: 2/20...  Training Step: 5150...  Training loss: 1.7314...  0.1069 sec/batch\n",
      "59000\n",
      "59050\n",
      "59100\n",
      "59150\n",
      "59200\n",
      "59250\n",
      "59300\n",
      "59350\n",
      "59400\n",
      "59450\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59500\n",
      "59550\n",
      "59600\n",
      "59650\n",
      "59700\n",
      "59750\n",
      "59800\n",
      "59850\n",
      "59900\n",
      "59950\n",
      "60000\n",
      "60050\n",
      "60100\n",
      "60150\n",
      "60200\n",
      "60250\n",
      "60300\n",
      "60350\n",
      "60400\n",
      "60450\n",
      "60500\n",
      "60550\n",
      "60600\n",
      "60650\n",
      "60700\n",
      "60750\n",
      "60800\n",
      "60850\n",
      "60900\n",
      "60950\n",
      "61000\n",
      "61050\n",
      "61100\n",
      "61150\n",
      "61200\n",
      "61250\n",
      "61300\n",
      "61350\n",
      "61400\n",
      "61450\n",
      "Epoch: 2/20...  Training Step: 5200...  Training loss: 1.7938...  0.1106 sec/batch\n",
      "61500\n",
      "61550\n",
      "61600\n",
      "61650\n",
      "61700\n",
      "61750\n",
      "61800\n",
      "61850\n",
      "61900\n",
      "61950\n",
      "62000\n",
      "62050\n",
      "62100\n",
      "62150\n",
      "62200\n",
      "62250\n",
      "62300\n",
      "62350\n",
      "62400\n",
      "62450\n",
      "62500\n",
      "62550\n",
      "62600\n",
      "62650\n",
      "62700\n",
      "62750\n",
      "62800\n",
      "62850\n",
      "62900\n",
      "62950\n",
      "63000\n",
      "63050\n",
      "63100\n",
      "63150\n",
      "63200\n",
      "63250\n",
      "63300\n",
      "63350\n",
      "63400\n",
      "63450\n",
      "63500\n",
      "63550\n",
      "63600\n",
      "63650\n",
      "63700\n",
      "63750\n",
      "63800\n",
      "63850\n",
      "63900\n",
      "63950\n",
      "Epoch: 2/20...  Training Step: 5250...  Training loss: 1.6096...  0.0727 sec/batch\n",
      "64000\n",
      "64050\n",
      "64100\n",
      "64150\n",
      "64200\n",
      "64250\n",
      "64300\n",
      "64350\n",
      "64400\n",
      "64450\n",
      "64500\n",
      "64550\n",
      "64600\n",
      "64650\n",
      "64700\n",
      "64750\n",
      "64800\n",
      "64850\n",
      "64900\n",
      "64950\n",
      "65000\n",
      "65050\n",
      "65100\n",
      "65150\n",
      "65200\n",
      "65250\n",
      "65300\n",
      "65350\n",
      "65400\n",
      "65450\n",
      "65500\n",
      "65550\n",
      "65600\n",
      "65650\n",
      "65700\n",
      "65750\n",
      "65800\n",
      "65850\n",
      "65900\n",
      "65950\n",
      "66000\n",
      "66050\n",
      "66100\n",
      "66150\n",
      "66200\n",
      "66250\n",
      "66300\n",
      "66350\n",
      "66400\n",
      "66450\n",
      "Epoch: 2/20...  Training Step: 5300...  Training loss: 1.7555...  0.0824 sec/batch\n",
      "66500\n",
      "66550\n",
      "66600\n",
      "66650\n",
      "66700\n",
      "66750\n",
      "66800\n",
      "66850\n",
      "66900\n",
      "66950\n",
      "67000\n",
      "67050\n",
      "67100\n",
      "67150\n",
      "67200\n",
      "67250\n",
      "67300\n",
      "67350\n",
      "67400\n",
      "67450\n",
      "67500\n",
      "67550\n",
      "67600\n",
      "67650\n",
      "67700\n",
      "67750\n",
      "67800\n",
      "67850\n",
      "67900\n",
      "67950\n",
      "68000\n",
      "68050\n",
      "68100\n",
      "68150\n",
      "68200\n",
      "68250\n",
      "68300\n",
      "68350\n",
      "68400\n",
      "68450\n",
      "68500\n",
      "68550\n",
      "68600\n",
      "68650\n",
      "68700\n",
      "68750\n",
      "68800\n",
      "68850\n",
      "68900\n",
      "68950\n",
      "Epoch: 2/20...  Training Step: 5350...  Training loss: 1.6521...  0.0850 sec/batch\n",
      "69000\n",
      "69050\n",
      "69100\n",
      "69150\n",
      "69200\n",
      "69250\n",
      "69300\n",
      "69350\n",
      "69400\n",
      "69450\n",
      "69500\n",
      "69550\n",
      "69600\n",
      "69650\n",
      "69700\n",
      "69750\n",
      "69800\n",
      "69850\n",
      "69900\n",
      "69950\n",
      "70000\n",
      "70050\n",
      "70100\n",
      "70150\n",
      "70200\n",
      "70250\n",
      "70300\n",
      "70350\n",
      "70400\n",
      "70450\n",
      "70500\n",
      "70550\n",
      "70600\n",
      "70650\n",
      "70700\n",
      "70750\n",
      "70800\n",
      "70850\n",
      "70900\n",
      "70950\n",
      "71000\n",
      "71050\n",
      "71100\n",
      "71150\n",
      "71200\n",
      "71250\n",
      "71300\n",
      "71350\n",
      "71400\n",
      "71450\n",
      "Epoch: 2/20...  Training Step: 5400...  Training loss: 1.7775...  0.0870 sec/batch\n",
      "71500\n",
      "71550\n",
      "71600\n",
      "71650\n",
      "71700\n",
      "71750\n",
      "71800\n",
      "71850\n",
      "71900\n",
      "71950\n",
      "72000\n",
      "72050\n",
      "72100\n",
      "72150\n",
      "72200\n",
      "72250\n",
      "72300\n",
      "72350\n",
      "72400\n",
      "72450\n",
      "72500\n",
      "72550\n",
      "72600\n",
      "72650\n",
      "72700\n",
      "72750\n",
      "72800\n",
      "72850\n",
      "72900\n",
      "72950\n",
      "73000\n",
      "73050\n",
      "73100\n",
      "73150\n",
      "73200\n",
      "73250\n",
      "73300\n",
      "73350\n",
      "73400\n",
      "73450\n",
      "73500\n",
      "73550\n",
      "73600\n",
      "73650\n",
      "73700\n",
      "73750\n",
      "73800\n",
      "73850\n",
      "73900\n",
      "73950\n",
      "Epoch: 2/20...  Training Step: 5450...  Training loss: 1.7513...  0.0737 sec/batch\n",
      "74000\n",
      "74050\n",
      "74100\n",
      "74150\n",
      "74200\n",
      "74250\n",
      "74300\n",
      "74350\n",
      "74400\n",
      "74450\n",
      "74500\n",
      "74550\n",
      "74600\n",
      "74650\n",
      "74700\n",
      "74750\n",
      "74800\n",
      "74850\n",
      "74900\n",
      "74950\n",
      "75000\n",
      "75050\n",
      "75100\n",
      "75150\n",
      "75200\n",
      "75250\n",
      "75300\n",
      "75350\n",
      "75400\n",
      "75450\n",
      "75500\n",
      "75550\n",
      "75600\n",
      "75650\n",
      "75700\n",
      "75750\n",
      "75800\n",
      "75850\n",
      "75900\n",
      "75950\n",
      "76000\n",
      "76050\n",
      "76100\n",
      "76150\n",
      "76200\n",
      "76250\n",
      "76300\n",
      "76350\n",
      "76400\n",
      "76450\n",
      "Epoch: 2/20...  Training Step: 5500...  Training loss: 1.7228...  0.0744 sec/batch\n",
      "76500\n",
      "76550\n",
      "76600\n",
      "76650\n",
      "76700\n",
      "76750\n",
      "76800\n",
      "76850\n",
      "76900\n",
      "76950\n",
      "77000\n",
      "77050\n",
      "77100\n",
      "77150\n",
      "77200\n",
      "77250\n",
      "77300\n",
      "77350\n",
      "77400\n",
      "77450\n",
      "77500\n",
      "77550\n",
      "77600\n",
      "77650\n",
      "77700\n",
      "77750\n",
      "77800\n",
      "77850\n",
      "77900\n",
      "77950\n",
      "78000\n",
      "78050\n",
      "78100\n",
      "78150\n",
      "78200\n",
      "78250\n",
      "78300\n",
      "78350\n",
      "78400\n",
      "78450\n",
      "78500\n",
      "78550\n",
      "78600\n",
      "78650\n",
      "78700\n",
      "78750\n",
      "78800\n",
      "78850\n",
      "78900\n",
      "78950\n",
      "Epoch: 2/20...  Training Step: 5550...  Training loss: 1.6529...  0.0994 sec/batch\n",
      "79000\n",
      "79050\n",
      "79100\n",
      "79150\n",
      "79200\n",
      "79250\n",
      "79300\n",
      "79350\n",
      "79400\n",
      "79450\n",
      "79500\n",
      "79550\n",
      "79600\n",
      "79650\n",
      "79700\n",
      "79750\n",
      "79800\n",
      "79850\n",
      "79900\n",
      "79950\n",
      "80000\n",
      "80050\n",
      "80100\n",
      "80150\n",
      "80200\n",
      "80250\n",
      "80300\n",
      "80350\n",
      "80400\n",
      "80450\n",
      "80500\n",
      "80550\n",
      "80600\n",
      "80650\n",
      "80700\n",
      "80750\n",
      "80800\n",
      "80850\n",
      "80900\n",
      "80950\n",
      "81000\n",
      "81050\n",
      "81100\n",
      "81150\n",
      "81200\n",
      "81250\n",
      "81300\n",
      "81350\n",
      "81400\n",
      "81450\n",
      "Epoch: 2/20...  Training Step: 5600...  Training loss: 1.7787...  0.0746 sec/batch\n",
      "81500\n",
      "81550\n",
      "81600\n",
      "81650\n",
      "81700\n",
      "81750\n",
      "81800\n",
      "81850\n",
      "81900\n",
      "81950\n",
      "82000\n",
      "82050\n",
      "82100\n",
      "82150\n",
      "82200\n",
      "82250\n",
      "82300\n",
      "82350\n",
      "82400\n",
      "82450\n",
      "82500\n",
      "82550\n",
      "82600\n",
      "82650\n",
      "82700\n",
      "82750\n",
      "82800\n",
      "82850\n",
      "82900\n",
      "82950\n",
      "83000\n",
      "83050\n",
      "83100\n",
      "83150\n",
      "83200\n",
      "83250\n",
      "83300\n",
      "83350\n",
      "83400\n",
      "83450\n",
      "83500\n",
      "83550\n",
      "83600\n",
      "83650\n",
      "83700\n",
      "83750\n",
      "83800\n",
      "83850\n",
      "83900\n",
      "83950\n",
      "Epoch: 2/20...  Training Step: 5650...  Training loss: 1.6291...  0.0750 sec/batch\n",
      "84000\n",
      "84050\n",
      "84100\n",
      "84150\n",
      "84200\n",
      "84250\n",
      "84300\n",
      "84350\n",
      "84400\n",
      "84450\n",
      "84500\n",
      "84550\n",
      "84600\n",
      "84650\n",
      "84700\n",
      "84750\n",
      "84800\n",
      "84850\n",
      "84900\n",
      "84950\n",
      "85000\n",
      "85050\n",
      "85100\n",
      "85150\n",
      "85200\n",
      "85250\n",
      "85300\n",
      "85350\n",
      "85400\n",
      "85450\n",
      "85500\n",
      "85550\n",
      "85600\n",
      "85650\n",
      "85700\n",
      "85750\n",
      "85800\n",
      "85850\n",
      "85900\n",
      "85950\n",
      "86000\n",
      "86050\n",
      "86100\n",
      "86150\n",
      "86200\n",
      "86250\n",
      "86300\n",
      "86350\n",
      "86400\n",
      "86450\n",
      "Epoch: 2/20...  Training Step: 5700...  Training loss: 1.6036...  0.0836 sec/batch\n",
      "86500\n",
      "86550\n",
      "86600\n",
      "86650\n",
      "86700\n",
      "86750\n",
      "86800\n",
      "86850\n",
      "86900\n",
      "86950\n",
      "87000\n",
      "87050\n",
      "87100\n",
      "87150\n",
      "87200\n",
      "87250\n",
      "87300\n",
      "87350\n",
      "87400\n",
      "87450\n",
      "87500\n",
      "87550\n",
      "87600\n",
      "87650\n",
      "87700\n",
      "87750\n",
      "87800\n",
      "87850\n",
      "87900\n",
      "87950\n",
      "88000\n",
      "88050\n",
      "88100\n",
      "88150\n",
      "88200\n",
      "88250\n",
      "88300\n",
      "88350\n",
      "88400\n",
      "88450\n",
      "88500\n",
      "88550\n",
      "88600\n",
      "88650\n",
      "88700\n",
      "88750\n",
      "88800\n",
      "88850\n",
      "88900\n",
      "88950\n",
      "Epoch: 2/20...  Training Step: 5750...  Training loss: 1.6962...  0.0895 sec/batch\n",
      "89000\n",
      "89050\n",
      "89100\n",
      "89150\n",
      "89200\n",
      "89250\n",
      "89300\n",
      "89350\n",
      "89400\n",
      "89450\n",
      "89500\n",
      "89550\n",
      "89600\n",
      "89650\n",
      "89700\n",
      "89750\n",
      "89800\n",
      "89850\n",
      "89900\n",
      "89950\n",
      "90000\n",
      "90050\n",
      "90100\n",
      "90150\n",
      "90200\n",
      "90250\n",
      "90300\n",
      "90350\n",
      "90400\n",
      "90450\n",
      "90500\n",
      "90550\n",
      "90600\n",
      "90650\n",
      "90700\n",
      "90750\n",
      "90800\n",
      "90850\n",
      "90900\n",
      "90950\n",
      "91000\n",
      "91050\n",
      "91100\n",
      "91150\n",
      "91200\n",
      "91250\n",
      "91300\n",
      "91350\n",
      "91400\n",
      "91450\n",
      "Epoch: 2/20...  Training Step: 5800...  Training loss: 1.6626...  0.0808 sec/batch\n",
      "91500\n",
      "91550\n",
      "91600\n",
      "91650\n",
      "91700\n",
      "91750\n",
      "91800\n",
      "91850\n",
      "91900\n",
      "91950\n",
      "92000\n",
      "92050\n",
      "92100\n",
      "92150\n",
      "92200\n",
      "92250\n",
      "92300\n",
      "92350\n",
      "92400\n",
      "92450\n",
      "92500\n",
      "92550\n",
      "92600\n",
      "92650\n",
      "92700\n",
      "92750\n",
      "92800\n",
      "92850\n",
      "92900\n",
      "92950\n",
      "93000\n",
      "93050\n",
      "93100\n",
      "93150\n",
      "93200\n",
      "93250\n",
      "93300\n",
      "93350\n",
      "93400\n",
      "93450\n",
      "93500\n",
      "93550\n",
      "93600\n",
      "93650\n",
      "93700\n",
      "93750\n",
      "93800\n",
      "93850\n",
      "93900\n",
      "93950\n",
      "Epoch: 2/20...  Training Step: 5850...  Training loss: 1.7893...  0.0810 sec/batch\n",
      "94000\n",
      "94050\n",
      "94100\n",
      "94150\n",
      "94200\n",
      "94250\n",
      "94300\n",
      "94350\n",
      "94400\n",
      "94450\n",
      "94500\n",
      "94550\n",
      "94600\n",
      "94650\n",
      "94700\n",
      "94750\n",
      "94800\n",
      "94850\n",
      "94900\n",
      "94950\n",
      "95000\n",
      "95050\n",
      "95100\n",
      "95150\n",
      "95200\n",
      "95250\n",
      "95300\n",
      "95350\n",
      "95400\n",
      "95450\n",
      "95500\n",
      "95550\n",
      "95600\n",
      "95650\n",
      "95700\n",
      "95750\n",
      "95800\n",
      "95850\n",
      "95900\n",
      "95950\n",
      "96000\n",
      "96050\n",
      "96100\n",
      "96150\n",
      "96200\n",
      "96250\n",
      "96300\n",
      "96350\n",
      "96400\n",
      "96450\n",
      "Epoch: 2/20...  Training Step: 5900...  Training loss: 1.6869...  0.0933 sec/batch\n",
      "96500\n",
      "96550\n",
      "96600\n",
      "96650\n",
      "96700\n",
      "96750\n",
      "96800\n",
      "96850\n",
      "96900\n",
      "96950\n",
      "97000\n",
      "97050\n",
      "97100\n",
      "97150\n",
      "97200\n",
      "97250\n",
      "97300\n",
      "97350\n",
      "97400\n",
      "97450\n",
      "97500\n",
      "97550\n",
      "97600\n",
      "97650\n",
      "97700\n",
      "97750\n",
      "97800\n",
      "97850\n",
      "97900\n",
      "97950\n",
      "98000\n",
      "98050\n",
      "98100\n",
      "98150\n",
      "98200\n",
      "98250\n",
      "98300\n",
      "98350\n",
      "98400\n",
      "98450\n",
      "98500\n",
      "98550\n",
      "98600\n",
      "98650\n",
      "98700\n",
      "98750\n",
      "98800\n",
      "98850\n",
      "98900\n",
      "98950\n",
      "Epoch: 2/20...  Training Step: 5950...  Training loss: 1.6789...  0.0814 sec/batch\n",
      "99000\n",
      "99050\n",
      "99100\n",
      "99150\n",
      "99200\n",
      "99250\n",
      "99300\n",
      "99350\n",
      "99400\n",
      "99450\n",
      "99500\n",
      "99550\n",
      "99600\n",
      "99650\n",
      "99700\n",
      "99750\n",
      "99800\n",
      "99850\n",
      "99900\n",
      "99950\n",
      "100000\n",
      "100050\n",
      "100100\n",
      "100150\n",
      "100200\n",
      "100250\n",
      "100300\n",
      "100350\n",
      "100400\n",
      "100450\n",
      "100500\n",
      "100550\n",
      "100600\n",
      "100650\n",
      "100700\n",
      "100750\n",
      "100800\n",
      "100850\n",
      "100900\n",
      "100950\n",
      "101000\n",
      "101050\n",
      "101100\n",
      "101150\n",
      "101200\n",
      "101250\n",
      "101300\n",
      "101350\n",
      "101400\n",
      "101450\n",
      "Epoch: 2/20...  Training Step: 6000...  Training loss: 1.5670...  0.1054 sec/batch\n",
      "101500\n",
      "101550\n",
      "101600\n",
      "101650\n",
      "101700\n",
      "101750\n",
      "101800\n",
      "101850\n",
      "101900\n",
      "101950\n",
      "102000\n",
      "102050\n",
      "102100\n",
      "102150\n",
      "102200\n",
      "102250\n",
      "102300\n",
      "102350\n",
      "102400\n",
      "102450\n",
      "102500\n",
      "102550\n",
      "102600\n",
      "102650\n",
      "102700\n",
      "102750\n",
      "102800\n",
      "102850\n",
      "102900\n",
      "102950\n",
      "103000\n",
      "103050\n",
      "103100\n",
      "103150\n",
      "103200\n",
      "103250\n",
      "103300\n",
      "103350\n",
      "103400\n",
      "103450\n",
      "103500\n",
      "103550\n",
      "103600\n",
      "103650\n",
      "103700\n",
      "103750\n",
      "103800\n",
      "103850\n",
      "103900\n",
      "103950\n",
      "Epoch: 2/20...  Training Step: 6050...  Training loss: 1.6617...  0.0794 sec/batch\n",
      "104000\n",
      "104050\n",
      "104100\n",
      "104150\n",
      "104200\n",
      "104250\n",
      "104300\n",
      "104350\n",
      "104400\n",
      "104450\n",
      "104500\n",
      "104550\n",
      "104600\n",
      "104650\n",
      "104700\n",
      "104750\n",
      "104800\n",
      "104850\n",
      "104900\n",
      "104950\n",
      "105000\n",
      "105050\n",
      "105100\n",
      "105150\n",
      "105200\n",
      "105250\n",
      "105300\n",
      "105350\n",
      "105400\n",
      "105450\n",
      "105500\n",
      "105550\n",
      "105600\n",
      "105650\n",
      "105700\n",
      "105750\n",
      "105800\n",
      "105850\n",
      "105900\n",
      "105950\n",
      "106000\n",
      "106050\n",
      "106100\n",
      "106150\n",
      "106200\n",
      "106250\n",
      "106300\n",
      "106350\n",
      "106400\n",
      "106450\n",
      "Epoch: 2/20...  Training Step: 6100...  Training loss: 1.7626...  0.0785 sec/batch\n",
      "106500\n",
      "106550\n",
      "106600\n",
      "106650\n",
      "106700\n",
      "106750\n",
      "106800\n",
      "106850\n",
      "106900\n",
      "106950\n",
      "107000\n",
      "107050\n",
      "107100\n",
      "107150\n",
      "107200\n",
      "107250\n",
      "107300\n",
      "107350\n",
      "107400\n",
      "107450\n",
      "107500\n",
      "107550\n",
      "107600\n",
      "107650\n",
      "107700\n",
      "107750\n",
      "107800\n",
      "107850\n",
      "107900\n",
      "107950\n",
      "108000\n",
      "108050\n",
      "108100\n",
      "108150\n",
      "108200\n",
      "108250\n",
      "108300\n",
      "108350\n",
      "108400\n",
      "108450\n",
      "108500\n",
      "108550\n",
      "108600\n",
      "108650\n",
      "108700\n",
      "108750\n",
      "108800\n",
      "108850\n",
      "108900\n",
      "108950\n",
      "Epoch: 2/20...  Training Step: 6150...  Training loss: 1.7108...  0.0814 sec/batch\n",
      "109000\n",
      "109050\n",
      "109100\n",
      "109150\n",
      "109200\n",
      "109250\n",
      "109300\n",
      "109350\n",
      "109400\n",
      "109450\n",
      "109500\n",
      "109550\n",
      "109600\n",
      "109650\n",
      "109700\n",
      "109750\n",
      "109800\n",
      "109850\n",
      "109900\n",
      "109950\n",
      "110000\n",
      "110050\n",
      "110100\n",
      "110150\n",
      "110200\n",
      "110250\n",
      "110300\n",
      "110350\n",
      "110400\n",
      "110450\n",
      "110500\n",
      "110550\n",
      "110600\n",
      "110650\n",
      "110700\n",
      "110750\n",
      "110800\n",
      "110850\n",
      "110900\n",
      "110950\n",
      "111000\n",
      "111050\n",
      "111100\n",
      "111150\n",
      "111200\n",
      "111250\n",
      "111300\n",
      "111350\n",
      "111400\n",
      "111450\n",
      "Epoch: 2/20...  Training Step: 6200...  Training loss: 1.6473...  0.0795 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111500\n",
      "111550\n",
      "111600\n",
      "111650\n",
      "111700\n",
      "111750\n",
      "111800\n",
      "111850\n",
      "111900\n",
      "111950\n",
      "112000\n",
      "112050\n",
      "112100\n",
      "112150\n",
      "112200\n",
      "112250\n",
      "112300\n",
      "112350\n",
      "112400\n",
      "112450\n",
      "112500\n",
      "112550\n",
      "112600\n",
      "112650\n",
      "112700\n",
      "112750\n",
      "112800\n",
      "112850\n",
      "112900\n",
      "112950\n",
      "113000\n",
      "113050\n",
      "113100\n",
      "113150\n",
      "113200\n",
      "113250\n",
      "113300\n",
      "113350\n",
      "113400\n",
      "113450\n",
      "113500\n",
      "113550\n",
      "113600\n",
      "113650\n",
      "113700\n",
      "113750\n",
      "113800\n",
      "113850\n",
      "113900\n",
      "113950\n",
      "Epoch: 2/20...  Training Step: 6250...  Training loss: 1.5858...  0.0786 sec/batch\n",
      "114000\n",
      "114050\n",
      "114100\n",
      "114150\n",
      "114200\n",
      "114250\n",
      "114300\n",
      "114350\n",
      "114400\n",
      "114450\n",
      "114500\n",
      "114550\n",
      "114600\n",
      "114650\n",
      "114700\n",
      "114750\n",
      "114800\n",
      "114850\n",
      "114900\n",
      "114950\n",
      "115000\n",
      "115050\n",
      "115100\n",
      "115150\n",
      "115200\n",
      "115250\n",
      "115300\n",
      "115350\n",
      "115400\n",
      "115450\n",
      "115500\n",
      "115550\n",
      "115600\n",
      "115650\n",
      "115700\n",
      "115750\n",
      "115800\n",
      "115850\n",
      "115900\n",
      "115950\n",
      "116000\n",
      "116050\n",
      "116100\n",
      "116150\n",
      "116200\n",
      "116250\n",
      "116300\n",
      "116350\n",
      "116400\n",
      "116450\n",
      "Epoch: 2/20...  Training Step: 6300...  Training loss: 1.6205...  0.0752 sec/batch\n",
      "116500\n",
      "116550\n",
      "116600\n",
      "116650\n",
      "116700\n",
      "116750\n",
      "116800\n",
      "116850\n",
      "116900\n",
      "116950\n",
      "117000\n",
      "117050\n",
      "117100\n",
      "117150\n",
      "117200\n",
      "117250\n",
      "117300\n",
      "117350\n",
      "117400\n",
      "117450\n",
      "117500\n",
      "117550\n",
      "117600\n",
      "117650\n",
      "117700\n",
      "117750\n",
      "117800\n",
      "117850\n",
      "117900\n",
      "117950\n",
      "118000\n",
      "118050\n",
      "118100\n",
      "118150\n",
      "118200\n",
      "118250\n",
      "118300\n",
      "118350\n",
      "118400\n",
      "118450\n",
      "118500\n",
      "118550\n",
      "118600\n",
      "118650\n",
      "118700\n",
      "118750\n",
      "118800\n",
      "118850\n",
      "118900\n",
      "118950\n",
      "Epoch: 2/20...  Training Step: 6350...  Training loss: 1.7794...  0.1149 sec/batch\n",
      "119000\n",
      "119050\n",
      "119100\n",
      "119150\n",
      "119200\n",
      "119250\n",
      "119300\n",
      "119350\n",
      "119400\n",
      "119450\n",
      "119500\n",
      "119550\n",
      "119600\n",
      "119650\n",
      "119700\n",
      "119750\n",
      "119800\n",
      "119850\n",
      "119900\n",
      "119950\n",
      "120000\n",
      "120050\n",
      "120100\n",
      "120150\n",
      "120200\n",
      "120250\n",
      "120300\n",
      "120350\n",
      "120400\n",
      "120450\n",
      "120500\n",
      "120550\n",
      "120600\n",
      "120650\n",
      "120700\n",
      "120750\n",
      "120800\n",
      "120850\n",
      "120900\n",
      "120950\n",
      "121000\n",
      "121050\n",
      "121100\n",
      "121150\n",
      "121200\n",
      "121250\n",
      "121300\n",
      "121350\n",
      "121400\n",
      "121450\n",
      "Epoch: 2/20...  Training Step: 6400...  Training loss: 1.6039...  0.0806 sec/batch\n",
      "121500\n",
      "121550\n",
      "121600\n",
      "121650\n",
      "121700\n",
      "121750\n",
      "121800\n",
      "121850\n",
      "121900\n",
      "121950\n",
      "122000\n",
      "122050\n",
      "122100\n",
      "122150\n",
      "122200\n",
      "122250\n",
      "122300\n",
      "122350\n",
      "122400\n",
      "122450\n",
      "122500\n",
      "122550\n",
      "122600\n",
      "122650\n",
      "122700\n",
      "122750\n",
      "122800\n",
      "122850\n",
      "122900\n",
      "122950\n",
      "123000\n",
      "123050\n",
      "123100\n",
      "123150\n",
      "123200\n",
      "123250\n",
      "123300\n",
      "123350\n",
      "123400\n",
      "123450\n",
      "123500\n",
      "123550\n",
      "123600\n",
      "123650\n",
      "123700\n",
      "123750\n",
      "123800\n",
      "123850\n",
      "123900\n",
      "123950\n",
      "Epoch: 2/20...  Training Step: 6450...  Training loss: 1.7243...  0.0816 sec/batch\n",
      "124000\n",
      "124050\n",
      "124100\n",
      "124150\n",
      "124200\n",
      "124250\n",
      "124300\n",
      "124350\n",
      "124400\n",
      "124450\n",
      "124500\n",
      "124550\n",
      "124600\n",
      "124650\n",
      "124700\n",
      "124750\n",
      "124800\n",
      "124850\n",
      "124900\n",
      "124950\n",
      "125000\n",
      "125050\n",
      "125100\n",
      "125150\n",
      "125200\n",
      "125250\n",
      "125300\n",
      "125350\n",
      "125400\n",
      "125450\n",
      "125500\n",
      "125550\n",
      "125600\n",
      "125650\n",
      "125700\n",
      "125750\n",
      "125800\n",
      "125850\n",
      "125900\n",
      "125950\n",
      "126000\n",
      "126050\n",
      "126100\n",
      "126150\n",
      "126200\n",
      "126250\n",
      "126300\n",
      "126350\n",
      "126400\n",
      "126450\n",
      "Epoch: 2/20...  Training Step: 6500...  Training loss: 1.6836...  0.0832 sec/batch\n",
      "126500\n",
      "126550\n",
      "126600\n",
      "126650\n",
      "126700\n",
      "126750\n",
      "126800\n",
      "126850\n",
      "126900\n",
      "126950\n",
      "127000\n",
      "127050\n",
      "127100\n",
      "127150\n",
      "127200\n",
      "127250\n",
      "127300\n",
      "127350\n",
      "127400\n",
      "127450\n",
      "127500\n",
      "127550\n",
      "127600\n",
      "127650\n",
      "127700\n",
      "127750\n",
      "127800\n",
      "127850\n",
      "127900\n",
      "127950\n",
      "128000\n",
      "128050\n",
      "128100\n",
      "128150\n",
      "128200\n",
      "128250\n",
      "128300\n",
      "128350\n",
      "128400\n",
      "128450\n",
      "128500\n",
      "128550\n",
      "128600\n",
      "128650\n",
      "128700\n",
      "128750\n",
      "128800\n",
      "128850\n",
      "128900\n",
      "128950\n",
      "Epoch: 2/20...  Training Step: 6550...  Training loss: 1.7252...  0.1117 sec/batch\n",
      "129000\n",
      "129050\n",
      "129100\n",
      "129150\n",
      "129200\n",
      "129250\n",
      "129300\n",
      "129350\n",
      "129400\n",
      "129450\n",
      "129500\n",
      "129550\n",
      "129600\n",
      "129650\n",
      "129700\n",
      "129750\n",
      "129800\n",
      "129850\n",
      "129900\n",
      "129950\n",
      "130000\n",
      "130050\n",
      "130100\n",
      "130150\n",
      "130200\n",
      "130250\n",
      "130300\n",
      "130350\n",
      "130400\n",
      "130450\n",
      "130500\n",
      "130550\n",
      "130600\n",
      "130650\n",
      "130700\n",
      "130750\n",
      "130800\n",
      "130850\n",
      "130900\n",
      "130950\n",
      "131000\n",
      "131050\n",
      "131100\n",
      "131150\n",
      "131200\n",
      "131250\n",
      "131300\n",
      "131350\n",
      "131400\n",
      "131450\n",
      "Epoch: 2/20...  Training Step: 6600...  Training loss: 1.8748...  0.0936 sec/batch\n",
      "131500\n",
      "131550\n",
      "131600\n",
      "131650\n",
      "131700\n",
      "131750\n",
      "131800\n",
      "131850\n",
      "131900\n",
      "131950\n",
      "132000\n",
      "132050\n",
      "132100\n",
      "132150\n",
      "132200\n",
      "132250\n",
      "132300\n",
      "132350\n",
      "132400\n",
      "132450\n",
      "132500\n",
      "132550\n",
      "132600\n",
      "132650\n",
      "132700\n",
      "132750\n",
      "132800\n",
      "132850\n",
      "132900\n",
      "132950\n",
      "133000\n",
      "133050\n",
      "133100\n",
      "133150\n",
      "133200\n",
      "133250\n",
      "133300\n",
      "133350\n",
      "133400\n",
      "133450\n",
      "133500\n",
      "133550\n",
      "133600\n",
      "133650\n",
      "133700\n",
      "133750\n",
      "133800\n",
      "133850\n",
      "133900\n",
      "133950\n",
      "Epoch: 2/20...  Training Step: 6650...  Training loss: 1.5579...  0.1000 sec/batch\n",
      "134000\n",
      "134050\n",
      "134100\n",
      "134150\n",
      "134200\n",
      "134250\n",
      "134300\n",
      "134350\n",
      "134400\n",
      "134450\n",
      "134500\n",
      "134550\n",
      "134600\n",
      "134650\n",
      "134700\n",
      "134750\n",
      "134800\n",
      "134850\n",
      "134900\n",
      "134950\n",
      "135000\n",
      "135050\n",
      "135100\n",
      "135150\n",
      "135200\n",
      "135250\n",
      "135300\n",
      "135350\n",
      "135400\n",
      "135450\n",
      "135500\n",
      "135550\n",
      "135600\n",
      "135650\n",
      "135700\n",
      "135750\n",
      "135800\n",
      "135850\n",
      "135900\n",
      "135950\n",
      "136000\n",
      "136050\n",
      "136100\n",
      "136150\n",
      "136200\n",
      "136250\n",
      "136300\n",
      "136350\n",
      "136400\n",
      "136450\n",
      "Epoch: 2/20...  Training Step: 6700...  Training loss: 1.6380...  0.0799 sec/batch\n",
      "136500\n",
      "136550\n",
      "136600\n",
      "136650\n",
      "136700\n",
      "136750\n",
      "136800\n",
      "136850\n",
      "136900\n",
      "136950\n",
      "137000\n",
      "137050\n",
      "137100\n",
      "137150\n",
      "137200\n",
      "137250\n",
      "137300\n",
      "137350\n",
      "137400\n",
      "137450\n",
      "137500\n",
      "137550\n",
      "137600\n",
      "137650\n",
      "137700\n",
      "137750\n",
      "137800\n",
      "137850\n",
      "137900\n",
      "137950\n",
      "138000\n",
      "138050\n",
      "138100\n",
      "138150\n",
      "138200\n",
      "138250\n",
      "138300\n",
      "138350\n",
      "138400\n",
      "138450\n",
      "138500\n",
      "138550\n",
      "138600\n",
      "138650\n",
      "138700\n",
      "138750\n",
      "138800\n",
      "138850\n",
      "138900\n",
      "138950\n",
      "Epoch: 2/20...  Training Step: 6750...  Training loss: 1.6096...  0.0763 sec/batch\n",
      "139000\n",
      "139050\n",
      "139100\n",
      "139150\n",
      "139200\n",
      "139250\n",
      "139300\n",
      "139350\n",
      "139400\n",
      "139450\n",
      "139500\n",
      "139550\n",
      "139600\n",
      "139650\n",
      "139700\n",
      "139750\n",
      "139800\n",
      "139850\n",
      "139900\n",
      "139950\n",
      "140000\n",
      "140050\n",
      "140100\n",
      "140150\n",
      "140200\n",
      "140250\n",
      "140300\n",
      "140350\n",
      "140400\n",
      "140450\n",
      "140500\n",
      "140550\n",
      "140600\n",
      "140650\n",
      "140700\n",
      "140750\n",
      "140800\n",
      "140850\n",
      "140900\n",
      "140950\n",
      "141000\n",
      "141050\n",
      "141100\n",
      "141150\n",
      "141200\n",
      "141250\n",
      "141300\n",
      "141350\n",
      "141400\n",
      "141450\n",
      "Epoch: 2/20...  Training Step: 6800...  Training loss: 1.6855...  0.0818 sec/batch\n",
      "141500\n",
      "141550\n",
      "141600\n",
      "141650\n",
      "141700\n",
      "141750\n",
      "141800\n",
      "141850\n",
      "141900\n",
      "141950\n",
      "142000\n",
      "142050\n",
      "142100\n",
      "142150\n",
      "142200\n",
      "142250\n",
      "142300\n",
      "142350\n",
      "142400\n",
      "142450\n",
      "142500\n",
      "142550\n",
      "142600\n",
      "142650\n",
      "142700\n",
      "142750\n",
      "142800\n",
      "142850\n",
      "142900\n",
      "142950\n",
      "143000\n",
      "143050\n",
      "143100\n",
      "143150\n",
      "143200\n",
      "143250\n",
      "143300\n",
      "143350\n",
      "143400\n",
      "143450\n",
      "143500\n",
      "143550\n",
      "143600\n",
      "143650\n",
      "143700\n",
      "143750\n",
      "143800\n",
      "143850\n",
      "143900\n",
      "143950\n",
      "Epoch: 2/20...  Training Step: 6850...  Training loss: 1.6254...  0.0767 sec/batch\n",
      "144000\n",
      "144050\n",
      "144100\n",
      "144150\n",
      "144200\n",
      "144250\n",
      "144300\n",
      "144350\n",
      "144400\n",
      "144450\n",
      "144500\n",
      "144550\n",
      "144600\n",
      "144650\n",
      "144700\n",
      "144750\n",
      "144800\n",
      "144850\n",
      "144900\n",
      "144950\n",
      "145000\n",
      "145050\n",
      "145100\n",
      "145150\n",
      "145200\n",
      "145250\n",
      "145300\n",
      "145350\n",
      "145400\n",
      "145450\n",
      "145500\n",
      "145550\n",
      "145600\n",
      "145650\n",
      "145700\n",
      "145750\n",
      "145800\n",
      "145850\n",
      "145900\n",
      "145950\n",
      "146000\n",
      "146050\n",
      "146100\n",
      "146150\n",
      "146200\n",
      "146250\n",
      "146300\n",
      "146350\n",
      "146400\n",
      "146450\n",
      "Epoch: 2/20...  Training Step: 6900...  Training loss: 1.6201...  0.0760 sec/batch\n",
      "146500\n",
      "146550\n",
      "146600\n",
      "146650\n",
      "146700\n",
      "146750\n",
      "146800\n",
      "146850\n",
      "146900\n",
      "146950\n",
      "147000\n",
      "147050\n",
      "147100\n",
      "147150\n",
      "147200\n",
      "147250\n",
      "147300\n",
      "147350\n",
      "147400\n",
      "147450\n",
      "147500\n",
      "147550\n",
      "147600\n",
      "147650\n",
      "147700\n",
      "147750\n",
      "147800\n",
      "147850\n",
      "147900\n",
      "147950\n",
      "148000\n",
      "148050\n",
      "148100\n",
      "148150\n",
      "148200\n",
      "148250\n",
      "148300\n",
      "148350\n",
      "148400\n",
      "148450\n",
      "148500\n",
      "148550\n",
      "148600\n",
      "148650\n",
      "148700\n",
      "148750\n",
      "148800\n",
      "148850\n",
      "148900\n",
      "148950\n",
      "Epoch: 2/20...  Training Step: 6950...  Training loss: 1.8850...  0.0808 sec/batch\n",
      "149000\n",
      "149050\n",
      "149100\n",
      "149150\n",
      "149200\n",
      "149250\n",
      "149300\n",
      "149350\n",
      "149400\n",
      "149450\n",
      "149500\n",
      "149550\n",
      "149600\n",
      "149650\n",
      "149700\n",
      "149750\n",
      "149800\n",
      "149850\n",
      "149900\n",
      "149950\n",
      "150000\n",
      "150050\n",
      "150100\n",
      "150150\n",
      "150200\n",
      "150250\n",
      "150300\n",
      "150350\n",
      "150400\n",
      "150450\n",
      "150500\n",
      "150550\n",
      "150600\n",
      "150650\n",
      "150700\n",
      "150750\n",
      "150800\n",
      "150850\n",
      "150900\n",
      "150950\n",
      "151000\n",
      "151050\n",
      "151100\n",
      "151150\n",
      "151200\n",
      "151250\n",
      "151300\n",
      "151350\n",
      "151400\n",
      "151450\n",
      "Epoch: 2/20...  Training Step: 7000...  Training loss: 1.5748...  0.0769 sec/batch\n",
      "151500\n",
      "151550\n",
      "151600\n",
      "151650\n",
      "151700\n",
      "151750\n",
      "151800\n",
      "151850\n",
      "151900\n",
      "151950\n",
      "152000\n",
      "152050\n",
      "152100\n",
      "152150\n",
      "152200\n",
      "152250\n",
      "152300\n",
      "152350\n",
      "152400\n",
      "152450\n",
      "152500\n",
      "152550\n",
      "152600\n",
      "152650\n",
      "152700\n",
      "152750\n",
      "152800\n",
      "152850\n",
      "152900\n",
      "152950\n",
      "153000\n",
      "153050\n",
      "153100\n",
      "153150\n",
      "153200\n",
      "153250\n",
      "153300\n",
      "153350\n",
      "153400\n",
      "153450\n",
      "153500\n",
      "153550\n",
      "153600\n",
      "153650\n",
      "153700\n",
      "153750\n",
      "153800\n",
      "153850\n",
      "153900\n",
      "153950\n",
      "Epoch: 2/20...  Training Step: 7050...  Training loss: 1.6664...  0.0745 sec/batch\n",
      "154000\n",
      "154050\n",
      "154100\n",
      "154150\n",
      "154200\n",
      "154250\n",
      "154300\n",
      "154350\n",
      "154400\n",
      "154450\n",
      "154500\n",
      "154550\n",
      "154600\n",
      "154650\n",
      "154700\n",
      "154750\n",
      "154800\n",
      "154850\n",
      "154900\n",
      "154950\n",
      "155000\n",
      "155050\n",
      "155100\n",
      "155150\n",
      "155200\n",
      "155250\n",
      "155300\n",
      "155350\n",
      "155400\n",
      "155450\n",
      "155500\n",
      "155550\n",
      "155600\n",
      "155650\n",
      "155700\n",
      "155750\n",
      "155800\n",
      "155850\n",
      "155900\n",
      "155950\n",
      "156000\n",
      "156050\n",
      "156100\n",
      "156150\n",
      "156200\n",
      "156250\n",
      "156300\n",
      "156350\n",
      "156400\n",
      "156450\n",
      "Epoch: 2/20...  Training Step: 7100...  Training loss: 1.6129...  0.0750 sec/batch\n",
      "156500\n",
      "156550\n",
      "156600\n",
      "156650\n",
      "156700\n",
      "156750\n",
      "156800\n",
      "156850\n",
      "156900\n",
      "156950\n",
      "157000\n",
      "157050\n",
      "157100\n",
      "157150\n",
      "157200\n",
      "157250\n",
      "157300\n",
      "157350\n",
      "157400\n",
      "157450\n",
      "157500\n",
      "157550\n",
      "157600\n",
      "157650\n",
      "157700\n",
      "157750\n",
      "157800\n",
      "157850\n",
      "157900\n",
      "157950\n",
      "158000\n",
      "158050\n",
      "158100\n",
      "158150\n",
      "158200\n",
      "158250\n",
      "158300\n",
      "158350\n",
      "158400\n",
      "158450\n",
      "158500\n",
      "158550\n",
      "158600\n",
      "158650\n",
      "158700\n",
      "158750\n",
      "158800\n",
      "158850\n",
      "158900\n",
      "158950\n",
      "Epoch: 2/20...  Training Step: 7150...  Training loss: 1.5520...  0.0804 sec/batch\n",
      "159000\n",
      "159050\n",
      "159100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159150\n",
      "159200\n",
      "159250\n",
      "159300\n",
      "159350\n",
      "159400\n",
      "159450\n",
      "159500\n",
      "159550\n",
      "159600\n",
      "159650\n",
      "159700\n",
      "159750\n",
      "159800\n",
      "159850\n",
      "159900\n",
      "159950\n",
      "160000\n",
      "160050\n",
      "160100\n",
      "160150\n",
      "160200\n",
      "160250\n",
      "160300\n",
      "160350\n",
      "160400\n",
      "160450\n",
      "160500\n",
      "160550\n",
      "160600\n",
      "160650\n",
      "160700\n",
      "160750\n",
      "160800\n",
      "160850\n",
      "160900\n",
      "160950\n",
      "161000\n",
      "161050\n",
      "161100\n",
      "161150\n",
      "161200\n",
      "161250\n",
      "161300\n",
      "161350\n",
      "161400\n",
      "161450\n",
      "Epoch: 2/20...  Training Step: 7200...  Training loss: 1.7397...  0.0752 sec/batch\n",
      "161500\n",
      "161550\n",
      "161600\n",
      "161650\n",
      "161700\n",
      "161750\n",
      "161800\n",
      "161850\n",
      "161900\n",
      "161950\n",
      "162000\n",
      "162050\n",
      "162100\n",
      "162150\n",
      "162200\n",
      "162250\n",
      "162300\n",
      "162350\n",
      "162400\n",
      "162450\n",
      "162500\n",
      "162550\n",
      "162600\n",
      "162650\n",
      "162700\n",
      "162750\n",
      "162800\n",
      "162850\n",
      "162900\n",
      "162950\n",
      "163000\n",
      "163050\n",
      "163100\n",
      "163150\n",
      "163200\n",
      "163250\n",
      "163300\n",
      "163350\n",
      "163400\n",
      "163450\n",
      "163500\n",
      "163550\n",
      "163600\n",
      "163650\n",
      "163700\n",
      "163750\n",
      "163800\n",
      "163850\n",
      "163900\n",
      "163950\n",
      "Epoch: 2/20...  Training Step: 7250...  Training loss: 1.5302...  0.0851 sec/batch\n",
      "164000\n",
      "164050\n",
      "164100\n",
      "164150\n",
      "164200\n",
      "164250\n",
      "164300\n",
      "164350\n",
      "164400\n",
      "164450\n",
      "164500\n",
      "164550\n",
      "164600\n",
      "164650\n",
      "164700\n",
      "164750\n",
      "164800\n",
      "164850\n",
      "164900\n",
      "164950\n",
      "165000\n",
      "165050\n",
      "165100\n",
      "165150\n",
      "165200\n",
      "165250\n",
      "165300\n",
      "165350\n",
      "165400\n",
      "165450\n",
      "165500\n",
      "165550\n",
      "165600\n",
      "165650\n",
      "165700\n",
      "165750\n",
      "165800\n",
      "165850\n",
      "165900\n",
      "165950\n",
      "166000\n",
      "166050\n",
      "166100\n",
      "166150\n",
      "166200\n",
      "166250\n",
      "166300\n",
      "166350\n",
      "166400\n",
      "166450\n",
      "Epoch: 2/20...  Training Step: 7300...  Training loss: 1.7827...  0.1037 sec/batch\n",
      "166500\n",
      "166550\n",
      "166600\n",
      "166650\n",
      "166700\n",
      "166750\n",
      "166800\n",
      "166850\n",
      "166900\n",
      "166950\n",
      "167000\n",
      "167050\n",
      "167100\n",
      "167150\n",
      "167200\n",
      "167250\n",
      "167300\n",
      "167350\n",
      "167400\n",
      "167450\n",
      "167500\n",
      "167550\n",
      "167600\n",
      "167650\n",
      "167700\n",
      "167750\n",
      "167800\n",
      "167850\n",
      "167900\n",
      "167950\n",
      "168000\n",
      "168050\n",
      "168100\n",
      "168150\n",
      "168200\n",
      "168250\n",
      "168300\n",
      "168350\n",
      "168400\n",
      "168450\n",
      "168500\n",
      "168550\n",
      "168600\n",
      "168650\n",
      "168700\n",
      "168750\n",
      "168800\n",
      "168850\n",
      "168900\n",
      "168950\n",
      "Epoch: 2/20...  Training Step: 7350...  Training loss: 1.5533...  0.1241 sec/batch\n",
      "169000\n",
      "169050\n",
      "169100\n",
      "169150\n",
      "169200\n",
      "169250\n",
      "169300\n",
      "169350\n",
      "169400\n",
      "169450\n",
      "169500\n",
      "169550\n",
      "169600\n",
      "169650\n",
      "169700\n",
      "169750\n",
      "169800\n",
      "169850\n",
      "169900\n",
      "169950\n",
      "170000\n",
      "170050\n",
      "170100\n",
      "170150\n",
      "170200\n",
      "170250\n",
      "170300\n",
      "170350\n",
      "170400\n",
      "170450\n",
      "170500\n",
      "170550\n",
      "170600\n",
      "170650\n",
      "170700\n",
      "170750\n",
      "170800\n",
      "170850\n",
      "170900\n",
      "170950\n",
      "171000\n",
      "171050\n",
      "171100\n",
      "171150\n",
      "171200\n",
      "171250\n",
      "171300\n",
      "171350\n",
      "171400\n",
      "171450\n",
      "Epoch: 2/20...  Training Step: 7400...  Training loss: 1.5731...  0.1122 sec/batch\n",
      "171500\n",
      "171550\n",
      "171600\n",
      "171650\n",
      "171700\n",
      "171750\n",
      "171800\n",
      "171850\n",
      "171900\n",
      "171950\n",
      "172000\n",
      "172050\n",
      "172100\n",
      "172150\n",
      "172200\n",
      "172250\n",
      "172300\n",
      "172350\n",
      "172400\n",
      "172450\n",
      "172500\n",
      "172550\n",
      "172600\n",
      "172650\n",
      "172700\n",
      "172750\n",
      "172800\n",
      "172850\n",
      "172900\n",
      "172950\n",
      "173000\n",
      "173050\n",
      "173100\n",
      "173150\n",
      "173200\n",
      "173250\n",
      "173300\n",
      "173350\n",
      "173400\n",
      "173450\n",
      "173500\n",
      "173550\n",
      "173600\n",
      "173650\n",
      "173700\n",
      "173750\n",
      "173800\n",
      "173850\n",
      "173900\n",
      "173950\n",
      "Epoch: 2/20...  Training Step: 7450...  Training loss: 1.4828...  0.1351 sec/batch\n",
      "174000\n",
      "174050\n",
      "174100\n",
      "174150\n",
      "174200\n",
      "174250\n",
      "174300\n",
      "174350\n",
      "174400\n",
      "174450\n",
      "174500\n",
      "174550\n",
      "174600\n",
      "174650\n",
      "174700\n",
      "174750\n",
      "174800\n",
      "174850\n",
      "174900\n",
      "174950\n",
      "175000\n",
      "175050\n",
      "175100\n",
      "175150\n",
      "175200\n",
      "175250\n",
      "175300\n",
      "175350\n",
      "175400\n",
      "175450\n",
      "175500\n",
      "175550\n",
      "175600\n",
      "175650\n",
      "175700\n",
      "175750\n",
      "175800\n",
      "175850\n",
      "175900\n",
      "175950\n",
      "176000\n",
      "176050\n",
      "176100\n",
      "176150\n",
      "176200\n",
      "176250\n",
      "176300\n",
      "176350\n",
      "176400\n",
      "176450\n",
      "Epoch: 2/20...  Training Step: 7500...  Training loss: 1.5647...  0.1082 sec/batch\n",
      "176500\n",
      "176550\n",
      "176600\n",
      "176650\n",
      "176700\n",
      "176750\n",
      "176800\n",
      "176850\n",
      "176900\n",
      "176950\n",
      "177000\n",
      "177050\n",
      "177100\n",
      "177150\n",
      "177200\n",
      "177250\n",
      "177300\n",
      "177350\n",
      "177400\n",
      "177450\n",
      "177500\n",
      "177550\n",
      "177600\n",
      "177650\n",
      "177700\n",
      "177750\n",
      "177800\n",
      "177850\n",
      "177900\n",
      "177950\n",
      "178000\n",
      "178050\n",
      "178100\n",
      "178150\n",
      "178200\n",
      "178250\n",
      "178300\n",
      "178350\n",
      "178400\n",
      "178450\n",
      "178500\n",
      "178550\n",
      "178600\n",
      "178650\n",
      "178700\n",
      "178750\n",
      "178800\n",
      "178850\n",
      "178900\n",
      "178950\n",
      "Epoch: 2/20...  Training Step: 7550...  Training loss: 1.5775...  0.0810 sec/batch\n",
      "179000\n",
      "179050\n",
      "179100\n",
      "179150\n",
      "179200\n",
      "179250\n",
      "179300\n",
      "179350\n",
      "179400\n",
      "179450\n",
      "179500\n",
      "179550\n",
      "179600\n",
      "179650\n",
      "179700\n",
      "179750\n",
      "179800\n",
      "179850\n",
      "179900\n",
      "179950\n",
      "180000\n",
      "180050\n",
      "180100\n",
      "180150\n",
      "180200\n",
      "180250\n",
      "180300\n",
      "180350\n",
      "180400\n",
      "180450\n",
      "180500\n",
      "180550\n",
      "180600\n",
      "180650\n",
      "180700\n",
      "180750\n",
      "180800\n",
      "180850\n",
      "180900\n",
      "180950\n",
      "181000\n",
      "181050\n",
      "181100\n",
      "181150\n",
      "181200\n",
      "181250\n",
      "181300\n",
      "181350\n",
      "181400\n",
      "181450\n",
      "Epoch: 2/20...  Training Step: 7600...  Training loss: 1.6626...  0.0863 sec/batch\n",
      "181500\n",
      "181550\n",
      "181600\n",
      "181650\n",
      "181700\n",
      "181750\n",
      "181800\n",
      "181850\n",
      "181900\n",
      "181950\n",
      "182000\n",
      "182050\n",
      "182100\n",
      "182150\n",
      "182200\n",
      "182250\n",
      "182300\n",
      "182350\n",
      "182400\n",
      "182450\n",
      "182500\n",
      "182550\n",
      "182600\n",
      "182650\n",
      "182700\n",
      "182750\n",
      "182800\n",
      "182850\n",
      "182900\n",
      "182950\n",
      "183000\n",
      "183050\n",
      "183100\n",
      "183150\n",
      "183200\n",
      "183250\n",
      "183300\n",
      "183350\n",
      "183400\n",
      "183450\n",
      "183500\n",
      "183550\n",
      "183600\n",
      "183650\n",
      "183700\n",
      "183750\n",
      "183800\n",
      "183850\n",
      "183900\n",
      "183950\n",
      "Epoch: 2/20...  Training Step: 7650...  Training loss: 1.7842...  0.0808 sec/batch\n",
      "184000\n",
      "184050\n",
      "184100\n",
      "184150\n",
      "184200\n",
      "184250\n",
      "184300\n",
      "184350\n",
      "184400\n",
      "184450\n",
      "184500\n",
      "184550\n",
      "184600\n",
      "184650\n",
      "184700\n",
      "184750\n",
      "184800\n",
      "184850\n",
      "184900\n",
      "184950\n",
      "185000\n",
      "185050\n",
      "185100\n",
      "185150\n",
      "185200\n",
      "185250\n",
      "185300\n",
      "185350\n",
      "185400\n",
      "185450\n",
      "185500\n",
      "185550\n",
      "185600\n",
      "185650\n",
      "185700\n",
      "185750\n",
      "185800\n",
      "185850\n",
      "185900\n",
      "185950\n",
      "186000\n",
      "186050\n",
      "186100\n",
      "186150\n",
      "186200\n",
      "186250\n",
      "186300\n",
      "186350\n",
      "186400\n",
      "186450\n",
      "Epoch: 2/20...  Training Step: 7700...  Training loss: 1.8701...  0.0865 sec/batch\n",
      "186500\n",
      "186550\n",
      "186600\n",
      "186650\n",
      "186700\n",
      "186750\n",
      "186800\n",
      "186850\n",
      "186900\n",
      "186950\n",
      "187000\n",
      "187050\n",
      "187100\n",
      "187150\n",
      "187200\n",
      "187250\n",
      "187300\n",
      "187350\n",
      "187400\n",
      "187450\n",
      "187500\n",
      "187550\n",
      "187600\n",
      "187650\n",
      "187700\n",
      "187750\n",
      "187800\n",
      "187850\n",
      "187900\n",
      "187950\n",
      "188000\n",
      "188050\n",
      "188100\n",
      "188150\n",
      "188200\n",
      "188250\n",
      "188300\n",
      "188350\n",
      "188400\n",
      "188450\n",
      "188500\n",
      "188550\n",
      "188600\n",
      "188650\n",
      "188700\n",
      "188750\n",
      "188800\n",
      "188850\n",
      "188900\n",
      "188950\n",
      "Epoch: 2/20...  Training Step: 7750...  Training loss: 1.8680...  0.0834 sec/batch\n",
      "189000\n",
      "189050\n",
      "189100\n",
      "189150\n",
      "189200\n",
      "189250\n",
      "189300\n",
      "189350\n",
      "189400\n",
      "189450\n",
      "189500\n",
      "189550\n",
      "189600\n",
      "189650\n",
      "189700\n",
      "189750\n",
      "189800\n",
      "189850\n",
      "189900\n",
      "189950\n",
      "190000\n",
      "190050\n",
      "190100\n",
      "190150\n",
      "190200\n",
      "190250\n",
      "190300\n",
      "190350\n",
      "190400\n",
      "190450\n",
      "190500\n",
      "190550\n",
      "190600\n",
      "190650\n",
      "190700\n",
      "190750\n",
      "190800\n",
      "190850\n",
      "190900\n",
      "190950\n",
      "191000\n",
      "191050\n",
      "191100\n",
      "191150\n",
      "191200\n",
      "191250\n",
      "191300\n",
      "191350\n",
      "191400\n",
      "191450\n",
      "Epoch: 2/20...  Training Step: 7800...  Training loss: 1.7547...  0.0887 sec/batch\n",
      "191500\n",
      "191550\n",
      "191600\n",
      "191650\n",
      "191700\n",
      "191750\n",
      "191800\n",
      "191850\n",
      "191900\n",
      "191950\n",
      "192000\n",
      "192050\n",
      "192100\n",
      "192150\n",
      "192200\n",
      "192250\n",
      "192300\n",
      "192350\n",
      "192400\n",
      "192450\n",
      "192500\n",
      "192550\n",
      "192600\n",
      "192650\n",
      "192700\n",
      "192750\n",
      "192800\n",
      "192850\n",
      "192900\n",
      "192950\n",
      "193000\n",
      "193050\n",
      "193100\n",
      "193150\n",
      "193200\n",
      "193250\n",
      "193300\n",
      "193350\n",
      "193400\n",
      "193450\n",
      "193500\n",
      "193550\n",
      "193600\n",
      "193650\n",
      "193700\n",
      "193750\n",
      "193800\n",
      "193850\n",
      "193900\n",
      "193950\n",
      "Epoch: 2/20...  Training Step: 7850...  Training loss: 1.6828...  0.0942 sec/batch\n",
      "194000\n",
      "194050\n",
      "194100\n",
      "194150\n",
      "194200\n",
      "194250\n",
      "194300\n",
      "194350\n",
      "194400\n",
      "194450\n",
      "194500\n",
      "194550\n",
      "194600\n",
      "194650\n",
      "194700\n",
      "194750\n",
      "194800\n",
      "194850\n",
      "194900\n",
      "194950\n",
      "195000\n",
      "195050\n",
      "195100\n",
      "195150\n",
      "195200\n",
      "195250\n",
      "195300\n",
      "195350\n",
      "195400\n",
      "195450\n",
      "195500\n",
      "195550\n",
      "195600\n",
      "195650\n",
      "195700\n",
      "195750\n",
      "195800\n",
      "195850\n",
      "195900\n",
      "195950\n",
      "196000\n",
      "196050\n",
      "196100\n",
      "196150\n",
      "196200\n",
      "196250\n",
      "196300\n",
      "196350\n",
      "196400\n",
      "196450\n",
      "Epoch: 2/20...  Training Step: 7900...  Training loss: 1.7609...  0.0761 sec/batch\n",
      "196500\n",
      "196550\n",
      "196600\n",
      "196650\n",
      "196700\n",
      "196750\n",
      "196800\n",
      "196850\n",
      "196900\n",
      "196950\n",
      "197000\n",
      "197050\n",
      "197100\n",
      "197150\n",
      "197200\n",
      "197250\n",
      "197300\n",
      "197350\n",
      "197400\n",
      "197450\n",
      "197500\n",
      "197550\n",
      "197600\n",
      "197650\n",
      "197700\n",
      "197750\n",
      "197800\n",
      "197850\n",
      "197900\n",
      "197950\n",
      "198000\n",
      "198050\n",
      "198100\n",
      "198150\n",
      "198200\n",
      "198250\n",
      "198300\n",
      "198350\n",
      "198400\n",
      "198450\n",
      "3970\n",
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "Epoch: 3/20...  Training Step: 7950...  Training loss: 1.6897...  0.0884 sec/batch\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "800\n",
      "850\n",
      "900\n",
      "950\n",
      "1000\n",
      "1050\n",
      "1100\n",
      "1150\n",
      "1200\n",
      "1250\n",
      "1300\n",
      "1350\n",
      "1400\n",
      "1450\n",
      "1500\n",
      "1550\n",
      "1600\n",
      "1650\n",
      "1700\n",
      "1750\n",
      "1800\n",
      "1850\n",
      "1900\n",
      "1950\n",
      "2000\n",
      "2050\n",
      "2100\n",
      "2150\n",
      "2200\n",
      "2250\n",
      "2300\n",
      "2350\n",
      "2400\n",
      "2450\n",
      "2500\n",
      "2550\n",
      "2600\n",
      "2650\n",
      "2700\n",
      "2750\n",
      "2800\n",
      "2850\n",
      "2900\n",
      "2950\n",
      "Epoch: 3/20...  Training Step: 8000...  Training loss: 1.6755...  0.0991 sec/batch\n",
      "3000\n",
      "3050\n",
      "3100\n",
      "3150\n",
      "3200\n",
      "3250\n",
      "3300\n",
      "3350\n",
      "3400\n",
      "3450\n",
      "3500\n",
      "3550\n",
      "3600\n",
      "3650\n",
      "3700\n",
      "3750\n",
      "3800\n",
      "3850\n",
      "3900\n",
      "3950\n",
      "4000\n",
      "4050\n",
      "4100\n",
      "4150\n",
      "4200\n",
      "4250\n",
      "4300\n",
      "4350\n",
      "4400\n",
      "4450\n",
      "4500\n",
      "4550\n",
      "4600\n",
      "4650\n",
      "4700\n",
      "4750\n",
      "4800\n",
      "4850\n",
      "4900\n",
      "4950\n",
      "5000\n",
      "5050\n",
      "5100\n",
      "5150\n",
      "5200\n",
      "5250\n",
      "5300\n",
      "5350\n",
      "5400\n",
      "5450\n",
      "Epoch: 3/20...  Training Step: 8050...  Training loss: 1.6548...  0.0781 sec/batch\n",
      "5500\n",
      "5550\n",
      "5600\n",
      "5650\n",
      "5700\n",
      "5750\n",
      "5800\n",
      "5850\n",
      "5900\n",
      "5950\n",
      "6000\n",
      "6050\n",
      "6100\n",
      "6150\n",
      "6200\n",
      "6250\n",
      "6300\n",
      "6350\n",
      "6400\n",
      "6450\n",
      "6500\n",
      "6550\n",
      "6600\n",
      "6650\n",
      "6700\n",
      "6750\n",
      "6800\n",
      "6850\n",
      "6900\n",
      "6950\n",
      "7000\n",
      "7050\n",
      "7100\n",
      "7150\n",
      "7200\n",
      "7250\n",
      "7300\n",
      "7350\n",
      "7400\n",
      "7450\n",
      "7500\n",
      "7550\n",
      "7600\n",
      "7650\n",
      "7700\n",
      "7750\n",
      "7800\n",
      "7850\n",
      "7900\n",
      "7950\n",
      "Epoch: 3/20...  Training Step: 8100...  Training loss: 1.7978...  0.0755 sec/batch\n",
      "8000\n",
      "8050\n",
      "8100\n",
      "8150\n",
      "8200\n",
      "8250\n",
      "8300\n",
      "8350\n",
      "8400\n",
      "8450\n",
      "8500\n",
      "8550\n",
      "8600\n",
      "8650\n",
      "8700\n",
      "8750\n",
      "8800\n",
      "8850\n",
      "8900\n",
      "8950\n",
      "9000\n",
      "9050\n",
      "9100\n",
      "9150\n",
      "9200\n",
      "9250\n",
      "9300\n",
      "9350\n",
      "9400\n",
      "9450\n",
      "9500\n",
      "9550\n",
      "9600\n",
      "9650\n",
      "9700\n",
      "9750\n",
      "9800\n",
      "9850\n",
      "9900\n",
      "9950\n",
      "10000\n",
      "10050\n",
      "10100\n",
      "10150\n",
      "10200\n",
      "10250\n",
      "10300\n",
      "10350\n",
      "10400\n",
      "10450\n",
      "Epoch: 3/20...  Training Step: 8150...  Training loss: 1.6195...  0.0827 sec/batch\n",
      "10500\n",
      "10550\n",
      "10600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10650\n",
      "10700\n",
      "10750\n",
      "10800\n",
      "10850\n",
      "10900\n",
      "10950\n",
      "11000\n",
      "11050\n",
      "11100\n",
      "11150\n",
      "11200\n",
      "11250\n",
      "11300\n",
      "11350\n",
      "11400\n",
      "11450\n",
      "11500\n",
      "11550\n",
      "11600\n",
      "11650\n",
      "11700\n",
      "11750\n",
      "11800\n",
      "11850\n",
      "11900\n",
      "11950\n",
      "12000\n",
      "12050\n",
      "12100\n",
      "12150\n",
      "12200\n",
      "12250\n",
      "12300\n",
      "12350\n",
      "12400\n",
      "12450\n",
      "12500\n",
      "12550\n",
      "12600\n",
      "12650\n",
      "12700\n",
      "12750\n",
      "12800\n",
      "12850\n",
      "12900\n",
      "12950\n",
      "Epoch: 3/20...  Training Step: 8200...  Training loss: 1.5919...  0.0750 sec/batch\n",
      "13000\n",
      "13050\n",
      "13100\n",
      "13150\n",
      "13200\n",
      "13250\n",
      "13300\n",
      "13350\n",
      "13400\n",
      "13450\n",
      "13500\n",
      "13550\n",
      "13600\n",
      "13650\n",
      "13700\n",
      "13750\n",
      "13800\n",
      "13850\n",
      "13900\n",
      "13950\n",
      "14000\n",
      "14050\n",
      "14100\n",
      "14150\n",
      "14200\n",
      "14250\n",
      "14300\n",
      "14350\n",
      "14400\n",
      "14450\n",
      "14500\n",
      "14550\n",
      "14600\n",
      "14650\n",
      "14700\n",
      "14750\n",
      "14800\n",
      "14850\n",
      "14900\n",
      "14950\n",
      "15000\n",
      "15050\n",
      "15100\n",
      "15150\n",
      "15200\n",
      "15250\n",
      "15300\n",
      "15350\n",
      "15400\n",
      "15450\n",
      "Epoch: 3/20...  Training Step: 8250...  Training loss: 1.7469...  0.1512 sec/batch\n",
      "15500\n",
      "15550\n",
      "15600\n",
      "15650\n",
      "15700\n",
      "15750\n",
      "15800\n",
      "15850\n",
      "15900\n",
      "15950\n",
      "16000\n",
      "16050\n",
      "16100\n",
      "16150\n",
      "16200\n",
      "16250\n",
      "16300\n",
      "16350\n",
      "16400\n",
      "16450\n",
      "16500\n",
      "16550\n",
      "16600\n",
      "16650\n",
      "16700\n",
      "16750\n",
      "16800\n",
      "16850\n",
      "16900\n",
      "16950\n",
      "17000\n",
      "17050\n",
      "17100\n",
      "17150\n",
      "17200\n",
      "17250\n",
      "17300\n",
      "17350\n",
      "17400\n",
      "17450\n",
      "17500\n",
      "17550\n",
      "17600\n",
      "17650\n",
      "17700\n",
      "17750\n",
      "17800\n",
      "17850\n",
      "17900\n",
      "17950\n",
      "Epoch: 3/20...  Training Step: 8300...  Training loss: 1.7713...  0.0821 sec/batch\n",
      "18000\n",
      "18050\n",
      "18100\n",
      "18150\n",
      "18200\n",
      "18250\n",
      "18300\n",
      "18350\n",
      "18400\n",
      "18450\n",
      "18500\n",
      "18550\n",
      "18600\n",
      "18650\n",
      "18700\n",
      "18750\n",
      "18800\n",
      "18850\n",
      "18900\n",
      "18950\n",
      "19000\n",
      "19050\n",
      "19100\n",
      "19150\n",
      "19200\n",
      "19250\n",
      "19300\n",
      "19350\n",
      "19400\n",
      "19450\n",
      "19500\n",
      "19550\n",
      "19600\n",
      "19650\n",
      "19700\n",
      "19750\n",
      "19800\n",
      "19850\n",
      "19900\n",
      "19950\n",
      "20000\n",
      "20050\n",
      "20100\n",
      "20150\n",
      "20200\n",
      "20250\n",
      "20300\n",
      "20350\n",
      "20400\n",
      "20450\n",
      "Epoch: 3/20...  Training Step: 8350...  Training loss: 1.6965...  0.0857 sec/batch\n",
      "20500\n",
      "20550\n",
      "20600\n",
      "20650\n",
      "20700\n",
      "20750\n",
      "20800\n",
      "20850\n",
      "20900\n",
      "20950\n",
      "21000\n",
      "21050\n",
      "21100\n",
      "21150\n",
      "21200\n",
      "21250\n",
      "21300\n",
      "21350\n",
      "21400\n",
      "21450\n",
      "21500\n",
      "21550\n",
      "21600\n",
      "21650\n",
      "21700\n",
      "21750\n",
      "21800\n",
      "21850\n",
      "21900\n",
      "21950\n",
      "22000\n",
      "22050\n",
      "22100\n",
      "22150\n",
      "22200\n",
      "22250\n",
      "22300\n",
      "22350\n",
      "22400\n",
      "22450\n",
      "22500\n",
      "22550\n",
      "22600\n",
      "22650\n",
      "22700\n",
      "22750\n",
      "22800\n",
      "22850\n",
      "22900\n",
      "22950\n",
      "Epoch: 3/20...  Training Step: 8400...  Training loss: 1.8202...  0.0824 sec/batch\n",
      "23000\n",
      "23050\n",
      "23100\n",
      "23150\n",
      "23200\n",
      "23250\n",
      "23300\n",
      "23350\n",
      "23400\n",
      "23450\n",
      "23500\n",
      "23550\n",
      "23600\n",
      "23650\n",
      "23700\n",
      "23750\n",
      "23800\n",
      "23850\n",
      "23900\n",
      "23950\n",
      "24000\n",
      "24050\n",
      "24100\n",
      "24150\n",
      "24200\n",
      "24250\n",
      "24300\n",
      "24350\n",
      "24400\n",
      "24450\n",
      "24500\n",
      "24550\n",
      "24600\n",
      "24650\n",
      "24700\n",
      "24750\n",
      "24800\n",
      "24850\n",
      "24900\n",
      "24950\n",
      "25000\n",
      "25050\n",
      "25100\n",
      "25150\n",
      "25200\n",
      "25250\n",
      "25300\n",
      "25350\n",
      "25400\n",
      "25450\n",
      "Epoch: 3/20...  Training Step: 8450...  Training loss: 1.7208...  0.0925 sec/batch\n",
      "25500\n",
      "25550\n",
      "25600\n",
      "25650\n",
      "25700\n",
      "25750\n",
      "25800\n",
      "25850\n",
      "25900\n",
      "25950\n",
      "26000\n",
      "26050\n",
      "26100\n",
      "26150\n",
      "26200\n",
      "26250\n",
      "26300\n",
      "26350\n",
      "26400\n",
      "26450\n",
      "26500\n",
      "26550\n",
      "26600\n",
      "26650\n",
      "26700\n",
      "26750\n",
      "26800\n",
      "26850\n",
      "26900\n",
      "26950\n",
      "27000\n",
      "27050\n",
      "27100\n",
      "27150\n",
      "27200\n",
      "27250\n",
      "27300\n",
      "27350\n",
      "27400\n",
      "27450\n",
      "27500\n",
      "27550\n",
      "27600\n",
      "27650\n",
      "27700\n",
      "27750\n",
      "27800\n",
      "27850\n",
      "27900\n",
      "27950\n",
      "Epoch: 3/20...  Training Step: 8500...  Training loss: 1.6266...  0.0750 sec/batch\n",
      "28000\n",
      "28050\n",
      "28100\n",
      "28150\n",
      "28200\n",
      "28250\n",
      "28300\n",
      "28350\n",
      "28400\n",
      "28450\n",
      "28500\n",
      "28550\n",
      "28600\n",
      "28650\n",
      "28700\n",
      "28750\n",
      "28800\n",
      "28850\n",
      "28900\n",
      "28950\n",
      "29000\n",
      "29050\n",
      "29100\n",
      "29150\n",
      "29200\n",
      "29250\n",
      "29300\n",
      "29350\n",
      "29400\n",
      "29450\n",
      "29500\n",
      "29550\n",
      "29600\n",
      "29650\n",
      "29700\n",
      "29750\n",
      "29800\n",
      "29850\n",
      "29900\n",
      "29950\n",
      "30000\n",
      "30050\n",
      "30100\n",
      "30150\n",
      "30200\n",
      "30250\n",
      "30300\n",
      "30350\n",
      "30400\n",
      "30450\n",
      "Epoch: 3/20...  Training Step: 8550...  Training loss: 1.6487...  0.0840 sec/batch\n",
      "30500\n",
      "30550\n",
      "30600\n",
      "30650\n",
      "30700\n",
      "30750\n",
      "30800\n",
      "30850\n",
      "30900\n",
      "30950\n",
      "31000\n",
      "31050\n",
      "31100\n",
      "31150\n",
      "31200\n",
      "31250\n",
      "31300\n",
      "31350\n",
      "31400\n",
      "31450\n",
      "31500\n",
      "31550\n",
      "31600\n",
      "31650\n",
      "31700\n",
      "31750\n",
      "31800\n",
      "31850\n",
      "31900\n",
      "31950\n",
      "32000\n",
      "32050\n",
      "32100\n",
      "32150\n",
      "32200\n",
      "32250\n",
      "32300\n",
      "32350\n",
      "32400\n",
      "32450\n",
      "32500\n",
      "32550\n",
      "32600\n",
      "32650\n",
      "32700\n",
      "32750\n",
      "32800\n",
      "32850\n",
      "32900\n",
      "32950\n",
      "Epoch: 3/20...  Training Step: 8600...  Training loss: 1.5901...  0.0779 sec/batch\n",
      "33000\n",
      "33050\n",
      "33100\n",
      "33150\n",
      "33200\n",
      "33250\n",
      "33300\n",
      "33350\n",
      "33400\n",
      "33450\n",
      "33500\n",
      "33550\n",
      "33600\n",
      "33650\n",
      "33700\n",
      "33750\n",
      "33800\n",
      "33850\n",
      "33900\n",
      "33950\n",
      "34000\n",
      "34050\n",
      "34100\n",
      "34150\n",
      "34200\n",
      "34250\n",
      "34300\n",
      "34350\n",
      "34400\n",
      "34450\n",
      "34500\n",
      "34550\n",
      "34600\n",
      "34650\n",
      "34700\n",
      "34750\n",
      "34800\n",
      "34850\n",
      "34900\n",
      "34950\n",
      "35000\n",
      "35050\n",
      "35100\n",
      "35150\n",
      "35200\n",
      "35250\n",
      "35300\n",
      "35350\n",
      "35400\n",
      "35450\n",
      "Epoch: 3/20...  Training Step: 8650...  Training loss: 1.7013...  0.1202 sec/batch\n",
      "35500\n",
      "35550\n",
      "35600\n",
      "35650\n",
      "35700\n",
      "35750\n",
      "35800\n",
      "35850\n",
      "35900\n",
      "35950\n",
      "36000\n",
      "36050\n",
      "36100\n",
      "36150\n",
      "36200\n",
      "36250\n",
      "36300\n",
      "36350\n",
      "36400\n",
      "36450\n",
      "36500\n",
      "36550\n",
      "36600\n",
      "36650\n",
      "36700\n",
      "36750\n",
      "36800\n",
      "36850\n",
      "36900\n",
      "36950\n",
      "37000\n",
      "37050\n",
      "37100\n",
      "37150\n",
      "37200\n",
      "37250\n",
      "37300\n",
      "37350\n",
      "37400\n",
      "37450\n",
      "37500\n",
      "37550\n",
      "37600\n",
      "37650\n",
      "37700\n",
      "37750\n",
      "37800\n",
      "37850\n",
      "37900\n",
      "37950\n",
      "Epoch: 3/20...  Training Step: 8700...  Training loss: 1.6148...  0.0850 sec/batch\n",
      "38000\n",
      "38050\n",
      "38100\n",
      "38150\n",
      "38200\n",
      "38250\n",
      "38300\n",
      "38350\n",
      "38400\n",
      "38450\n",
      "38500\n",
      "38550\n",
      "38600\n",
      "38650\n",
      "38700\n",
      "38750\n",
      "38800\n",
      "38850\n",
      "38900\n",
      "38950\n",
      "39000\n",
      "39050\n",
      "39100\n",
      "39150\n",
      "39200\n",
      "39250\n",
      "39300\n",
      "39350\n",
      "39400\n",
      "39450\n",
      "39500\n",
      "39550\n",
      "39600\n",
      "39650\n",
      "39700\n",
      "39750\n",
      "39800\n",
      "39850\n",
      "39900\n",
      "39950\n",
      "40000\n",
      "40050\n",
      "40100\n",
      "40150\n",
      "40200\n",
      "40250\n",
      "40300\n",
      "40350\n",
      "40400\n",
      "40450\n",
      "Epoch: 3/20...  Training Step: 8750...  Training loss: 1.5890...  0.0769 sec/batch\n",
      "40500\n",
      "40550\n",
      "40600\n",
      "40650\n",
      "40700\n",
      "40750\n",
      "40800\n",
      "40850\n",
      "40900\n",
      "40950\n",
      "41000\n",
      "41050\n",
      "41100\n",
      "41150\n",
      "41200\n",
      "41250\n",
      "41300\n",
      "41350\n",
      "41400\n",
      "41450\n",
      "41500\n",
      "41550\n",
      "41600\n",
      "41650\n",
      "41700\n",
      "41750\n",
      "41800\n",
      "41850\n",
      "41900\n",
      "41950\n",
      "42000\n",
      "42050\n",
      "42100\n",
      "42150\n",
      "42200\n",
      "42250\n",
      "42300\n",
      "42350\n",
      "42400\n",
      "42450\n",
      "42500\n",
      "42550\n",
      "42600\n",
      "42650\n",
      "42700\n",
      "42750\n",
      "42800\n",
      "42850\n",
      "42900\n",
      "42950\n",
      "Epoch: 3/20...  Training Step: 8800...  Training loss: 1.8119...  0.0839 sec/batch\n",
      "43000\n",
      "43050\n",
      "43100\n",
      "43150\n",
      "43200\n",
      "43250\n",
      "43300\n",
      "43350\n",
      "43400\n",
      "43450\n",
      "43500\n",
      "43550\n",
      "43600\n",
      "43650\n",
      "43700\n",
      "43750\n",
      "43800\n",
      "43850\n",
      "43900\n",
      "43950\n",
      "44000\n",
      "44050\n",
      "44100\n",
      "44150\n",
      "44200\n",
      "44250\n",
      "44300\n",
      "44350\n",
      "44400\n",
      "44450\n",
      "44500\n",
      "44550\n",
      "44600\n",
      "44650\n",
      "44700\n",
      "44750\n",
      "44800\n",
      "44850\n",
      "44900\n",
      "44950\n",
      "45000\n",
      "45050\n",
      "45100\n",
      "45150\n",
      "45200\n",
      "45250\n",
      "45300\n",
      "45350\n",
      "45400\n",
      "45450\n",
      "Epoch: 3/20...  Training Step: 8850...  Training loss: 1.7050...  0.0753 sec/batch\n",
      "45500\n",
      "45550\n",
      "45600\n",
      "45650\n",
      "45700\n",
      "45750\n",
      "45800\n",
      "45850\n",
      "45900\n",
      "45950\n",
      "46000\n",
      "46050\n",
      "46100\n",
      "46150\n",
      "46200\n",
      "46250\n",
      "46300\n",
      "46350\n",
      "46400\n",
      "46450\n",
      "46500\n",
      "46550\n",
      "46600\n",
      "46650\n",
      "46700\n",
      "46750\n",
      "46800\n",
      "46850\n",
      "46900\n",
      "46950\n",
      "47000\n",
      "47050\n",
      "47100\n",
      "47150\n",
      "47200\n",
      "47250\n",
      "47300\n",
      "47350\n",
      "47400\n",
      "47450\n",
      "47500\n",
      "47550\n",
      "47600\n",
      "47650\n",
      "47700\n",
      "47750\n",
      "47800\n",
      "47850\n",
      "47900\n",
      "47950\n",
      "Epoch: 3/20...  Training Step: 8900...  Training loss: 1.7402...  0.0918 sec/batch\n",
      "48000\n",
      "48050\n",
      "48100\n",
      "48150\n",
      "48200\n",
      "48250\n",
      "48300\n",
      "48350\n",
      "48400\n",
      "48450\n",
      "48500\n",
      "48550\n",
      "48600\n",
      "48650\n",
      "48700\n",
      "48750\n",
      "48800\n",
      "48850\n",
      "48900\n",
      "48950\n",
      "49000\n",
      "49050\n",
      "49100\n",
      "49150\n",
      "49200\n",
      "49250\n",
      "49300\n",
      "49350\n",
      "49400\n",
      "49450\n",
      "49500\n",
      "49550\n",
      "49600\n",
      "49650\n",
      "49700\n",
      "49750\n",
      "49800\n",
      "49850\n",
      "49900\n",
      "49950\n",
      "50000\n",
      "50050\n",
      "50100\n",
      "50150\n",
      "50200\n",
      "50250\n",
      "50300\n",
      "50350\n",
      "50400\n",
      "50450\n",
      "Epoch: 3/20...  Training Step: 8950...  Training loss: 1.7182...  0.0951 sec/batch\n",
      "50500\n",
      "50550\n",
      "50600\n",
      "50650\n",
      "50700\n",
      "50750\n",
      "50800\n",
      "50850\n",
      "50900\n",
      "50950\n",
      "51000\n",
      "51050\n",
      "51100\n",
      "51150\n",
      "51200\n",
      "51250\n",
      "51300\n",
      "51350\n",
      "51400\n",
      "51450\n",
      "51500\n",
      "51550\n",
      "51600\n",
      "51650\n",
      "51700\n",
      "51750\n",
      "51800\n",
      "51850\n",
      "51900\n",
      "51950\n",
      "52000\n",
      "52050\n",
      "52100\n",
      "52150\n",
      "52200\n",
      "52250\n",
      "52300\n",
      "52350\n",
      "52400\n",
      "52450\n",
      "52500\n",
      "52550\n",
      "52600\n",
      "52650\n",
      "52700\n",
      "52750\n",
      "52800\n",
      "52850\n",
      "52900\n",
      "52950\n",
      "Epoch: 3/20...  Training Step: 9000...  Training loss: 1.6018...  0.0839 sec/batch\n",
      "53000\n",
      "53050\n",
      "53100\n",
      "53150\n",
      "53200\n",
      "53250\n",
      "53300\n",
      "53350\n",
      "53400\n",
      "53450\n",
      "53500\n",
      "53550\n",
      "53600\n",
      "53650\n",
      "53700\n",
      "53750\n",
      "53800\n",
      "53850\n",
      "53900\n",
      "53950\n",
      "54000\n",
      "54050\n",
      "54100\n",
      "54150\n",
      "54200\n",
      "54250\n",
      "54300\n",
      "54350\n",
      "54400\n",
      "54450\n",
      "54500\n",
      "54550\n",
      "54600\n",
      "54650\n",
      "54700\n",
      "54750\n",
      "54800\n",
      "54850\n",
      "54900\n",
      "54950\n",
      "55000\n",
      "55050\n",
      "55100\n",
      "55150\n",
      "55200\n",
      "55250\n",
      "55300\n",
      "55350\n",
      "55400\n",
      "55450\n",
      "Epoch: 3/20...  Training Step: 9050...  Training loss: 1.6612...  0.0893 sec/batch\n",
      "55500\n",
      "55550\n",
      "55600\n",
      "55650\n",
      "55700\n",
      "55750\n",
      "55800\n",
      "55850\n",
      "55900\n",
      "55950\n",
      "56000\n",
      "56050\n",
      "56100\n",
      "56150\n",
      "56200\n",
      "56250\n",
      "56300\n",
      "56350\n",
      "56400\n",
      "56450\n",
      "56500\n",
      "56550\n",
      "56600\n",
      "56650\n",
      "56700\n",
      "56750\n",
      "56800\n",
      "56850\n",
      "56900\n",
      "56950\n",
      "57000\n",
      "57050\n",
      "57100\n",
      "57150\n",
      "57200\n",
      "57250\n",
      "57300\n",
      "57350\n",
      "57400\n",
      "57450\n",
      "57500\n",
      "57550\n",
      "57600\n",
      "57650\n",
      "57700\n",
      "57750\n",
      "57800\n",
      "57850\n",
      "57900\n",
      "57950\n",
      "Epoch: 3/20...  Training Step: 9100...  Training loss: 1.4392...  0.0855 sec/batch\n",
      "58000\n",
      "58050\n",
      "58100\n",
      "58150\n",
      "58200\n",
      "58250\n",
      "58300\n",
      "58350\n",
      "58400\n",
      "58450\n",
      "58500\n",
      "58550\n",
      "58600\n",
      "58650\n",
      "58700\n",
      "58750\n",
      "58800\n",
      "58850\n",
      "58900\n",
      "58950\n",
      "59000\n",
      "59050\n",
      "59100\n",
      "59150\n",
      "59200\n",
      "59250\n",
      "59300\n",
      "59350\n",
      "59400\n",
      "59450\n",
      "59500\n",
      "59550\n",
      "59600\n",
      "59650\n",
      "59700\n",
      "59750\n",
      "59800\n",
      "59850\n",
      "59900\n",
      "59950\n",
      "60000\n",
      "60050\n",
      "60100\n",
      "60150\n",
      "60200\n",
      "60250\n",
      "60300\n",
      "60350\n",
      "60400\n",
      "60450\n",
      "Epoch: 3/20...  Training Step: 9150...  Training loss: 1.7037...  0.0973 sec/batch\n",
      "60500\n",
      "60550\n",
      "60600\n",
      "60650\n",
      "60700\n",
      "60750\n",
      "60800\n",
      "60850\n",
      "60900\n",
      "60950\n",
      "61000\n",
      "61050\n",
      "61100\n",
      "61150\n",
      "61200\n",
      "61250\n",
      "61300\n",
      "61350\n",
      "61400\n",
      "61450\n",
      "61500\n",
      "61550\n",
      "61600\n",
      "61650\n",
      "61700\n",
      "61750\n",
      "61800\n",
      "61850\n",
      "61900\n",
      "61950\n",
      "62000\n",
      "62050\n",
      "62100\n",
      "62150\n",
      "62200\n",
      "62250\n",
      "62300\n",
      "62350\n",
      "62400\n",
      "62450\n",
      "62500\n",
      "62550\n",
      "62600\n",
      "62650\n",
      "62700\n",
      "62750\n",
      "62800\n",
      "62850\n",
      "62900\n",
      "62950\n",
      "Epoch: 3/20...  Training Step: 9200...  Training loss: 1.6346...  0.0781 sec/batch\n",
      "63000\n",
      "63050\n",
      "63100\n",
      "63150\n",
      "63200\n",
      "63250\n",
      "63300\n",
      "63350\n",
      "63400\n",
      "63450\n",
      "63500\n",
      "63550\n",
      "63600\n",
      "63650\n",
      "63700\n",
      "63750\n",
      "63800\n",
      "63850\n",
      "63900\n",
      "63950\n",
      "64000\n",
      "64050\n",
      "64100\n",
      "64150\n",
      "64200\n",
      "64250\n",
      "64300\n",
      "64350\n",
      "64400\n",
      "64450\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64500\n",
      "64550\n",
      "64600\n",
      "64650\n",
      "64700\n",
      "64750\n",
      "64800\n",
      "64850\n",
      "64900\n",
      "64950\n",
      "65000\n",
      "65050\n",
      "65100\n",
      "65150\n",
      "65200\n",
      "65250\n",
      "65300\n",
      "65350\n",
      "65400\n",
      "65450\n",
      "Epoch: 3/20...  Training Step: 9250...  Training loss: 1.7305...  0.0875 sec/batch\n",
      "65500\n",
      "65550\n",
      "65600\n",
      "65650\n",
      "65700\n",
      "65750\n",
      "65800\n",
      "65850\n",
      "65900\n",
      "65950\n",
      "66000\n",
      "66050\n",
      "66100\n",
      "66150\n",
      "66200\n",
      "66250\n",
      "66300\n",
      "66350\n",
      "66400\n",
      "66450\n",
      "66500\n",
      "66550\n",
      "66600\n",
      "66650\n",
      "66700\n",
      "66750\n",
      "66800\n",
      "66850\n",
      "66900\n",
      "66950\n",
      "67000\n",
      "67050\n",
      "67100\n",
      "67150\n",
      "67200\n",
      "67250\n",
      "67300\n",
      "67350\n",
      "67400\n",
      "67450\n",
      "67500\n",
      "67550\n",
      "67600\n",
      "67650\n",
      "67700\n",
      "67750\n",
      "67800\n",
      "67850\n",
      "67900\n",
      "67950\n",
      "Epoch: 3/20...  Training Step: 9300...  Training loss: 1.5179...  0.0898 sec/batch\n",
      "68000\n",
      "68050\n",
      "68100\n",
      "68150\n",
      "68200\n",
      "68250\n",
      "68300\n",
      "68350\n",
      "68400\n",
      "68450\n",
      "68500\n",
      "68550\n",
      "68600\n",
      "68650\n",
      "68700\n",
      "68750\n",
      "68800\n",
      "68850\n",
      "68900\n",
      "68950\n",
      "69000\n",
      "69050\n",
      "69100\n",
      "69150\n",
      "69200\n",
      "69250\n",
      "69300\n",
      "69350\n",
      "69400\n",
      "69450\n",
      "69500\n",
      "69550\n",
      "69600\n",
      "69650\n",
      "69700\n",
      "69750\n",
      "69800\n",
      "69850\n",
      "69900\n",
      "69950\n",
      "70000\n",
      "70050\n",
      "70100\n",
      "70150\n",
      "70200\n",
      "70250\n",
      "70300\n",
      "70350\n",
      "70400\n",
      "70450\n",
      "Epoch: 3/20...  Training Step: 9350...  Training loss: 1.6865...  0.0941 sec/batch\n",
      "70500\n",
      "70550\n",
      "70600\n",
      "70650\n",
      "70700\n",
      "70750\n",
      "70800\n",
      "70850\n",
      "70900\n",
      "70950\n",
      "71000\n",
      "71050\n",
      "71100\n",
      "71150\n",
      "71200\n",
      "71250\n",
      "71300\n",
      "71350\n",
      "71400\n",
      "71450\n",
      "71500\n",
      "71550\n",
      "71600\n",
      "71650\n",
      "71700\n",
      "71750\n",
      "71800\n",
      "71850\n",
      "71900\n",
      "71950\n",
      "72000\n",
      "72050\n",
      "72100\n",
      "72150\n",
      "72200\n",
      "72250\n",
      "72300\n",
      "72350\n",
      "72400\n",
      "72450\n",
      "72500\n",
      "72550\n",
      "72600\n",
      "72650\n",
      "72700\n",
      "72750\n",
      "72800\n",
      "72850\n",
      "72900\n",
      "72950\n",
      "Epoch: 3/20...  Training Step: 9400...  Training loss: 1.6662...  0.0823 sec/batch\n",
      "73000\n",
      "73050\n",
      "73100\n",
      "73150\n",
      "73200\n",
      "73250\n",
      "73300\n",
      "73350\n",
      "73400\n",
      "73450\n",
      "73500\n",
      "73550\n",
      "73600\n",
      "73650\n",
      "73700\n",
      "73750\n",
      "73800\n",
      "73850\n",
      "73900\n",
      "73950\n",
      "74000\n",
      "74050\n",
      "74100\n",
      "74150\n",
      "74200\n",
      "74250\n",
      "74300\n",
      "74350\n",
      "74400\n",
      "74450\n",
      "74500\n",
      "74550\n",
      "74600\n",
      "74650\n",
      "74700\n",
      "74750\n",
      "74800\n",
      "74850\n",
      "74900\n",
      "74950\n",
      "75000\n",
      "75050\n",
      "75100\n",
      "75150\n",
      "75200\n",
      "75250\n",
      "75300\n",
      "75350\n",
      "75400\n",
      "75450\n",
      "Epoch: 3/20...  Training Step: 9450...  Training loss: 1.5648...  0.0821 sec/batch\n",
      "75500\n",
      "75550\n",
      "75600\n",
      "75650\n",
      "75700\n",
      "75750\n",
      "75800\n",
      "75850\n",
      "75900\n",
      "75950\n",
      "76000\n",
      "76050\n",
      "76100\n",
      "76150\n",
      "76200\n",
      "76250\n",
      "76300\n",
      "76350\n",
      "76400\n",
      "76450\n",
      "76500\n",
      "76550\n",
      "76600\n",
      "76650\n",
      "76700\n",
      "76750\n",
      "76800\n",
      "76850\n",
      "76900\n",
      "76950\n",
      "77000\n",
      "77050\n",
      "77100\n",
      "77150\n",
      "77200\n",
      "77250\n",
      "77300\n",
      "77350\n",
      "77400\n",
      "77450\n",
      "77500\n",
      "77550\n",
      "77600\n",
      "77650\n",
      "77700\n",
      "77750\n",
      "77800\n",
      "77850\n",
      "77900\n",
      "77950\n",
      "Epoch: 3/20...  Training Step: 9500...  Training loss: 1.6735...  0.0751 sec/batch\n",
      "78000\n",
      "78050\n",
      "78100\n",
      "78150\n",
      "78200\n",
      "78250\n",
      "78300\n",
      "78350\n",
      "78400\n",
      "78450\n",
      "78500\n",
      "78550\n",
      "78600\n",
      "78650\n",
      "78700\n",
      "78750\n",
      "78800\n",
      "78850\n",
      "78900\n",
      "78950\n",
      "79000\n",
      "79050\n",
      "79100\n",
      "79150\n",
      "79200\n",
      "79250\n",
      "79300\n",
      "79350\n",
      "79400\n",
      "79450\n",
      "79500\n",
      "79550\n",
      "79600\n",
      "79650\n",
      "79700\n",
      "79750\n",
      "79800\n",
      "79850\n",
      "79900\n",
      "79950\n",
      "80000\n",
      "80050\n",
      "80100\n",
      "80150\n",
      "80200\n",
      "80250\n",
      "80300\n",
      "80350\n",
      "80400\n",
      "80450\n",
      "Epoch: 3/20...  Training Step: 9550...  Training loss: 1.7027...  0.0765 sec/batch\n",
      "80500\n",
      "80550\n",
      "80600\n",
      "80650\n",
      "80700\n",
      "80750\n",
      "80800\n",
      "80850\n",
      "80900\n",
      "80950\n",
      "81000\n",
      "81050\n",
      "81100\n",
      "81150\n",
      "81200\n",
      "81250\n",
      "81300\n",
      "81350\n",
      "81400\n",
      "81450\n",
      "81500\n",
      "81550\n",
      "81600\n",
      "81650\n",
      "81700\n",
      "81750\n",
      "81800\n",
      "81850\n",
      "81900\n",
      "81950\n",
      "82000\n",
      "82050\n",
      "82100\n",
      "82150\n",
      "82200\n",
      "82250\n",
      "82300\n",
      "82350\n",
      "82400\n",
      "82450\n",
      "82500\n",
      "82550\n",
      "82600\n",
      "82650\n",
      "82700\n",
      "82750\n",
      "82800\n",
      "82850\n",
      "82900\n",
      "82950\n",
      "Epoch: 3/20...  Training Step: 9600...  Training loss: 1.5957...  0.0878 sec/batch\n",
      "83000\n",
      "83050\n",
      "83100\n",
      "83150\n",
      "83200\n",
      "83250\n",
      "83300\n",
      "83350\n",
      "83400\n",
      "83450\n",
      "83500\n",
      "83550\n",
      "83600\n",
      "83650\n",
      "83700\n",
      "83750\n",
      "83800\n",
      "83850\n",
      "83900\n",
      "83950\n",
      "84000\n",
      "84050\n",
      "84100\n",
      "84150\n",
      "84200\n",
      "84250\n",
      "84300\n",
      "84350\n",
      "84400\n",
      "84450\n",
      "84500\n",
      "84550\n",
      "84600\n",
      "84650\n",
      "84700\n",
      "84750\n",
      "84800\n",
      "84850\n",
      "84900\n",
      "84950\n",
      "85000\n",
      "85050\n",
      "85100\n",
      "85150\n",
      "85200\n",
      "85250\n",
      "85300\n",
      "85350\n",
      "85400\n",
      "85450\n",
      "Epoch: 3/20...  Training Step: 9650...  Training loss: 1.6230...  0.0751 sec/batch\n",
      "85500\n",
      "85550\n",
      "85600\n",
      "85650\n",
      "85700\n",
      "85750\n",
      "85800\n",
      "85850\n",
      "85900\n",
      "85950\n",
      "86000\n",
      "86050\n",
      "86100\n",
      "86150\n",
      "86200\n",
      "86250\n",
      "86300\n",
      "86350\n",
      "86400\n",
      "86450\n",
      "86500\n",
      "86550\n",
      "86600\n",
      "86650\n",
      "86700\n",
      "86750\n",
      "86800\n",
      "86850\n",
      "86900\n",
      "86950\n",
      "87000\n",
      "87050\n",
      "87100\n",
      "87150\n",
      "87200\n",
      "87250\n",
      "87300\n",
      "87350\n",
      "87400\n",
      "87450\n",
      "87500\n",
      "87550\n",
      "87600\n",
      "87650\n",
      "87700\n",
      "87750\n",
      "87800\n",
      "87850\n",
      "87900\n",
      "87950\n",
      "Epoch: 3/20...  Training Step: 9700...  Training loss: 1.5594...  0.0867 sec/batch\n",
      "88000\n",
      "88050\n",
      "88100\n",
      "88150\n",
      "88200\n",
      "88250\n",
      "88300\n",
      "88350\n",
      "88400\n",
      "88450\n",
      "88500\n",
      "88550\n",
      "88600\n",
      "88650\n",
      "88700\n",
      "88750\n",
      "88800\n",
      "88850\n",
      "88900\n",
      "88950\n",
      "89000\n",
      "89050\n",
      "89100\n",
      "89150\n",
      "89200\n",
      "89250\n",
      "89300\n",
      "89350\n",
      "89400\n",
      "89450\n",
      "89500\n",
      "89550\n",
      "89600\n",
      "89650\n",
      "89700\n",
      "89750\n",
      "89800\n",
      "89850\n",
      "89900\n",
      "89950\n",
      "90000\n",
      "90050\n",
      "90100\n",
      "90150\n",
      "90200\n",
      "90250\n",
      "90300\n",
      "90350\n",
      "90400\n",
      "90450\n",
      "Epoch: 3/20...  Training Step: 9750...  Training loss: 1.4388...  0.0976 sec/batch\n",
      "90500\n",
      "90550\n",
      "90600\n",
      "90650\n",
      "90700\n",
      "90750\n",
      "90800\n",
      "90850\n",
      "90900\n",
      "90950\n",
      "91000\n",
      "91050\n",
      "91100\n",
      "91150\n",
      "91200\n",
      "91250\n",
      "91300\n",
      "91350\n",
      "91400\n",
      "91450\n",
      "91500\n",
      "91550\n",
      "91600\n",
      "91650\n",
      "91700\n",
      "91750\n",
      "91800\n",
      "91850\n",
      "91900\n",
      "91950\n",
      "92000\n",
      "92050\n",
      "92100\n",
      "92150\n",
      "92200\n",
      "92250\n",
      "92300\n",
      "92350\n",
      "92400\n",
      "92450\n",
      "92500\n",
      "92550\n",
      "92600\n",
      "92650\n",
      "92700\n",
      "92750\n",
      "92800\n",
      "92850\n",
      "92900\n",
      "92950\n",
      "Epoch: 3/20...  Training Step: 9800...  Training loss: 1.6652...  0.0864 sec/batch\n",
      "93000\n",
      "93050\n",
      "93100\n",
      "93150\n",
      "93200\n",
      "93250\n",
      "93300\n",
      "93350\n",
      "93400\n",
      "93450\n",
      "93500\n",
      "93550\n",
      "93600\n",
      "93650\n",
      "93700\n",
      "93750\n",
      "93800\n",
      "93850\n",
      "93900\n",
      "93950\n",
      "94000\n",
      "94050\n",
      "94100\n",
      "94150\n",
      "94200\n",
      "94250\n",
      "94300\n",
      "94350\n",
      "94400\n",
      "94450\n",
      "94500\n",
      "94550\n",
      "94600\n",
      "94650\n",
      "94700\n",
      "94750\n",
      "94800\n",
      "94850\n",
      "94900\n",
      "94950\n",
      "95000\n",
      "95050\n",
      "95100\n",
      "95150\n",
      "95200\n",
      "95250\n",
      "95300\n",
      "95350\n",
      "95400\n",
      "95450\n",
      "Epoch: 3/20...  Training Step: 9850...  Training loss: 1.5608...  0.0875 sec/batch\n",
      "95500\n",
      "95550\n",
      "95600\n",
      "95650\n",
      "95700\n",
      "95750\n",
      "95800\n",
      "95850\n",
      "95900\n",
      "95950\n",
      "96000\n",
      "96050\n",
      "96100\n",
      "96150\n",
      "96200\n",
      "96250\n",
      "96300\n",
      "96350\n",
      "96400\n",
      "96450\n",
      "96500\n",
      "96550\n",
      "96600\n",
      "96650\n",
      "96700\n",
      "96750\n",
      "96800\n",
      "96850\n",
      "96900\n",
      "96950\n",
      "97000\n",
      "97050\n",
      "97100\n",
      "97150\n",
      "97200\n",
      "97250\n",
      "97300\n",
      "97350\n",
      "97400\n",
      "97450\n",
      "97500\n",
      "97550\n",
      "97600\n",
      "97650\n",
      "97700\n",
      "97750\n",
      "97800\n",
      "97850\n",
      "97900\n",
      "97950\n",
      "Epoch: 3/20...  Training Step: 9900...  Training loss: 1.6530...  0.0796 sec/batch\n",
      "98000\n",
      "98050\n",
      "98100\n",
      "98150\n",
      "98200\n",
      "98250\n",
      "98300\n",
      "98350\n",
      "98400\n",
      "98450\n",
      "98500\n",
      "98550\n",
      "98600\n",
      "98650\n",
      "98700\n",
      "98750\n",
      "98800\n",
      "98850\n",
      "98900\n",
      "98950\n",
      "99000\n",
      "99050\n",
      "99100\n",
      "99150\n",
      "99200\n",
      "99250\n",
      "99300\n",
      "99350\n",
      "99400\n",
      "99450\n",
      "99500\n",
      "99550\n",
      "99600\n",
      "99650\n",
      "99700\n",
      "99750\n",
      "99800\n",
      "99850\n",
      "99900\n",
      "99950\n",
      "100000\n",
      "100050\n",
      "100100\n",
      "100150\n",
      "100200\n",
      "100250\n",
      "100300\n",
      "100350\n",
      "100400\n",
      "100450\n",
      "Epoch: 3/20...  Training Step: 9950...  Training loss: 1.6983...  0.0758 sec/batch\n",
      "100500\n",
      "100550\n",
      "100600\n",
      "100650\n",
      "100700\n",
      "100750\n",
      "100800\n",
      "100850\n",
      "100900\n",
      "100950\n",
      "101000\n",
      "101050\n",
      "101100\n",
      "101150\n",
      "101200\n",
      "101250\n",
      "101300\n",
      "101350\n",
      "101400\n",
      "101450\n",
      "101500\n",
      "101550\n",
      "101600\n",
      "101650\n",
      "101700\n",
      "101750\n",
      "101800\n",
      "101850\n",
      "101900\n",
      "101950\n",
      "102000\n",
      "102050\n",
      "102100\n",
      "102150\n",
      "102200\n",
      "102250\n",
      "102300\n",
      "102350\n",
      "102400\n",
      "102450\n",
      "102500\n",
      "102550\n",
      "102600\n",
      "102650\n",
      "102700\n",
      "102750\n",
      "102800\n",
      "102850\n",
      "102900\n",
      "102950\n",
      "Epoch: 3/20...  Training Step: 10000...  Training loss: 1.5623...  0.0870 sec/batch\n",
      "103000\n",
      "103050\n",
      "103100\n",
      "103150\n",
      "103200\n",
      "103250\n",
      "103300\n",
      "103350\n",
      "103400\n",
      "103450\n",
      "103500\n",
      "103550\n",
      "103600\n",
      "103650\n",
      "103700\n",
      "103750\n",
      "103800\n",
      "103850\n",
      "103900\n",
      "103950\n",
      "104000\n",
      "104050\n",
      "104100\n",
      "104150\n",
      "104200\n",
      "104250\n",
      "104300\n",
      "104350\n",
      "104400\n",
      "104450\n",
      "104500\n",
      "104550\n",
      "104600\n",
      "104650\n",
      "104700\n",
      "104750\n",
      "104800\n",
      "104850\n",
      "104900\n",
      "104950\n",
      "105000\n",
      "105050\n",
      "105100\n",
      "105150\n",
      "105200\n",
      "105250\n",
      "105300\n",
      "105350\n",
      "105400\n",
      "105450\n",
      "Epoch: 3/20...  Training Step: 10050...  Training loss: 1.6133...  0.0853 sec/batch\n",
      "105500\n",
      "105550\n",
      "105600\n",
      "105650\n",
      "105700\n",
      "105750\n",
      "105800\n",
      "105850\n",
      "105900\n",
      "105950\n",
      "106000\n",
      "106050\n",
      "106100\n",
      "106150\n",
      "106200\n",
      "106250\n",
      "106300\n",
      "106350\n",
      "106400\n",
      "106450\n",
      "106500\n",
      "106550\n",
      "106600\n",
      "106650\n",
      "106700\n",
      "106750\n",
      "106800\n",
      "106850\n",
      "106900\n",
      "106950\n",
      "107000\n",
      "107050\n",
      "107100\n",
      "107150\n",
      "107200\n",
      "107250\n",
      "107300\n",
      "107350\n",
      "107400\n",
      "107450\n",
      "107500\n",
      "107550\n",
      "107600\n",
      "107650\n",
      "107700\n",
      "107750\n",
      "107800\n",
      "107850\n",
      "107900\n",
      "107950\n",
      "Epoch: 3/20...  Training Step: 10100...  Training loss: 1.6188...  0.0772 sec/batch\n",
      "108000\n",
      "108050\n",
      "108100\n",
      "108150\n",
      "108200\n",
      "108250\n",
      "108300\n",
      "108350\n",
      "108400\n",
      "108450\n",
      "108500\n",
      "108550\n",
      "108600\n",
      "108650\n",
      "108700\n",
      "108750\n",
      "108800\n",
      "108850\n",
      "108900\n",
      "108950\n",
      "109000\n",
      "109050\n",
      "109100\n",
      "109150\n",
      "109200\n",
      "109250\n",
      "109300\n",
      "109350\n",
      "109400\n",
      "109450\n",
      "109500\n",
      "109550\n",
      "109600\n",
      "109650\n",
      "109700\n",
      "109750\n",
      "109800\n",
      "109850\n",
      "109900\n",
      "109950\n",
      "110000\n",
      "110050\n",
      "110100\n",
      "110150\n",
      "110200\n",
      "110250\n",
      "110300\n",
      "110350\n",
      "110400\n",
      "110450\n",
      "Epoch: 3/20...  Training Step: 10150...  Training loss: 1.7209...  0.0766 sec/batch\n",
      "110500\n",
      "110550\n",
      "110600\n",
      "110650\n",
      "110700\n",
      "110750\n",
      "110800\n",
      "110850\n",
      "110900\n",
      "110950\n",
      "111000\n",
      "111050\n",
      "111100\n",
      "111150\n",
      "111200\n",
      "111250\n",
      "111300\n",
      "111350\n",
      "111400\n",
      "111450\n",
      "111500\n",
      "111550\n",
      "111600\n",
      "111650\n",
      "111700\n",
      "111750\n",
      "111800\n",
      "111850\n",
      "111900\n",
      "111950\n",
      "112000\n",
      "112050\n",
      "112100\n",
      "112150\n",
      "112200\n",
      "112250\n",
      "112300\n",
      "112350\n",
      "112400\n",
      "112450\n",
      "112500\n",
      "112550\n",
      "112600\n",
      "112650\n",
      "112700\n",
      "112750\n",
      "112800\n",
      "112850\n",
      "112900\n",
      "112950\n",
      "Epoch: 3/20...  Training Step: 10200...  Training loss: 1.7609...  0.0888 sec/batch\n",
      "113000\n",
      "113050\n",
      "113100\n",
      "113150\n",
      "113200\n",
      "113250\n",
      "113300\n",
      "113350\n",
      "113400\n",
      "113450\n",
      "113500\n",
      "113550\n",
      "113600\n",
      "113650\n",
      "113700\n",
      "113750\n",
      "113800\n",
      "113850\n",
      "113900\n",
      "113950\n",
      "114000\n",
      "114050\n",
      "114100\n",
      "114150\n",
      "114200\n",
      "114250\n",
      "114300\n",
      "114350\n",
      "114400\n",
      "114450\n",
      "114500\n",
      "114550\n",
      "114600\n",
      "114650\n",
      "114700\n",
      "114750\n",
      "114800\n",
      "114850\n",
      "114900\n",
      "114950\n",
      "115000\n",
      "115050\n",
      "115100\n",
      "115150\n",
      "115200\n",
      "115250\n",
      "115300\n",
      "115350\n",
      "115400\n",
      "115450\n",
      "Epoch: 3/20...  Training Step: 10250...  Training loss: 1.5717...  0.0905 sec/batch\n",
      "115500\n",
      "115550\n",
      "115600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115650\n",
      "115700\n",
      "115750\n",
      "115800\n",
      "115850\n",
      "115900\n",
      "115950\n",
      "116000\n",
      "116050\n",
      "116100\n",
      "116150\n",
      "116200\n",
      "116250\n",
      "116300\n",
      "116350\n",
      "116400\n",
      "116450\n",
      "116500\n",
      "116550\n",
      "116600\n",
      "116650\n",
      "116700\n",
      "116750\n",
      "116800\n",
      "116850\n",
      "116900\n",
      "116950\n",
      "117000\n",
      "117050\n",
      "117100\n",
      "117150\n",
      "117200\n",
      "117250\n",
      "117300\n",
      "117350\n",
      "117400\n",
      "117450\n",
      "117500\n",
      "117550\n",
      "117600\n",
      "117650\n",
      "117700\n",
      "117750\n",
      "117800\n",
      "117850\n",
      "117900\n",
      "117950\n",
      "Epoch: 3/20...  Training Step: 10300...  Training loss: 1.8128...  0.0886 sec/batch\n",
      "118000\n",
      "118050\n",
      "118100\n",
      "118150\n",
      "118200\n",
      "118250\n",
      "118300\n",
      "118350\n",
      "118400\n",
      "118450\n",
      "118500\n",
      "118550\n",
      "118600\n",
      "118650\n",
      "118700\n",
      "118750\n",
      "118800\n",
      "118850\n",
      "118900\n",
      "118950\n",
      "119000\n",
      "119050\n",
      "119100\n",
      "119150\n",
      "119200\n",
      "119250\n",
      "119300\n",
      "119350\n",
      "119400\n",
      "119450\n",
      "119500\n",
      "119550\n",
      "119600\n",
      "119650\n",
      "119700\n",
      "119750\n",
      "119800\n",
      "119850\n",
      "119900\n",
      "119950\n",
      "120000\n",
      "120050\n",
      "120100\n",
      "120150\n",
      "120200\n",
      "120250\n",
      "120300\n",
      "120350\n",
      "120400\n",
      "120450\n",
      "Epoch: 3/20...  Training Step: 10350...  Training loss: 1.5910...  0.0765 sec/batch\n",
      "120500\n",
      "120550\n",
      "120600\n",
      "120650\n",
      "120700\n",
      "120750\n",
      "120800\n",
      "120850\n",
      "120900\n",
      "120950\n",
      "121000\n",
      "121050\n",
      "121100\n",
      "121150\n",
      "121200\n",
      "121250\n",
      "121300\n",
      "121350\n",
      "121400\n",
      "121450\n",
      "121500\n",
      "121550\n",
      "121600\n",
      "121650\n",
      "121700\n",
      "121750\n",
      "121800\n",
      "121850\n",
      "121900\n",
      "121950\n",
      "122000\n",
      "122050\n",
      "122100\n",
      "122150\n",
      "122200\n",
      "122250\n",
      "122300\n",
      "122350\n",
      "122400\n",
      "122450\n",
      "122500\n",
      "122550\n",
      "122600\n",
      "122650\n",
      "122700\n",
      "122750\n",
      "122800\n",
      "122850\n",
      "122900\n",
      "122950\n",
      "Epoch: 3/20...  Training Step: 10400...  Training loss: 1.6968...  0.0814 sec/batch\n",
      "123000\n",
      "123050\n",
      "123100\n",
      "123150\n",
      "123200\n",
      "123250\n",
      "123300\n",
      "123350\n",
      "123400\n",
      "123450\n",
      "123500\n",
      "123550\n",
      "123600\n",
      "123650\n",
      "123700\n",
      "123750\n",
      "123800\n",
      "123850\n",
      "123900\n",
      "123950\n",
      "124000\n",
      "124050\n",
      "124100\n",
      "124150\n",
      "124200\n",
      "124250\n",
      "124300\n",
      "124350\n",
      "124400\n",
      "124450\n",
      "124500\n",
      "124550\n",
      "124600\n",
      "124650\n",
      "124700\n",
      "124750\n",
      "124800\n",
      "124850\n",
      "124900\n",
      "124950\n",
      "125000\n",
      "125050\n",
      "125100\n",
      "125150\n",
      "125200\n",
      "125250\n",
      "125300\n",
      "125350\n",
      "125400\n",
      "125450\n",
      "Epoch: 3/20...  Training Step: 10450...  Training loss: 1.6972...  0.0817 sec/batch\n",
      "125500\n",
      "125550\n",
      "125600\n",
      "125650\n",
      "125700\n",
      "125750\n",
      "125800\n",
      "125850\n",
      "125900\n",
      "125950\n",
      "126000\n",
      "126050\n",
      "126100\n",
      "126150\n",
      "126200\n",
      "126250\n",
      "126300\n",
      "126350\n",
      "126400\n",
      "126450\n",
      "126500\n",
      "126550\n",
      "126600\n",
      "126650\n",
      "126700\n",
      "126750\n",
      "126800\n",
      "126850\n",
      "126900\n",
      "126950\n",
      "127000\n",
      "127050\n",
      "127100\n",
      "127150\n",
      "127200\n",
      "127250\n",
      "127300\n",
      "127350\n",
      "127400\n",
      "127450\n",
      "127500\n",
      "127550\n",
      "127600\n",
      "127650\n",
      "127700\n",
      "127750\n",
      "127800\n",
      "127850\n",
      "127900\n",
      "127950\n",
      "Epoch: 3/20...  Training Step: 10500...  Training loss: 1.4483...  0.0792 sec/batch\n",
      "128000\n",
      "128050\n",
      "128100\n",
      "128150\n",
      "128200\n",
      "128250\n",
      "128300\n",
      "128350\n",
      "128400\n",
      "128450\n",
      "128500\n",
      "128550\n",
      "128600\n",
      "128650\n",
      "128700\n",
      "128750\n",
      "128800\n",
      "128850\n",
      "128900\n",
      "128950\n",
      "129000\n",
      "129050\n",
      "129100\n",
      "129150\n",
      "129200\n",
      "129250\n",
      "129300\n",
      "129350\n",
      "129400\n",
      "129450\n",
      "129500\n",
      "129550\n",
      "129600\n",
      "129650\n",
      "129700\n",
      "129750\n",
      "129800\n",
      "129850\n",
      "129900\n",
      "129950\n",
      "130000\n",
      "130050\n",
      "130100\n",
      "130150\n",
      "130200\n",
      "130250\n",
      "130300\n",
      "130350\n",
      "130400\n",
      "130450\n",
      "Epoch: 3/20...  Training Step: 10550...  Training loss: 1.6790...  0.0873 sec/batch\n",
      "130500\n",
      "130550\n",
      "130600\n",
      "130650\n",
      "130700\n",
      "130750\n",
      "130800\n",
      "130850\n",
      "130900\n",
      "130950\n",
      "131000\n",
      "131050\n",
      "131100\n",
      "131150\n",
      "131200\n",
      "131250\n",
      "131300\n",
      "131350\n",
      "131400\n",
      "131450\n",
      "131500\n",
      "131550\n",
      "131600\n",
      "131650\n",
      "131700\n",
      "131750\n",
      "131800\n",
      "131850\n",
      "131900\n",
      "131950\n",
      "132000\n",
      "132050\n",
      "132100\n",
      "132150\n",
      "132200\n",
      "132250\n",
      "132300\n",
      "132350\n",
      "132400\n",
      "132450\n",
      "132500\n",
      "132550\n",
      "132600\n",
      "132650\n",
      "132700\n",
      "132750\n",
      "132800\n",
      "132850\n",
      "132900\n",
      "132950\n",
      "Epoch: 3/20...  Training Step: 10600...  Training loss: 1.7378...  0.0958 sec/batch\n",
      "133000\n",
      "133050\n",
      "133100\n",
      "133150\n",
      "133200\n",
      "133250\n",
      "133300\n",
      "133350\n",
      "133400\n",
      "133450\n",
      "133500\n",
      "133550\n",
      "133600\n",
      "133650\n",
      "133700\n",
      "133750\n",
      "133800\n",
      "133850\n",
      "133900\n",
      "133950\n",
      "134000\n",
      "134050\n",
      "134100\n",
      "134150\n",
      "134200\n",
      "134250\n",
      "134300\n",
      "134350\n",
      "134400\n",
      "134450\n",
      "134500\n",
      "134550\n",
      "134600\n",
      "134650\n",
      "134700\n",
      "134750\n",
      "134800\n",
      "134850\n",
      "134900\n",
      "134950\n",
      "135000\n",
      "135050\n",
      "135100\n",
      "135150\n",
      "135200\n",
      "135250\n",
      "135300\n",
      "135350\n",
      "135400\n",
      "135450\n",
      "Epoch: 3/20...  Training Step: 10650...  Training loss: 1.5586...  0.0871 sec/batch\n",
      "135500\n",
      "135550\n",
      "135600\n",
      "135650\n",
      "135700\n",
      "135750\n",
      "135800\n",
      "135850\n",
      "135900\n",
      "135950\n",
      "136000\n",
      "136050\n",
      "136100\n",
      "136150\n",
      "136200\n",
      "136250\n",
      "136300\n",
      "136350\n",
      "136400\n",
      "136450\n",
      "136500\n",
      "136550\n",
      "136600\n",
      "136650\n",
      "136700\n",
      "136750\n",
      "136800\n",
      "136850\n",
      "136900\n",
      "136950\n",
      "137000\n",
      "137050\n",
      "137100\n",
      "137150\n",
      "137200\n",
      "137250\n",
      "137300\n",
      "137350\n",
      "137400\n",
      "137450\n",
      "137500\n",
      "137550\n",
      "137600\n",
      "137650\n",
      "137700\n",
      "137750\n",
      "137800\n",
      "137850\n",
      "137900\n",
      "137950\n",
      "Epoch: 3/20...  Training Step: 10700...  Training loss: 1.6128...  0.0863 sec/batch\n",
      "138000\n",
      "138050\n",
      "138100\n",
      "138150\n",
      "138200\n",
      "138250\n",
      "138300\n",
      "138350\n",
      "138400\n",
      "138450\n",
      "138500\n",
      "138550\n",
      "138600\n",
      "138650\n",
      "138700\n",
      "138750\n",
      "138800\n",
      "138850\n",
      "138900\n",
      "138950\n",
      "139000\n",
      "139050\n",
      "139100\n",
      "139150\n",
      "139200\n",
      "139250\n",
      "139300\n",
      "139350\n",
      "139400\n",
      "139450\n",
      "139500\n",
      "139550\n",
      "139600\n",
      "139650\n",
      "139700\n",
      "139750\n",
      "139800\n",
      "139850\n",
      "139900\n",
      "139950\n",
      "140000\n",
      "140050\n",
      "140100\n",
      "140150\n",
      "140200\n",
      "140250\n",
      "140300\n",
      "140350\n",
      "140400\n",
      "140450\n",
      "Epoch: 3/20...  Training Step: 10750...  Training loss: 1.6497...  0.1166 sec/batch\n",
      "140500\n",
      "140550\n",
      "140600\n",
      "140650\n",
      "140700\n",
      "140750\n",
      "140800\n",
      "140850\n",
      "140900\n",
      "140950\n",
      "141000\n",
      "141050\n",
      "141100\n",
      "141150\n",
      "141200\n",
      "141250\n",
      "141300\n",
      "141350\n",
      "141400\n",
      "141450\n",
      "141500\n",
      "141550\n",
      "141600\n",
      "141650\n",
      "141700\n",
      "141750\n",
      "141800\n",
      "141850\n",
      "141900\n",
      "141950\n",
      "142000\n",
      "142050\n",
      "142100\n",
      "142150\n",
      "142200\n",
      "142250\n",
      "142300\n",
      "142350\n",
      "142400\n",
      "142450\n",
      "142500\n",
      "142550\n",
      "142600\n",
      "142650\n",
      "142700\n",
      "142750\n",
      "142800\n",
      "142850\n",
      "142900\n",
      "142950\n",
      "Epoch: 3/20...  Training Step: 10800...  Training loss: 1.5658...  0.0806 sec/batch\n",
      "143000\n",
      "143050\n",
      "143100\n",
      "143150\n",
      "143200\n",
      "143250\n",
      "143300\n",
      "143350\n",
      "143400\n",
      "143450\n",
      "143500\n",
      "143550\n",
      "143600\n",
      "143650\n",
      "143700\n",
      "143750\n",
      "143800\n",
      "143850\n",
      "143900\n",
      "143950\n",
      "144000\n",
      "144050\n",
      "144100\n",
      "144150\n",
      "144200\n",
      "144250\n",
      "144300\n",
      "144350\n",
      "144400\n",
      "144450\n",
      "144500\n",
      "144550\n",
      "144600\n",
      "144650\n",
      "144700\n",
      "144750\n",
      "144800\n",
      "144850\n",
      "144900\n",
      "144950\n",
      "145000\n",
      "145050\n",
      "145100\n",
      "145150\n",
      "145200\n",
      "145250\n",
      "145300\n",
      "145350\n",
      "145400\n",
      "145450\n",
      "Epoch: 3/20...  Training Step: 10850...  Training loss: 1.5773...  0.0838 sec/batch\n",
      "145500\n",
      "145550\n",
      "145600\n",
      "145650\n",
      "145700\n",
      "145750\n",
      "145800\n",
      "145850\n",
      "145900\n",
      "145950\n",
      "146000\n",
      "146050\n",
      "146100\n",
      "146150\n",
      "146200\n",
      "146250\n",
      "146300\n",
      "146350\n",
      "146400\n",
      "146450\n",
      "146500\n",
      "146550\n",
      "146600\n",
      "146650\n",
      "146700\n",
      "146750\n",
      "146800\n",
      "146850\n",
      "146900\n",
      "146950\n",
      "147000\n",
      "147050\n",
      "147100\n",
      "147150\n",
      "147200\n",
      "147250\n",
      "147300\n",
      "147350\n",
      "147400\n",
      "147450\n",
      "147500\n",
      "147550\n",
      "147600\n",
      "147650\n",
      "147700\n",
      "147750\n",
      "147800\n",
      "147850\n",
      "147900\n",
      "147950\n",
      "Epoch: 3/20...  Training Step: 10900...  Training loss: 1.6873...  0.0839 sec/batch\n",
      "148000\n",
      "148050\n",
      "148100\n",
      "148150\n",
      "148200\n",
      "148250\n",
      "148300\n",
      "148350\n",
      "148400\n",
      "148450\n",
      "148500\n",
      "148550\n",
      "148600\n",
      "148650\n",
      "148700\n",
      "148750\n",
      "148800\n",
      "148850\n",
      "148900\n",
      "148950\n",
      "149000\n",
      "149050\n",
      "149100\n",
      "149150\n",
      "149200\n",
      "149250\n",
      "149300\n",
      "149350\n",
      "149400\n",
      "149450\n",
      "149500\n",
      "149550\n",
      "149600\n",
      "149650\n",
      "149700\n",
      "149750\n",
      "149800\n",
      "149850\n",
      "149900\n",
      "149950\n",
      "150000\n",
      "150050\n",
      "150100\n",
      "150150\n",
      "150200\n",
      "150250\n",
      "150300\n",
      "150350\n",
      "150400\n",
      "150450\n",
      "Epoch: 3/20...  Training Step: 10950...  Training loss: 1.7510...  0.0879 sec/batch\n",
      "150500\n",
      "150550\n",
      "150600\n",
      "150650\n",
      "150700\n",
      "150750\n",
      "150800\n",
      "150850\n",
      "150900\n",
      "150950\n",
      "151000\n",
      "151050\n",
      "151100\n",
      "151150\n",
      "151200\n",
      "151250\n",
      "151300\n",
      "151350\n",
      "151400\n",
      "151450\n",
      "151500\n",
      "151550\n",
      "151600\n",
      "151650\n",
      "151700\n",
      "151750\n",
      "151800\n",
      "151850\n",
      "151900\n",
      "151950\n",
      "152000\n",
      "152050\n",
      "152100\n",
      "152150\n",
      "152200\n",
      "152250\n",
      "152300\n",
      "152350\n",
      "152400\n",
      "152450\n",
      "152500\n",
      "152550\n",
      "152600\n",
      "152650\n",
      "152700\n",
      "152750\n",
      "152800\n",
      "152850\n",
      "152900\n",
      "152950\n",
      "Epoch: 3/20...  Training Step: 11000...  Training loss: 1.6486...  0.0915 sec/batch\n",
      "153000\n",
      "153050\n",
      "153100\n",
      "153150\n",
      "153200\n",
      "153250\n",
      "153300\n",
      "153350\n",
      "153400\n",
      "153450\n",
      "153500\n",
      "153550\n",
      "153600\n",
      "153650\n",
      "153700\n",
      "153750\n",
      "153800\n",
      "153850\n",
      "153900\n",
      "153950\n",
      "154000\n",
      "154050\n",
      "154100\n",
      "154150\n",
      "154200\n",
      "154250\n",
      "154300\n",
      "154350\n",
      "154400\n",
      "154450\n",
      "154500\n",
      "154550\n",
      "154600\n",
      "154650\n",
      "154700\n",
      "154750\n",
      "154800\n",
      "154850\n",
      "154900\n",
      "154950\n",
      "155000\n",
      "155050\n",
      "155100\n",
      "155150\n",
      "155200\n",
      "155250\n",
      "155300\n",
      "155350\n",
      "155400\n",
      "155450\n",
      "Epoch: 3/20...  Training Step: 11050...  Training loss: 1.5805...  0.0828 sec/batch\n",
      "155500\n",
      "155550\n",
      "155600\n",
      "155650\n",
      "155700\n",
      "155750\n",
      "155800\n",
      "155850\n",
      "155900\n",
      "155950\n",
      "156000\n",
      "156050\n",
      "156100\n",
      "156150\n",
      "156200\n",
      "156250\n",
      "156300\n",
      "156350\n",
      "156400\n",
      "156450\n",
      "156500\n",
      "156550\n",
      "156600\n",
      "156650\n",
      "156700\n",
      "156750\n",
      "156800\n",
      "156850\n",
      "156900\n",
      "156950\n",
      "157000\n",
      "157050\n",
      "157100\n",
      "157150\n",
      "157200\n",
      "157250\n",
      "157300\n",
      "157350\n",
      "157400\n",
      "157450\n",
      "157500\n",
      "157550\n",
      "157600\n",
      "157650\n",
      "157700\n",
      "157750\n",
      "157800\n",
      "157850\n",
      "157900\n",
      "157950\n",
      "Epoch: 3/20...  Training Step: 11100...  Training loss: 1.5815...  0.0787 sec/batch\n",
      "158000\n",
      "158050\n",
      "158100\n",
      "158150\n",
      "158200\n",
      "158250\n",
      "158300\n",
      "158350\n",
      "158400\n",
      "158450\n",
      "158500\n",
      "158550\n",
      "158600\n",
      "158650\n",
      "158700\n",
      "158750\n",
      "158800\n",
      "158850\n",
      "158900\n",
      "158950\n",
      "159000\n",
      "159050\n",
      "159100\n",
      "159150\n",
      "159200\n",
      "159250\n",
      "159300\n",
      "159350\n",
      "159400\n",
      "159450\n",
      "159500\n",
      "159550\n",
      "159600\n",
      "159650\n",
      "159700\n",
      "159750\n",
      "159800\n",
      "159850\n",
      "159900\n",
      "159950\n",
      "160000\n",
      "160050\n",
      "160100\n",
      "160150\n",
      "160200\n",
      "160250\n",
      "160300\n",
      "160350\n",
      "160400\n",
      "160450\n",
      "Epoch: 3/20...  Training Step: 11150...  Training loss: 1.5529...  0.1030 sec/batch\n",
      "160500\n",
      "160550\n",
      "160600\n",
      "160650\n",
      "160700\n",
      "160750\n",
      "160800\n",
      "160850\n",
      "160900\n",
      "160950\n",
      "161000\n",
      "161050\n",
      "161100\n",
      "161150\n",
      "161200\n",
      "161250\n",
      "161300\n",
      "161350\n",
      "161400\n",
      "161450\n",
      "161500\n",
      "161550\n",
      "161600\n",
      "161650\n",
      "161700\n",
      "161750\n",
      "161800\n",
      "161850\n",
      "161900\n",
      "161950\n",
      "162000\n",
      "162050\n",
      "162100\n",
      "162150\n",
      "162200\n",
      "162250\n",
      "162300\n",
      "162350\n",
      "162400\n",
      "162450\n",
      "162500\n",
      "162550\n",
      "162600\n",
      "162650\n",
      "162700\n",
      "162750\n",
      "162800\n",
      "162850\n",
      "162900\n",
      "162950\n",
      "Epoch: 3/20...  Training Step: 11200...  Training loss: 1.5673...  0.0844 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163000\n",
      "163050\n",
      "163100\n",
      "163150\n",
      "163200\n",
      "163250\n",
      "163300\n",
      "163350\n",
      "163400\n",
      "163450\n",
      "163500\n",
      "163550\n",
      "163600\n",
      "163650\n",
      "163700\n",
      "163750\n",
      "163800\n",
      "163850\n",
      "163900\n",
      "163950\n",
      "164000\n",
      "164050\n",
      "164100\n",
      "164150\n",
      "164200\n",
      "164250\n",
      "164300\n",
      "164350\n",
      "164400\n",
      "164450\n",
      "164500\n",
      "164550\n",
      "164600\n",
      "164650\n",
      "164700\n",
      "164750\n",
      "164800\n",
      "164850\n",
      "164900\n",
      "164950\n",
      "165000\n",
      "165050\n",
      "165100\n",
      "165150\n",
      "165200\n",
      "165250\n",
      "165300\n",
      "165350\n",
      "165400\n",
      "165450\n",
      "Epoch: 3/20...  Training Step: 11250...  Training loss: 1.6701...  0.0919 sec/batch\n",
      "165500\n",
      "165550\n",
      "165600\n",
      "165650\n",
      "165700\n",
      "165750\n",
      "165800\n",
      "165850\n",
      "165900\n",
      "165950\n",
      "166000\n",
      "166050\n",
      "166100\n",
      "166150\n",
      "166200\n",
      "166250\n",
      "166300\n",
      "166350\n",
      "166400\n",
      "166450\n",
      "166500\n",
      "166550\n",
      "166600\n",
      "166650\n",
      "166700\n",
      "166750\n",
      "166800\n",
      "166850\n",
      "166900\n",
      "166950\n",
      "167000\n",
      "167050\n",
      "167100\n",
      "167150\n",
      "167200\n",
      "167250\n",
      "167300\n",
      "167350\n",
      "167400\n",
      "167450\n",
      "167500\n",
      "167550\n",
      "167600\n",
      "167650\n",
      "167700\n",
      "167750\n",
      "167800\n",
      "167850\n",
      "167900\n",
      "167950\n",
      "Epoch: 3/20...  Training Step: 11300...  Training loss: 1.5407...  0.0782 sec/batch\n",
      "168000\n",
      "168050\n",
      "168100\n",
      "168150\n",
      "168200\n",
      "168250\n",
      "168300\n",
      "168350\n",
      "168400\n",
      "168450\n",
      "168500\n",
      "168550\n",
      "168600\n",
      "168650\n",
      "168700\n",
      "168750\n",
      "168800\n",
      "168850\n",
      "168900\n",
      "168950\n",
      "169000\n",
      "169050\n",
      "169100\n",
      "169150\n",
      "169200\n",
      "169250\n",
      "169300\n",
      "169350\n",
      "169400\n",
      "169450\n",
      "169500\n",
      "169550\n",
      "169600\n",
      "169650\n",
      "169700\n",
      "169750\n",
      "169800\n",
      "169850\n",
      "169900\n",
      "169950\n",
      "170000\n",
      "170050\n",
      "170100\n",
      "170150\n",
      "170200\n",
      "170250\n",
      "170300\n",
      "170350\n",
      "170400\n",
      "170450\n",
      "Epoch: 3/20...  Training Step: 11350...  Training loss: 1.6515...  0.0873 sec/batch\n",
      "170500\n",
      "170550\n",
      "170600\n",
      "170650\n",
      "170700\n",
      "170750\n",
      "170800\n",
      "170850\n",
      "170900\n",
      "170950\n",
      "171000\n",
      "171050\n",
      "171100\n",
      "171150\n",
      "171200\n",
      "171250\n",
      "171300\n",
      "171350\n",
      "171400\n",
      "171450\n",
      "171500\n",
      "171550\n",
      "171600\n",
      "171650\n",
      "171700\n",
      "171750\n",
      "171800\n",
      "171850\n",
      "171900\n",
      "171950\n",
      "172000\n",
      "172050\n",
      "172100\n",
      "172150\n",
      "172200\n",
      "172250\n",
      "172300\n",
      "172350\n",
      "172400\n",
      "172450\n",
      "172500\n",
      "172550\n",
      "172600\n",
      "172650\n",
      "172700\n",
      "172750\n",
      "172800\n",
      "172850\n",
      "172900\n",
      "172950\n",
      "Epoch: 3/20...  Training Step: 11400...  Training loss: 1.5431...  0.0832 sec/batch\n",
      "173000\n",
      "173050\n",
      "173100\n",
      "173150\n",
      "173200\n",
      "173250\n",
      "173300\n",
      "173350\n",
      "173400\n",
      "173450\n",
      "173500\n",
      "173550\n",
      "173600\n",
      "173650\n",
      "173700\n",
      "173750\n",
      "173800\n",
      "173850\n",
      "173900\n",
      "173950\n",
      "174000\n",
      "174050\n",
      "174100\n",
      "174150\n",
      "174200\n",
      "174250\n",
      "174300\n",
      "174350\n",
      "174400\n",
      "174450\n",
      "174500\n",
      "174550\n",
      "174600\n",
      "174650\n",
      "174700\n",
      "174750\n",
      "174800\n",
      "174850\n",
      "174900\n",
      "174950\n",
      "175000\n",
      "175050\n",
      "175100\n",
      "175150\n",
      "175200\n",
      "175250\n",
      "175300\n",
      "175350\n",
      "175400\n",
      "175450\n",
      "Epoch: 3/20...  Training Step: 11450...  Training loss: 1.5636...  0.0769 sec/batch\n",
      "175500\n",
      "175550\n",
      "175600\n",
      "175650\n",
      "175700\n",
      "175750\n",
      "175800\n",
      "175850\n",
      "175900\n",
      "175950\n",
      "176000\n",
      "176050\n",
      "176100\n",
      "176150\n",
      "176200\n",
      "176250\n",
      "176300\n",
      "176350\n",
      "176400\n",
      "176450\n",
      "176500\n",
      "176550\n",
      "176600\n",
      "176650\n",
      "176700\n",
      "176750\n",
      "176800\n",
      "176850\n",
      "176900\n",
      "176950\n",
      "177000\n",
      "177050\n",
      "177100\n",
      "177150\n",
      "177200\n",
      "177250\n",
      "177300\n",
      "177350\n",
      "177400\n",
      "177450\n",
      "177500\n",
      "177550\n",
      "177600\n",
      "177650\n",
      "177700\n",
      "177750\n",
      "177800\n",
      "177850\n",
      "177900\n",
      "177950\n",
      "Epoch: 3/20...  Training Step: 11500...  Training loss: 1.6370...  0.0790 sec/batch\n",
      "178000\n",
      "178050\n",
      "178100\n",
      "178150\n",
      "178200\n",
      "178250\n",
      "178300\n",
      "178350\n",
      "178400\n",
      "178450\n",
      "178500\n",
      "178550\n",
      "178600\n",
      "178650\n",
      "178700\n",
      "178750\n",
      "178800\n",
      "178850\n",
      "178900\n",
      "178950\n",
      "179000\n",
      "179050\n",
      "179100\n",
      "179150\n",
      "179200\n",
      "179250\n",
      "179300\n",
      "179350\n",
      "179400\n",
      "179450\n",
      "179500\n",
      "179550\n",
      "179600\n",
      "179650\n",
      "179700\n",
      "179750\n",
      "179800\n",
      "179850\n",
      "179900\n",
      "179950\n",
      "180000\n",
      "180050\n",
      "180100\n",
      "180150\n",
      "180200\n",
      "180250\n",
      "180300\n",
      "180350\n",
      "180400\n",
      "180450\n",
      "Epoch: 3/20...  Training Step: 11550...  Training loss: 1.7008...  0.1025 sec/batch\n",
      "180500\n",
      "180550\n",
      "180600\n",
      "180650\n",
      "180700\n",
      "180750\n",
      "180800\n",
      "180850\n",
      "180900\n",
      "180950\n",
      "181000\n",
      "181050\n",
      "181100\n",
      "181150\n",
      "181200\n",
      "181250\n",
      "181300\n",
      "181350\n",
      "181400\n",
      "181450\n",
      "181500\n",
      "181550\n",
      "181600\n",
      "181650\n",
      "181700\n",
      "181750\n",
      "181800\n",
      "181850\n",
      "181900\n",
      "181950\n",
      "182000\n",
      "182050\n",
      "182100\n",
      "182150\n",
      "182200\n",
      "182250\n",
      "182300\n",
      "182350\n",
      "182400\n",
      "182450\n",
      "182500\n",
      "182550\n",
      "182600\n",
      "182650\n",
      "182700\n",
      "182750\n",
      "182800\n",
      "182850\n",
      "182900\n",
      "182950\n",
      "Epoch: 3/20...  Training Step: 11600...  Training loss: 1.7432...  0.0798 sec/batch\n",
      "183000\n",
      "183050\n",
      "183100\n",
      "183150\n",
      "183200\n",
      "183250\n",
      "183300\n",
      "183350\n",
      "183400\n",
      "183450\n",
      "183500\n",
      "183550\n",
      "183600\n",
      "183650\n",
      "183700\n",
      "183750\n",
      "183800\n",
      "183850\n",
      "183900\n",
      "183950\n",
      "184000\n",
      "184050\n",
      "184100\n",
      "184150\n",
      "184200\n",
      "184250\n",
      "184300\n",
      "184350\n",
      "184400\n",
      "184450\n",
      "184500\n",
      "184550\n",
      "184600\n",
      "184650\n",
      "184700\n",
      "184750\n",
      "184800\n",
      "184850\n",
      "184900\n",
      "184950\n",
      "185000\n",
      "185050\n",
      "185100\n",
      "185150\n",
      "185200\n",
      "185250\n",
      "185300\n",
      "185350\n",
      "185400\n",
      "185450\n",
      "Epoch: 3/20...  Training Step: 11650...  Training loss: 1.5822...  0.0753 sec/batch\n",
      "185500\n",
      "185550\n",
      "185600\n",
      "185650\n",
      "185700\n",
      "185750\n",
      "185800\n",
      "185850\n",
      "185900\n",
      "185950\n",
      "186000\n",
      "186050\n",
      "186100\n",
      "186150\n",
      "186200\n",
      "186250\n",
      "186300\n",
      "186350\n",
      "186400\n",
      "186450\n",
      "186500\n",
      "186550\n",
      "186600\n",
      "186650\n",
      "186700\n",
      "186750\n",
      "186800\n",
      "186850\n",
      "186900\n",
      "186950\n",
      "187000\n",
      "187050\n",
      "187100\n",
      "187150\n",
      "187200\n",
      "187250\n",
      "187300\n",
      "187350\n",
      "187400\n",
      "187450\n",
      "187500\n",
      "187550\n",
      "187600\n",
      "187650\n",
      "187700\n",
      "187750\n",
      "187800\n",
      "187850\n",
      "187900\n",
      "187950\n",
      "Epoch: 3/20...  Training Step: 11700...  Training loss: 1.5858...  0.0735 sec/batch\n",
      "188000\n",
      "188050\n",
      "188100\n",
      "188150\n",
      "188200\n",
      "188250\n",
      "188300\n",
      "188350\n",
      "188400\n",
      "188450\n",
      "188500\n",
      "188550\n",
      "188600\n",
      "188650\n",
      "188700\n",
      "188750\n",
      "188800\n",
      "188850\n",
      "188900\n",
      "188950\n",
      "189000\n",
      "189050\n",
      "189100\n",
      "189150\n",
      "189200\n",
      "189250\n",
      "189300\n",
      "189350\n",
      "189400\n",
      "189450\n",
      "189500\n",
      "189550\n",
      "189600\n",
      "189650\n",
      "189700\n",
      "189750\n",
      "189800\n",
      "189850\n",
      "189900\n",
      "189950\n",
      "190000\n",
      "190050\n",
      "190100\n",
      "190150\n",
      "190200\n",
      "190250\n",
      "190300\n",
      "190350\n",
      "190400\n",
      "190450\n",
      "Epoch: 3/20...  Training Step: 11750...  Training loss: 1.7077...  0.0791 sec/batch\n",
      "190500\n",
      "190550\n",
      "190600\n",
      "190650\n",
      "190700\n",
      "190750\n",
      "190800\n",
      "190850\n",
      "190900\n",
      "190950\n",
      "191000\n",
      "191050\n",
      "191100\n",
      "191150\n",
      "191200\n",
      "191250\n",
      "191300\n",
      "191350\n",
      "191400\n",
      "191450\n",
      "191500\n",
      "191550\n",
      "191600\n",
      "191650\n",
      "191700\n",
      "191750\n",
      "191800\n",
      "191850\n",
      "191900\n",
      "191950\n",
      "192000\n",
      "192050\n",
      "192100\n",
      "192150\n",
      "192200\n",
      "192250\n",
      "192300\n",
      "192350\n",
      "192400\n",
      "192450\n",
      "192500\n",
      "192550\n",
      "192600\n",
      "192650\n",
      "192700\n",
      "192750\n",
      "192800\n",
      "192850\n",
      "192900\n",
      "192950\n",
      "Epoch: 3/20...  Training Step: 11800...  Training loss: 1.6631...  0.0722 sec/batch\n",
      "193000\n",
      "193050\n",
      "193100\n",
      "193150\n",
      "193200\n",
      "193250\n",
      "193300\n",
      "193350\n",
      "193400\n",
      "193450\n",
      "193500\n",
      "193550\n",
      "193600\n",
      "193650\n",
      "193700\n",
      "193750\n",
      "193800\n",
      "193850\n",
      "193900\n",
      "193950\n",
      "194000\n",
      "194050\n",
      "194100\n",
      "194150\n",
      "194200\n",
      "194250\n",
      "194300\n",
      "194350\n",
      "194400\n",
      "194450\n",
      "194500\n",
      "194550\n",
      "194600\n",
      "194650\n",
      "194700\n",
      "194750\n",
      "194800\n",
      "194850\n",
      "194900\n",
      "194950\n",
      "195000\n",
      "195050\n",
      "195100\n",
      "195150\n",
      "195200\n",
      "195250\n",
      "195300\n",
      "195350\n",
      "195400\n",
      "195450\n",
      "Epoch: 3/20...  Training Step: 11850...  Training loss: 1.9793...  0.0759 sec/batch\n",
      "195500\n",
      "195550\n",
      "195600\n",
      "195650\n",
      "195700\n",
      "195750\n",
      "195800\n",
      "195850\n",
      "195900\n",
      "195950\n",
      "196000\n",
      "196050\n",
      "196100\n",
      "196150\n",
      "196200\n",
      "196250\n",
      "196300\n",
      "196350\n",
      "196400\n",
      "196450\n",
      "196500\n",
      "196550\n",
      "196600\n",
      "196650\n",
      "196700\n",
      "196750\n",
      "196800\n",
      "196850\n",
      "196900\n",
      "196950\n",
      "197000\n",
      "197050\n",
      "197100\n",
      "197150\n",
      "197200\n",
      "197250\n",
      "197300\n",
      "197350\n",
      "197400\n",
      "197450\n",
      "197500\n",
      "197550\n",
      "197600\n",
      "197650\n",
      "197700\n",
      "197750\n",
      "197800\n",
      "197850\n",
      "197900\n",
      "197950\n",
      "Epoch: 3/20...  Training Step: 11900...  Training loss: 1.7050...  0.0733 sec/batch\n",
      "198000\n",
      "198050\n",
      "198100\n",
      "198150\n",
      "198200\n",
      "198250\n",
      "198300\n",
      "198350\n",
      "198400\n",
      "198450\n",
      "3970\n",
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "800\n",
      "850\n",
      "900\n",
      "950\n",
      "1000\n",
      "1050\n",
      "1100\n",
      "1150\n",
      "1200\n",
      "1250\n",
      "1300\n",
      "1350\n",
      "1400\n",
      "1450\n",
      "1500\n",
      "1550\n",
      "1600\n",
      "1650\n",
      "1700\n",
      "1750\n",
      "1800\n",
      "1850\n",
      "1900\n",
      "1950\n",
      "Epoch: 4/20...  Training Step: 11950...  Training loss: 1.6739...  0.0798 sec/batch\n",
      "2000\n",
      "2050\n",
      "2100\n",
      "2150\n",
      "2200\n",
      "2250\n",
      "2300\n",
      "2350\n",
      "2400\n",
      "2450\n",
      "2500\n",
      "2550\n",
      "2600\n",
      "2650\n",
      "2700\n",
      "2750\n",
      "2800\n",
      "2850\n",
      "2900\n",
      "2950\n",
      "3000\n",
      "3050\n",
      "3100\n",
      "3150\n",
      "3200\n",
      "3250\n",
      "3300\n",
      "3350\n",
      "3400\n",
      "3450\n",
      "3500\n",
      "3550\n",
      "3600\n",
      "3650\n",
      "3700\n",
      "3750\n",
      "3800\n",
      "3850\n",
      "3900\n",
      "3950\n",
      "4000\n",
      "4050\n",
      "4100\n",
      "4150\n",
      "4200\n",
      "4250\n",
      "4300\n",
      "4350\n",
      "4400\n",
      "4450\n",
      "Epoch: 4/20...  Training Step: 12000...  Training loss: 1.5955...  0.0739 sec/batch\n",
      "4500\n",
      "4550\n",
      "4600\n",
      "4650\n",
      "4700\n",
      "4750\n",
      "4800\n",
      "4850\n",
      "4900\n",
      "4950\n",
      "5000\n",
      "5050\n",
      "5100\n",
      "5150\n",
      "5200\n",
      "5250\n",
      "5300\n",
      "5350\n",
      "5400\n",
      "5450\n",
      "5500\n",
      "5550\n",
      "5600\n",
      "5650\n",
      "5700\n",
      "5750\n",
      "5800\n",
      "5850\n",
      "5900\n",
      "5950\n",
      "6000\n",
      "6050\n",
      "6100\n",
      "6150\n",
      "6200\n",
      "6250\n",
      "6300\n",
      "6350\n",
      "6400\n",
      "6450\n",
      "6500\n",
      "6550\n",
      "6600\n",
      "6650\n",
      "6700\n",
      "6750\n",
      "6800\n",
      "6850\n",
      "6900\n",
      "6950\n",
      "Epoch: 4/20...  Training Step: 12050...  Training loss: 1.6773...  0.0748 sec/batch\n",
      "7000\n",
      "7050\n",
      "7100\n",
      "7150\n",
      "7200\n",
      "7250\n",
      "7300\n",
      "7350\n",
      "7400\n",
      "7450\n",
      "7500\n",
      "7550\n",
      "7600\n",
      "7650\n",
      "7700\n",
      "7750\n",
      "7800\n",
      "7850\n",
      "7900\n",
      "7950\n",
      "8000\n",
      "8050\n",
      "8100\n",
      "8150\n",
      "8200\n",
      "8250\n",
      "8300\n",
      "8350\n",
      "8400\n",
      "8450\n",
      "8500\n",
      "8550\n",
      "8600\n",
      "8650\n",
      "8700\n",
      "8750\n",
      "8800\n",
      "8850\n",
      "8900\n",
      "8950\n",
      "9000\n",
      "9050\n",
      "9100\n",
      "9150\n",
      "9200\n",
      "9250\n",
      "9300\n",
      "9350\n",
      "9400\n",
      "9450\n",
      "Epoch: 4/20...  Training Step: 12100...  Training loss: 1.6589...  0.0749 sec/batch\n",
      "9500\n",
      "9550\n",
      "9600\n",
      "9650\n",
      "9700\n",
      "9750\n",
      "9800\n",
      "9850\n",
      "9900\n",
      "9950\n",
      "10000\n",
      "10050\n",
      "10100\n",
      "10150\n",
      "10200\n",
      "10250\n",
      "10300\n",
      "10350\n",
      "10400\n",
      "10450\n",
      "10500\n",
      "10550\n",
      "10600\n",
      "10650\n",
      "10700\n",
      "10750\n",
      "10800\n",
      "10850\n",
      "10900\n",
      "10950\n",
      "11000\n",
      "11050\n",
      "11100\n",
      "11150\n",
      "11200\n",
      "11250\n",
      "11300\n",
      "11350\n",
      "11400\n",
      "11450\n",
      "11500\n",
      "11550\n",
      "11600\n",
      "11650\n",
      "11700\n",
      "11750\n",
      "11800\n",
      "11850\n",
      "11900\n",
      "11950\n",
      "Epoch: 4/20...  Training Step: 12150...  Training loss: 1.7028...  0.0752 sec/batch\n",
      "12000\n",
      "12050\n",
      "12100\n",
      "12150\n",
      "12200\n",
      "12250\n",
      "12300\n",
      "12350\n",
      "12400\n",
      "12450\n",
      "12500\n",
      "12550\n",
      "12600\n",
      "12650\n",
      "12700\n",
      "12750\n",
      "12800\n",
      "12850\n",
      "12900\n",
      "12950\n",
      "13000\n",
      "13050\n",
      "13100\n",
      "13150\n",
      "13200\n",
      "13250\n",
      "13300\n",
      "13350\n",
      "13400\n",
      "13450\n",
      "13500\n",
      "13550\n",
      "13600\n",
      "13650\n",
      "13700\n",
      "13750\n",
      "13800\n",
      "13850\n",
      "13900\n",
      "13950\n",
      "14000\n",
      "14050\n",
      "14100\n",
      "14150\n",
      "14200\n",
      "14250\n",
      "14300\n",
      "14350\n",
      "14400\n",
      "14450\n",
      "Epoch: 4/20...  Training Step: 12200...  Training loss: 1.6429...  0.1262 sec/batch\n",
      "14500\n",
      "14550\n",
      "14600\n",
      "14650\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14700\n",
      "14750\n",
      "14800\n",
      "14850\n",
      "14900\n",
      "14950\n",
      "15000\n",
      "15050\n",
      "15100\n",
      "15150\n",
      "15200\n",
      "15250\n",
      "15300\n",
      "15350\n",
      "15400\n",
      "15450\n",
      "15500\n",
      "15550\n",
      "15600\n",
      "15650\n",
      "15700\n",
      "15750\n",
      "15800\n",
      "15850\n",
      "15900\n",
      "15950\n",
      "16000\n",
      "16050\n",
      "16100\n",
      "16150\n",
      "16200\n",
      "16250\n",
      "16300\n",
      "16350\n",
      "16400\n",
      "16450\n",
      "16500\n",
      "16550\n",
      "16600\n",
      "16650\n",
      "16700\n",
      "16750\n",
      "16800\n",
      "16850\n",
      "16900\n",
      "16950\n",
      "Epoch: 4/20...  Training Step: 12250...  Training loss: 1.6249...  0.0891 sec/batch\n",
      "17000\n",
      "17050\n",
      "17100\n",
      "17150\n",
      "17200\n",
      "17250\n",
      "17300\n",
      "17350\n",
      "17400\n",
      "17450\n",
      "17500\n",
      "17550\n",
      "17600\n",
      "17650\n",
      "17700\n",
      "17750\n",
      "17800\n",
      "17850\n",
      "17900\n",
      "17950\n",
      "18000\n",
      "18050\n",
      "18100\n",
      "18150\n",
      "18200\n",
      "18250\n",
      "18300\n",
      "18350\n",
      "18400\n",
      "18450\n",
      "18500\n",
      "18550\n",
      "18600\n",
      "18650\n",
      "18700\n",
      "18750\n",
      "18800\n",
      "18850\n",
      "18900\n",
      "18950\n",
      "19000\n",
      "19050\n",
      "19100\n",
      "19150\n",
      "19200\n",
      "19250\n",
      "19300\n",
      "19350\n",
      "19400\n",
      "19450\n",
      "Epoch: 4/20...  Training Step: 12300...  Training loss: 1.5103...  0.0852 sec/batch\n",
      "19500\n",
      "19550\n",
      "19600\n",
      "19650\n",
      "19700\n",
      "19750\n",
      "19800\n",
      "19850\n",
      "19900\n",
      "19950\n",
      "20000\n",
      "20050\n",
      "20100\n",
      "20150\n",
      "20200\n",
      "20250\n",
      "20300\n",
      "20350\n",
      "20400\n",
      "20450\n",
      "20500\n",
      "20550\n",
      "20600\n",
      "20650\n",
      "20700\n",
      "20750\n",
      "20800\n",
      "20850\n",
      "20900\n",
      "20950\n",
      "21000\n",
      "21050\n",
      "21100\n",
      "21150\n",
      "21200\n",
      "21250\n",
      "21300\n",
      "21350\n",
      "21400\n",
      "21450\n",
      "21500\n",
      "21550\n",
      "21600\n",
      "21650\n",
      "21700\n",
      "21750\n",
      "21800\n",
      "21850\n",
      "21900\n",
      "21950\n",
      "Epoch: 4/20...  Training Step: 12350...  Training loss: 1.6581...  0.0765 sec/batch\n",
      "22000\n",
      "22050\n",
      "22100\n",
      "22150\n",
      "22200\n",
      "22250\n",
      "22300\n",
      "22350\n",
      "22400\n",
      "22450\n",
      "22500\n",
      "22550\n",
      "22600\n",
      "22650\n",
      "22700\n",
      "22750\n",
      "22800\n",
      "22850\n",
      "22900\n",
      "22950\n",
      "23000\n",
      "23050\n",
      "23100\n",
      "23150\n",
      "23200\n",
      "23250\n",
      "23300\n",
      "23350\n",
      "23400\n",
      "23450\n",
      "23500\n",
      "23550\n",
      "23600\n",
      "23650\n",
      "23700\n",
      "23750\n",
      "23800\n",
      "23850\n",
      "23900\n",
      "23950\n",
      "24000\n",
      "24050\n",
      "24100\n",
      "24150\n",
      "24200\n",
      "24250\n",
      "24300\n",
      "24350\n",
      "24400\n",
      "24450\n",
      "Epoch: 4/20...  Training Step: 12400...  Training loss: 1.6596...  0.0809 sec/batch\n",
      "24500\n",
      "24550\n",
      "24600\n",
      "24650\n",
      "24700\n",
      "24750\n",
      "24800\n",
      "24850\n",
      "24900\n",
      "24950\n",
      "25000\n",
      "25050\n",
      "25100\n",
      "25150\n",
      "25200\n",
      "25250\n",
      "25300\n",
      "25350\n",
      "25400\n",
      "25450\n",
      "25500\n",
      "25550\n",
      "25600\n",
      "25650\n",
      "25700\n",
      "25750\n",
      "25800\n",
      "25850\n",
      "25900\n",
      "25950\n",
      "26000\n",
      "26050\n",
      "26100\n",
      "26150\n",
      "26200\n",
      "26250\n",
      "26300\n",
      "26350\n",
      "26400\n",
      "26450\n",
      "26500\n",
      "26550\n",
      "26600\n",
      "26650\n",
      "26700\n",
      "26750\n",
      "26800\n",
      "26850\n",
      "26900\n",
      "26950\n",
      "Epoch: 4/20...  Training Step: 12450...  Training loss: 1.4483...  0.0850 sec/batch\n",
      "27000\n",
      "27050\n",
      "27100\n",
      "27150\n",
      "27200\n",
      "27250\n",
      "27300\n",
      "27350\n",
      "27400\n",
      "27450\n",
      "27500\n",
      "27550\n",
      "27600\n",
      "27650\n",
      "27700\n",
      "27750\n",
      "27800\n",
      "27850\n",
      "27900\n",
      "27950\n",
      "28000\n",
      "28050\n",
      "28100\n",
      "28150\n",
      "28200\n",
      "28250\n",
      "28300\n",
      "28350\n",
      "28400\n",
      "28450\n",
      "28500\n",
      "28550\n",
      "28600\n",
      "28650\n",
      "28700\n",
      "28750\n",
      "28800\n",
      "28850\n",
      "28900\n",
      "28950\n",
      "29000\n",
      "29050\n",
      "29100\n",
      "29150\n",
      "29200\n",
      "29250\n",
      "29300\n",
      "29350\n",
      "29400\n",
      "29450\n",
      "Epoch: 4/20...  Training Step: 12500...  Training loss: 1.7838...  0.0872 sec/batch\n",
      "29500\n",
      "29550\n",
      "29600\n",
      "29650\n",
      "29700\n",
      "29750\n",
      "29800\n",
      "29850\n",
      "29900\n",
      "29950\n",
      "30000\n",
      "30050\n",
      "30100\n",
      "30150\n",
      "30200\n",
      "30250\n",
      "30300\n",
      "30350\n",
      "30400\n",
      "30450\n",
      "30500\n",
      "30550\n",
      "30600\n",
      "30650\n",
      "30700\n",
      "30750\n",
      "30800\n",
      "30850\n",
      "30900\n",
      "30950\n",
      "31000\n",
      "31050\n",
      "31100\n",
      "31150\n",
      "31200\n",
      "31250\n",
      "31300\n",
      "31350\n",
      "31400\n",
      "31450\n",
      "31500\n",
      "31550\n",
      "31600\n",
      "31650\n",
      "31700\n",
      "31750\n",
      "31800\n",
      "31850\n",
      "31900\n",
      "31950\n",
      "Epoch: 4/20...  Training Step: 12550...  Training loss: 1.5373...  0.0872 sec/batch\n",
      "32000\n",
      "32050\n",
      "32100\n",
      "32150\n",
      "32200\n",
      "32250\n",
      "32300\n",
      "32350\n",
      "32400\n",
      "32450\n",
      "32500\n",
      "32550\n",
      "32600\n",
      "32650\n",
      "32700\n",
      "32750\n",
      "32800\n",
      "32850\n",
      "32900\n",
      "32950\n",
      "33000\n",
      "33050\n",
      "33100\n",
      "33150\n",
      "33200\n",
      "33250\n",
      "33300\n",
      "33350\n",
      "33400\n",
      "33450\n",
      "33500\n",
      "33550\n",
      "33600\n",
      "33650\n",
      "33700\n",
      "33750\n",
      "33800\n",
      "33850\n",
      "33900\n",
      "33950\n",
      "34000\n",
      "34050\n",
      "34100\n",
      "34150\n",
      "34200\n",
      "34250\n",
      "34300\n",
      "34350\n",
      "34400\n",
      "34450\n",
      "Epoch: 4/20...  Training Step: 12600...  Training loss: 1.5608...  0.0862 sec/batch\n",
      "34500\n",
      "34550\n",
      "34600\n",
      "34650\n",
      "34700\n",
      "34750\n",
      "34800\n",
      "34850\n",
      "34900\n",
      "34950\n",
      "35000\n",
      "35050\n",
      "35100\n",
      "35150\n",
      "35200\n",
      "35250\n",
      "35300\n",
      "35350\n",
      "35400\n",
      "35450\n",
      "35500\n",
      "35550\n",
      "35600\n",
      "35650\n",
      "35700\n",
      "35750\n",
      "35800\n",
      "35850\n",
      "35900\n",
      "35950\n",
      "36000\n",
      "36050\n",
      "36100\n",
      "36150\n",
      "36200\n",
      "36250\n",
      "36300\n",
      "36350\n",
      "36400\n",
      "36450\n",
      "36500\n",
      "36550\n",
      "36600\n",
      "36650\n",
      "36700\n",
      "36750\n",
      "36800\n",
      "36850\n",
      "36900\n",
      "36950\n",
      "Epoch: 4/20...  Training Step: 12650...  Training loss: 1.5065...  0.0846 sec/batch\n",
      "37000\n",
      "37050\n",
      "37100\n",
      "37150\n",
      "37200\n",
      "37250\n",
      "37300\n",
      "37350\n",
      "37400\n",
      "37450\n",
      "37500\n",
      "37550\n",
      "37600\n",
      "37650\n",
      "37700\n",
      "37750\n",
      "37800\n",
      "37850\n",
      "37900\n",
      "37950\n",
      "38000\n",
      "38050\n",
      "38100\n",
      "38150\n",
      "38200\n",
      "38250\n",
      "38300\n",
      "38350\n",
      "38400\n",
      "38450\n",
      "38500\n",
      "38550\n",
      "38600\n",
      "38650\n",
      "38700\n",
      "38750\n",
      "38800\n",
      "38850\n",
      "38900\n",
      "38950\n",
      "39000\n",
      "39050\n",
      "39100\n",
      "39150\n",
      "39200\n",
      "39250\n",
      "39300\n",
      "39350\n",
      "39400\n",
      "39450\n",
      "Epoch: 4/20...  Training Step: 12700...  Training loss: 1.5459...  0.0888 sec/batch\n",
      "39500\n",
      "39550\n",
      "39600\n",
      "39650\n",
      "39700\n",
      "39750\n",
      "39800\n",
      "39850\n",
      "39900\n",
      "39950\n",
      "40000\n",
      "40050\n",
      "40100\n",
      "40150\n",
      "40200\n",
      "40250\n",
      "40300\n",
      "40350\n",
      "40400\n",
      "40450\n",
      "40500\n",
      "40550\n",
      "40600\n",
      "40650\n",
      "40700\n",
      "40750\n",
      "40800\n",
      "40850\n",
      "40900\n",
      "40950\n",
      "41000\n",
      "41050\n",
      "41100\n",
      "41150\n",
      "41200\n",
      "41250\n",
      "41300\n",
      "41350\n",
      "41400\n",
      "41450\n",
      "41500\n",
      "41550\n",
      "41600\n",
      "41650\n",
      "41700\n",
      "41750\n",
      "41800\n",
      "41850\n",
      "41900\n",
      "41950\n",
      "Epoch: 4/20...  Training Step: 12750...  Training loss: 1.6076...  0.0822 sec/batch\n",
      "42000\n",
      "42050\n",
      "42100\n",
      "42150\n",
      "42200\n",
      "42250\n",
      "42300\n",
      "42350\n",
      "42400\n",
      "42450\n",
      "42500\n",
      "42550\n",
      "42600\n",
      "42650\n",
      "42700\n",
      "42750\n",
      "42800\n",
      "42850\n",
      "42900\n",
      "42950\n",
      "43000\n",
      "43050\n",
      "43100\n",
      "43150\n",
      "43200\n",
      "43250\n",
      "43300\n",
      "43350\n",
      "43400\n",
      "43450\n",
      "43500\n",
      "43550\n",
      "43600\n",
      "43650\n",
      "43700\n",
      "43750\n",
      "43800\n",
      "43850\n",
      "43900\n",
      "43950\n",
      "44000\n",
      "44050\n",
      "44100\n",
      "44150\n",
      "44200\n",
      "44250\n",
      "44300\n",
      "44350\n",
      "44400\n",
      "44450\n",
      "Epoch: 4/20...  Training Step: 12800...  Training loss: 1.6135...  0.0826 sec/batch\n",
      "44500\n",
      "44550\n",
      "44600\n",
      "44650\n",
      "44700\n",
      "44750\n",
      "44800\n",
      "44850\n",
      "44900\n",
      "44950\n",
      "45000\n",
      "45050\n",
      "45100\n",
      "45150\n",
      "45200\n",
      "45250\n",
      "45300\n",
      "45350\n",
      "45400\n",
      "45450\n",
      "45500\n",
      "45550\n",
      "45600\n",
      "45650\n",
      "45700\n",
      "45750\n",
      "45800\n",
      "45850\n",
      "45900\n",
      "45950\n",
      "46000\n",
      "46050\n",
      "46100\n",
      "46150\n",
      "46200\n",
      "46250\n",
      "46300\n",
      "46350\n",
      "46400\n",
      "46450\n",
      "46500\n",
      "46550\n",
      "46600\n",
      "46650\n",
      "46700\n",
      "46750\n",
      "46800\n",
      "46850\n",
      "46900\n",
      "46950\n",
      "Epoch: 4/20...  Training Step: 12850...  Training loss: 1.5341...  0.0954 sec/batch\n",
      "47000\n",
      "47050\n",
      "47100\n",
      "47150\n",
      "47200\n",
      "47250\n",
      "47300\n",
      "47350\n",
      "47400\n",
      "47450\n",
      "47500\n",
      "47550\n",
      "47600\n",
      "47650\n",
      "47700\n",
      "47750\n",
      "47800\n",
      "47850\n",
      "47900\n",
      "47950\n",
      "48000\n",
      "48050\n",
      "48100\n",
      "48150\n",
      "48200\n",
      "48250\n",
      "48300\n",
      "48350\n",
      "48400\n",
      "48450\n",
      "48500\n",
      "48550\n",
      "48600\n",
      "48650\n",
      "48700\n",
      "48750\n",
      "48800\n",
      "48850\n",
      "48900\n",
      "48950\n",
      "49000\n",
      "49050\n",
      "49100\n",
      "49150\n",
      "49200\n",
      "49250\n",
      "49300\n",
      "49350\n",
      "49400\n",
      "49450\n",
      "Epoch: 4/20...  Training Step: 12900...  Training loss: 1.5394...  0.0805 sec/batch\n",
      "49500\n",
      "49550\n",
      "49600\n",
      "49650\n",
      "49700\n",
      "49750\n",
      "49800\n",
      "49850\n",
      "49900\n",
      "49950\n",
      "50000\n",
      "50050\n",
      "50100\n",
      "50150\n",
      "50200\n",
      "50250\n",
      "50300\n",
      "50350\n",
      "50400\n",
      "50450\n",
      "50500\n",
      "50550\n",
      "50600\n",
      "50650\n",
      "50700\n",
      "50750\n",
      "50800\n",
      "50850\n",
      "50900\n",
      "50950\n",
      "51000\n",
      "51050\n",
      "51100\n",
      "51150\n",
      "51200\n",
      "51250\n",
      "51300\n",
      "51350\n",
      "51400\n",
      "51450\n",
      "51500\n",
      "51550\n",
      "51600\n",
      "51650\n",
      "51700\n",
      "51750\n",
      "51800\n",
      "51850\n",
      "51900\n",
      "51950\n",
      "Epoch: 4/20...  Training Step: 12950...  Training loss: 1.5811...  0.0890 sec/batch\n",
      "52000\n",
      "52050\n",
      "52100\n",
      "52150\n",
      "52200\n",
      "52250\n",
      "52300\n",
      "52350\n",
      "52400\n",
      "52450\n",
      "52500\n",
      "52550\n",
      "52600\n",
      "52650\n",
      "52700\n",
      "52750\n",
      "52800\n",
      "52850\n",
      "52900\n",
      "52950\n",
      "53000\n",
      "53050\n",
      "53100\n",
      "53150\n",
      "53200\n",
      "53250\n",
      "53300\n",
      "53350\n",
      "53400\n",
      "53450\n",
      "53500\n",
      "53550\n",
      "53600\n",
      "53650\n",
      "53700\n",
      "53750\n",
      "53800\n",
      "53850\n",
      "53900\n",
      "53950\n",
      "54000\n",
      "54050\n",
      "54100\n",
      "54150\n",
      "54200\n",
      "54250\n",
      "54300\n",
      "54350\n",
      "54400\n",
      "54450\n",
      "Epoch: 4/20...  Training Step: 13000...  Training loss: 1.7547...  0.0798 sec/batch\n",
      "54500\n",
      "54550\n",
      "54600\n",
      "54650\n",
      "54700\n",
      "54750\n",
      "54800\n",
      "54850\n",
      "54900\n",
      "54950\n",
      "55000\n",
      "55050\n",
      "55100\n",
      "55150\n",
      "55200\n",
      "55250\n",
      "55300\n",
      "55350\n",
      "55400\n",
      "55450\n",
      "55500\n",
      "55550\n",
      "55600\n",
      "55650\n",
      "55700\n",
      "55750\n",
      "55800\n",
      "55850\n",
      "55900\n",
      "55950\n",
      "56000\n",
      "56050\n",
      "56100\n",
      "56150\n",
      "56200\n",
      "56250\n",
      "56300\n",
      "56350\n",
      "56400\n",
      "56450\n",
      "56500\n",
      "56550\n",
      "56600\n",
      "56650\n",
      "56700\n",
      "56750\n",
      "56800\n",
      "56850\n",
      "56900\n",
      "56950\n",
      "Epoch: 4/20...  Training Step: 13050...  Training loss: 1.6821...  0.0804 sec/batch\n",
      "57000\n",
      "57050\n",
      "57100\n",
      "57150\n",
      "57200\n",
      "57250\n",
      "57300\n",
      "57350\n",
      "57400\n",
      "57450\n",
      "57500\n",
      "57550\n",
      "57600\n",
      "57650\n",
      "57700\n",
      "57750\n",
      "57800\n",
      "57850\n",
      "57900\n",
      "57950\n",
      "58000\n",
      "58050\n",
      "58100\n",
      "58150\n",
      "58200\n",
      "58250\n",
      "58300\n",
      "58350\n",
      "58400\n",
      "58450\n",
      "58500\n",
      "58550\n",
      "58600\n",
      "58650\n",
      "58700\n",
      "58750\n",
      "58800\n",
      "58850\n",
      "58900\n",
      "58950\n",
      "59000\n",
      "59050\n",
      "59100\n",
      "59150\n",
      "59200\n",
      "59250\n",
      "59300\n",
      "59350\n",
      "59400\n",
      "59450\n",
      "Epoch: 4/20...  Training Step: 13100...  Training loss: 1.5210...  0.0762 sec/batch\n",
      "59500\n",
      "59550\n",
      "59600\n",
      "59650\n",
      "59700\n",
      "59750\n",
      "59800\n",
      "59850\n",
      "59900\n",
      "59950\n",
      "60000\n",
      "60050\n",
      "60100\n",
      "60150\n",
      "60200\n",
      "60250\n",
      "60300\n",
      "60350\n",
      "60400\n",
      "60450\n",
      "60500\n",
      "60550\n",
      "60600\n",
      "60650\n",
      "60700\n",
      "60750\n",
      "60800\n",
      "60850\n",
      "60900\n",
      "60950\n",
      "61000\n",
      "61050\n",
      "61100\n",
      "61150\n",
      "61200\n",
      "61250\n",
      "61300\n",
      "61350\n",
      "61400\n",
      "61450\n",
      "61500\n",
      "61550\n",
      "61600\n",
      "61650\n",
      "61700\n",
      "61750\n",
      "61800\n",
      "61850\n",
      "61900\n",
      "61950\n",
      "Epoch: 4/20...  Training Step: 13150...  Training loss: 1.5295...  0.0966 sec/batch\n",
      "62000\n",
      "62050\n",
      "62100\n",
      "62150\n",
      "62200\n",
      "62250\n",
      "62300\n",
      "62350\n",
      "62400\n",
      "62450\n",
      "62500\n",
      "62550\n",
      "62600\n",
      "62650\n",
      "62700\n",
      "62750\n",
      "62800\n",
      "62850\n",
      "62900\n",
      "62950\n",
      "63000\n",
      "63050\n",
      "63100\n",
      "63150\n",
      "63200\n",
      "63250\n",
      "63300\n",
      "63350\n",
      "63400\n",
      "63450\n",
      "63500\n",
      "63550\n",
      "63600\n",
      "63650\n",
      "63700\n",
      "63750\n",
      "63800\n",
      "63850\n",
      "63900\n",
      "63950\n",
      "64000\n",
      "64050\n",
      "64100\n",
      "64150\n",
      "64200\n",
      "64250\n",
      "64300\n",
      "64350\n",
      "64400\n",
      "64450\n",
      "Epoch: 4/20...  Training Step: 13200...  Training loss: 1.7104...  0.0794 sec/batch\n",
      "64500\n",
      "64550\n",
      "64600\n",
      "64650\n",
      "64700\n",
      "64750\n",
      "64800\n",
      "64850\n",
      "64900\n",
      "64950\n",
      "65000\n",
      "65050\n",
      "65100\n",
      "65150\n",
      "65200\n",
      "65250\n",
      "65300\n",
      "65350\n",
      "65400\n",
      "65450\n",
      "65500\n",
      "65550\n",
      "65600\n",
      "65650\n",
      "65700\n",
      "65750\n",
      "65800\n",
      "65850\n",
      "65900\n",
      "65950\n",
      "66000\n",
      "66050\n",
      "66100\n",
      "66150\n",
      "66200\n",
      "66250\n",
      "66300\n",
      "66350\n",
      "66400\n",
      "66450\n",
      "66500\n",
      "66550\n",
      "66600\n",
      "66650\n",
      "66700\n",
      "66750\n",
      "66800\n",
      "66850\n",
      "66900\n",
      "66950\n",
      "Epoch: 4/20...  Training Step: 13250...  Training loss: 1.5742...  0.0768 sec/batch\n",
      "67000\n",
      "67050\n",
      "67100\n",
      "67150\n",
      "67200\n",
      "67250\n",
      "67300\n",
      "67350\n",
      "67400\n",
      "67450\n",
      "67500\n",
      "67550\n",
      "67600\n",
      "67650\n",
      "67700\n",
      "67750\n",
      "67800\n",
      "67850\n",
      "67900\n",
      "67950\n",
      "68000\n",
      "68050\n",
      "68100\n",
      "68150\n",
      "68200\n",
      "68250\n",
      "68300\n",
      "68350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68400\n",
      "68450\n",
      "68500\n",
      "68550\n",
      "68600\n",
      "68650\n",
      "68700\n",
      "68750\n",
      "68800\n",
      "68850\n",
      "68900\n",
      "68950\n",
      "69000\n",
      "69050\n",
      "69100\n",
      "69150\n",
      "69200\n",
      "69250\n",
      "69300\n",
      "69350\n",
      "69400\n",
      "69450\n",
      "Epoch: 4/20...  Training Step: 13300...  Training loss: 1.6613...  0.0821 sec/batch\n",
      "69500\n",
      "69550\n",
      "69600\n",
      "69650\n",
      "69700\n",
      "69750\n",
      "69800\n",
      "69850\n",
      "69900\n",
      "69950\n",
      "70000\n",
      "70050\n",
      "70100\n",
      "70150\n",
      "70200\n",
      "70250\n",
      "70300\n",
      "70350\n",
      "70400\n",
      "70450\n",
      "70500\n",
      "70550\n",
      "70600\n",
      "70650\n",
      "70700\n",
      "70750\n",
      "70800\n",
      "70850\n",
      "70900\n",
      "70950\n",
      "71000\n",
      "71050\n",
      "71100\n",
      "71150\n",
      "71200\n",
      "71250\n",
      "71300\n",
      "71350\n",
      "71400\n",
      "71450\n",
      "71500\n",
      "71550\n",
      "71600\n",
      "71650\n",
      "71700\n",
      "71750\n",
      "71800\n",
      "71850\n",
      "71900\n",
      "71950\n",
      "Epoch: 4/20...  Training Step: 13350...  Training loss: 1.5196...  0.0848 sec/batch\n",
      "72000\n",
      "72050\n",
      "72100\n",
      "72150\n",
      "72200\n",
      "72250\n",
      "72300\n",
      "72350\n",
      "72400\n",
      "72450\n",
      "72500\n",
      "72550\n",
      "72600\n",
      "72650\n",
      "72700\n",
      "72750\n",
      "72800\n",
      "72850\n",
      "72900\n",
      "72950\n",
      "73000\n",
      "73050\n",
      "73100\n",
      "73150\n",
      "73200\n",
      "73250\n",
      "73300\n",
      "73350\n",
      "73400\n",
      "73450\n",
      "73500\n",
      "73550\n",
      "73600\n",
      "73650\n",
      "73700\n",
      "73750\n",
      "73800\n",
      "73850\n",
      "73900\n",
      "73950\n",
      "74000\n",
      "74050\n",
      "74100\n",
      "74150\n",
      "74200\n",
      "74250\n",
      "74300\n",
      "74350\n",
      "74400\n",
      "74450\n",
      "Epoch: 4/20...  Training Step: 13400...  Training loss: 1.6377...  0.0759 sec/batch\n",
      "74500\n",
      "74550\n",
      "74600\n",
      "74650\n",
      "74700\n",
      "74750\n",
      "74800\n",
      "74850\n",
      "74900\n",
      "74950\n",
      "75000\n",
      "75050\n",
      "75100\n",
      "75150\n",
      "75200\n",
      "75250\n",
      "75300\n",
      "75350\n",
      "75400\n",
      "75450\n",
      "75500\n",
      "75550\n",
      "75600\n",
      "75650\n",
      "75700\n",
      "75750\n",
      "75800\n",
      "75850\n",
      "75900\n",
      "75950\n",
      "76000\n",
      "76050\n",
      "76100\n",
      "76150\n",
      "76200\n",
      "76250\n",
      "76300\n",
      "76350\n",
      "76400\n",
      "76450\n",
      "76500\n",
      "76550\n",
      "76600\n",
      "76650\n",
      "76700\n",
      "76750\n",
      "76800\n",
      "76850\n",
      "76900\n",
      "76950\n",
      "Epoch: 4/20...  Training Step: 13450...  Training loss: 1.6652...  0.0891 sec/batch\n",
      "77000\n",
      "77050\n",
      "77100\n",
      "77150\n",
      "77200\n",
      "77250\n",
      "77300\n",
      "77350\n",
      "77400\n",
      "77450\n",
      "77500\n",
      "77550\n",
      "77600\n",
      "77650\n",
      "77700\n",
      "77750\n",
      "77800\n",
      "77850\n",
      "77900\n",
      "77950\n",
      "78000\n",
      "78050\n",
      "78100\n",
      "78150\n",
      "78200\n",
      "78250\n",
      "78300\n",
      "78350\n",
      "78400\n",
      "78450\n",
      "78500\n",
      "78550\n",
      "78600\n",
      "78650\n",
      "78700\n",
      "78750\n",
      "78800\n",
      "78850\n",
      "78900\n",
      "78950\n",
      "79000\n",
      "79050\n",
      "79100\n",
      "79150\n",
      "79200\n",
      "79250\n",
      "79300\n",
      "79350\n",
      "79400\n",
      "79450\n",
      "Epoch: 4/20...  Training Step: 13500...  Training loss: 1.6199...  0.0838 sec/batch\n",
      "79500\n",
      "79550\n",
      "79600\n",
      "79650\n",
      "79700\n",
      "79750\n",
      "79800\n",
      "79850\n",
      "79900\n",
      "79950\n",
      "80000\n",
      "80050\n",
      "80100\n",
      "80150\n",
      "80200\n",
      "80250\n",
      "80300\n",
      "80350\n",
      "80400\n",
      "80450\n",
      "80500\n",
      "80550\n",
      "80600\n",
      "80650\n",
      "80700\n",
      "80750\n",
      "80800\n",
      "80850\n",
      "80900\n",
      "80950\n",
      "81000\n",
      "81050\n",
      "81100\n",
      "81150\n",
      "81200\n",
      "81250\n",
      "81300\n",
      "81350\n",
      "81400\n",
      "81450\n",
      "81500\n",
      "81550\n",
      "81600\n",
      "81650\n",
      "81700\n",
      "81750\n",
      "81800\n",
      "81850\n",
      "81900\n",
      "81950\n",
      "Epoch: 4/20...  Training Step: 13550...  Training loss: 1.6589...  0.0840 sec/batch\n",
      "82000\n",
      "82050\n",
      "82100\n",
      "82150\n",
      "82200\n",
      "82250\n",
      "82300\n",
      "82350\n",
      "82400\n",
      "82450\n",
      "82500\n",
      "82550\n",
      "82600\n",
      "82650\n",
      "82700\n",
      "82750\n",
      "82800\n",
      "82850\n",
      "82900\n",
      "82950\n",
      "83000\n",
      "83050\n",
      "83100\n",
      "83150\n",
      "83200\n",
      "83250\n",
      "83300\n",
      "83350\n",
      "83400\n",
      "83450\n",
      "83500\n",
      "83550\n",
      "83600\n",
      "83650\n",
      "83700\n",
      "83750\n",
      "83800\n",
      "83850\n",
      "83900\n",
      "83950\n",
      "84000\n",
      "84050\n",
      "84100\n",
      "84150\n",
      "84200\n",
      "84250\n",
      "84300\n",
      "84350\n",
      "84400\n",
      "84450\n",
      "Epoch: 4/20...  Training Step: 13600...  Training loss: 1.5521...  0.0790 sec/batch\n",
      "84500\n",
      "84550\n",
      "84600\n",
      "84650\n",
      "84700\n",
      "84750\n",
      "84800\n",
      "84850\n",
      "84900\n",
      "84950\n",
      "85000\n",
      "85050\n",
      "85100\n",
      "85150\n",
      "85200\n",
      "85250\n",
      "85300\n",
      "85350\n",
      "85400\n",
      "85450\n",
      "85500\n",
      "85550\n",
      "85600\n",
      "85650\n",
      "85700\n",
      "85750\n",
      "85800\n",
      "85850\n",
      "85900\n",
      "85950\n",
      "86000\n",
      "86050\n",
      "86100\n",
      "86150\n",
      "86200\n",
      "86250\n",
      "86300\n",
      "86350\n",
      "86400\n",
      "86450\n",
      "86500\n",
      "86550\n",
      "86600\n",
      "86650\n",
      "86700\n",
      "86750\n",
      "86800\n",
      "86850\n",
      "86900\n",
      "86950\n",
      "Epoch: 4/20...  Training Step: 13650...  Training loss: 1.5943...  0.0815 sec/batch\n",
      "87000\n",
      "87050\n",
      "87100\n",
      "87150\n",
      "87200\n",
      "87250\n",
      "87300\n",
      "87350\n",
      "87400\n",
      "87450\n",
      "87500\n",
      "87550\n",
      "87600\n",
      "87650\n",
      "87700\n",
      "87750\n",
      "87800\n",
      "87850\n",
      "87900\n",
      "87950\n",
      "88000\n",
      "88050\n",
      "88100\n",
      "88150\n",
      "88200\n",
      "88250\n",
      "88300\n",
      "88350\n",
      "88400\n",
      "88450\n",
      "88500\n",
      "88550\n",
      "88600\n",
      "88650\n",
      "88700\n",
      "88750\n",
      "88800\n",
      "88850\n",
      "88900\n",
      "88950\n",
      "89000\n",
      "89050\n",
      "89100\n",
      "89150\n",
      "89200\n",
      "89250\n",
      "89300\n",
      "89350\n",
      "89400\n",
      "89450\n",
      "Epoch: 4/20...  Training Step: 13700...  Training loss: 1.5423...  0.0750 sec/batch\n",
      "89500\n",
      "89550\n",
      "89600\n",
      "89650\n",
      "89700\n",
      "89750\n",
      "89800\n",
      "89850\n",
      "89900\n",
      "89950\n",
      "90000\n",
      "90050\n",
      "90100\n",
      "90150\n",
      "90200\n",
      "90250\n",
      "90300\n",
      "90350\n",
      "90400\n",
      "90450\n",
      "90500\n",
      "90550\n",
      "90600\n",
      "90650\n",
      "90700\n",
      "90750\n",
      "90800\n",
      "90850\n",
      "90900\n",
      "90950\n",
      "91000\n",
      "91050\n",
      "91100\n",
      "91150\n",
      "91200\n",
      "91250\n",
      "91300\n",
      "91350\n",
      "91400\n",
      "91450\n",
      "91500\n",
      "91550\n",
      "91600\n",
      "91650\n",
      "91700\n",
      "91750\n",
      "91800\n",
      "91850\n",
      "91900\n",
      "91950\n",
      "Epoch: 4/20...  Training Step: 13750...  Training loss: 1.6010...  0.0805 sec/batch\n",
      "92000\n",
      "92050\n",
      "92100\n",
      "92150\n",
      "92200\n",
      "92250\n",
      "92300\n",
      "92350\n",
      "92400\n",
      "92450\n",
      "92500\n",
      "92550\n",
      "92600\n",
      "92650\n",
      "92700\n",
      "92750\n",
      "92800\n",
      "92850\n",
      "92900\n",
      "92950\n",
      "93000\n",
      "93050\n",
      "93100\n",
      "93150\n",
      "93200\n",
      "93250\n",
      "93300\n",
      "93350\n",
      "93400\n",
      "93450\n",
      "93500\n",
      "93550\n",
      "93600\n",
      "93650\n",
      "93700\n",
      "93750\n",
      "93800\n",
      "93850\n",
      "93900\n",
      "93950\n",
      "94000\n",
      "94050\n",
      "94100\n",
      "94150\n",
      "94200\n",
      "94250\n",
      "94300\n",
      "94350\n",
      "94400\n",
      "94450\n",
      "Epoch: 4/20...  Training Step: 13800...  Training loss: 1.6454...  0.0736 sec/batch\n",
      "94500\n",
      "94550\n",
      "94600\n",
      "94650\n",
      "94700\n",
      "94750\n",
      "94800\n",
      "94850\n",
      "94900\n",
      "94950\n",
      "95000\n",
      "95050\n",
      "95100\n",
      "95150\n",
      "95200\n",
      "95250\n",
      "95300\n",
      "95350\n",
      "95400\n",
      "95450\n",
      "95500\n",
      "95550\n",
      "95600\n",
      "95650\n",
      "95700\n",
      "95750\n",
      "95800\n",
      "95850\n",
      "95900\n",
      "95950\n",
      "96000\n",
      "96050\n",
      "96100\n",
      "96150\n",
      "96200\n",
      "96250\n",
      "96300\n",
      "96350\n",
      "96400\n",
      "96450\n",
      "96500\n",
      "96550\n",
      "96600\n",
      "96650\n",
      "96700\n",
      "96750\n",
      "96800\n",
      "96850\n",
      "96900\n",
      "96950\n",
      "Epoch: 4/20...  Training Step: 13850...  Training loss: 1.8329...  0.0776 sec/batch\n",
      "97000\n",
      "97050\n",
      "97100\n",
      "97150\n",
      "97200\n",
      "97250\n",
      "97300\n",
      "97350\n",
      "97400\n",
      "97450\n",
      "97500\n",
      "97550\n",
      "97600\n",
      "97650\n",
      "97700\n",
      "97750\n",
      "97800\n",
      "97850\n",
      "97900\n",
      "97950\n",
      "98000\n",
      "98050\n",
      "98100\n",
      "98150\n",
      "98200\n",
      "98250\n",
      "98300\n",
      "98350\n",
      "98400\n",
      "98450\n",
      "98500\n",
      "98550\n",
      "98600\n",
      "98650\n",
      "98700\n",
      "98750\n",
      "98800\n",
      "98850\n",
      "98900\n",
      "98950\n",
      "99000\n",
      "99050\n",
      "99100\n",
      "99150\n",
      "99200\n",
      "99250\n",
      "99300\n",
      "99350\n",
      "99400\n",
      "99450\n",
      "Epoch: 4/20...  Training Step: 13900...  Training loss: 1.6486...  0.0741 sec/batch\n",
      "99500\n",
      "99550\n",
      "99600\n",
      "99650\n",
      "99700\n",
      "99750\n",
      "99800\n",
      "99850\n",
      "99900\n",
      "99950\n",
      "100000\n",
      "100050\n",
      "100100\n",
      "100150\n",
      "100200\n",
      "100250\n",
      "100300\n",
      "100350\n",
      "100400\n",
      "100450\n",
      "100500\n",
      "100550\n",
      "100600\n",
      "100650\n",
      "100700\n",
      "100750\n",
      "100800\n",
      "100850\n",
      "100900\n",
      "100950\n",
      "101000\n",
      "101050\n",
      "101100\n",
      "101150\n",
      "101200\n",
      "101250\n",
      "101300\n",
      "101350\n",
      "101400\n",
      "101450\n",
      "101500\n",
      "101550\n",
      "101600\n",
      "101650\n",
      "101700\n",
      "101750\n",
      "101800\n",
      "101850\n",
      "101900\n",
      "101950\n",
      "Epoch: 4/20...  Training Step: 13950...  Training loss: 1.5840...  0.0813 sec/batch\n",
      "102000\n",
      "102050\n",
      "102100\n",
      "102150\n",
      "102200\n",
      "102250\n",
      "102300\n",
      "102350\n",
      "102400\n",
      "102450\n",
      "102500\n",
      "102550\n",
      "102600\n",
      "102650\n",
      "102700\n",
      "102750\n",
      "102800\n",
      "102850\n",
      "102900\n",
      "102950\n",
      "103000\n",
      "103050\n",
      "103100\n",
      "103150\n",
      "103200\n",
      "103250\n",
      "103300\n",
      "103350\n",
      "103400\n",
      "103450\n",
      "103500\n",
      "103550\n",
      "103600\n",
      "103650\n",
      "103700\n",
      "103750\n",
      "103800\n",
      "103850\n",
      "103900\n",
      "103950\n",
      "104000\n",
      "104050\n",
      "104100\n",
      "104150\n",
      "104200\n",
      "104250\n",
      "104300\n",
      "104350\n",
      "104400\n",
      "104450\n",
      "Epoch: 4/20...  Training Step: 14000...  Training loss: 1.5469...  0.0840 sec/batch\n",
      "104500\n",
      "104550\n",
      "104600\n",
      "104650\n",
      "104700\n",
      "104750\n",
      "104800\n",
      "104850\n",
      "104900\n",
      "104950\n",
      "105000\n",
      "105050\n",
      "105100\n",
      "105150\n",
      "105200\n",
      "105250\n",
      "105300\n",
      "105350\n",
      "105400\n",
      "105450\n",
      "105500\n",
      "105550\n",
      "105600\n",
      "105650\n",
      "105700\n",
      "105750\n",
      "105800\n",
      "105850\n",
      "105900\n",
      "105950\n",
      "106000\n",
      "106050\n",
      "106100\n",
      "106150\n",
      "106200\n",
      "106250\n",
      "106300\n",
      "106350\n",
      "106400\n",
      "106450\n",
      "106500\n",
      "106550\n",
      "106600\n",
      "106650\n",
      "106700\n",
      "106750\n",
      "106800\n",
      "106850\n",
      "106900\n",
      "106950\n",
      "Epoch: 4/20...  Training Step: 14050...  Training loss: 1.5656...  0.0810 sec/batch\n",
      "107000\n",
      "107050\n",
      "107100\n",
      "107150\n",
      "107200\n",
      "107250\n",
      "107300\n",
      "107350\n",
      "107400\n",
      "107450\n",
      "107500\n",
      "107550\n",
      "107600\n",
      "107650\n",
      "107700\n",
      "107750\n",
      "107800\n",
      "107850\n",
      "107900\n",
      "107950\n",
      "108000\n",
      "108050\n",
      "108100\n",
      "108150\n",
      "108200\n",
      "108250\n",
      "108300\n",
      "108350\n",
      "108400\n",
      "108450\n",
      "108500\n",
      "108550\n",
      "108600\n",
      "108650\n",
      "108700\n",
      "108750\n",
      "108800\n",
      "108850\n",
      "108900\n",
      "108950\n",
      "109000\n",
      "109050\n",
      "109100\n",
      "109150\n",
      "109200\n",
      "109250\n",
      "109300\n",
      "109350\n",
      "109400\n",
      "109450\n",
      "Epoch: 4/20...  Training Step: 14100...  Training loss: 1.5791...  0.0776 sec/batch\n",
      "109500\n",
      "109550\n",
      "109600\n",
      "109650\n",
      "109700\n",
      "109750\n",
      "109800\n",
      "109850\n",
      "109900\n",
      "109950\n",
      "110000\n",
      "110050\n",
      "110100\n",
      "110150\n",
      "110200\n",
      "110250\n",
      "110300\n",
      "110350\n",
      "110400\n",
      "110450\n",
      "110500\n",
      "110550\n",
      "110600\n",
      "110650\n",
      "110700\n",
      "110750\n",
      "110800\n",
      "110850\n",
      "110900\n",
      "110950\n",
      "111000\n",
      "111050\n",
      "111100\n",
      "111150\n",
      "111200\n",
      "111250\n",
      "111300\n",
      "111350\n",
      "111400\n",
      "111450\n",
      "111500\n",
      "111550\n",
      "111600\n",
      "111650\n",
      "111700\n",
      "111750\n",
      "111800\n",
      "111850\n",
      "111900\n",
      "111950\n",
      "Epoch: 4/20...  Training Step: 14150...  Training loss: 1.8250...  0.0850 sec/batch\n",
      "112000\n",
      "112050\n",
      "112100\n",
      "112150\n",
      "112200\n",
      "112250\n",
      "112300\n",
      "112350\n",
      "112400\n",
      "112450\n",
      "112500\n",
      "112550\n",
      "112600\n",
      "112650\n",
      "112700\n",
      "112750\n",
      "112800\n",
      "112850\n",
      "112900\n",
      "112950\n",
      "113000\n",
      "113050\n",
      "113100\n",
      "113150\n",
      "113200\n",
      "113250\n",
      "113300\n",
      "113350\n",
      "113400\n",
      "113450\n",
      "113500\n",
      "113550\n",
      "113600\n",
      "113650\n",
      "113700\n",
      "113750\n",
      "113800\n",
      "113850\n",
      "113900\n",
      "113950\n",
      "114000\n",
      "114050\n",
      "114100\n",
      "114150\n",
      "114200\n",
      "114250\n",
      "114300\n",
      "114350\n",
      "114400\n",
      "114450\n",
      "Epoch: 4/20...  Training Step: 14200...  Training loss: 1.5771...  0.0825 sec/batch\n",
      "114500\n",
      "114550\n",
      "114600\n",
      "114650\n",
      "114700\n",
      "114750\n",
      "114800\n",
      "114850\n",
      "114900\n",
      "114950\n",
      "115000\n",
      "115050\n",
      "115100\n",
      "115150\n",
      "115200\n",
      "115250\n",
      "115300\n",
      "115350\n",
      "115400\n",
      "115450\n",
      "115500\n",
      "115550\n",
      "115600\n",
      "115650\n",
      "115700\n",
      "115750\n",
      "115800\n",
      "115850\n",
      "115900\n",
      "115950\n",
      "116000\n",
      "116050\n",
      "116100\n",
      "116150\n",
      "116200\n",
      "116250\n",
      "116300\n",
      "116350\n",
      "116400\n",
      "116450\n",
      "116500\n",
      "116550\n",
      "116600\n",
      "116650\n",
      "116700\n",
      "116750\n",
      "116800\n",
      "116850\n",
      "116900\n",
      "116950\n",
      "Epoch: 4/20...  Training Step: 14250...  Training loss: 1.7650...  0.0736 sec/batch\n",
      "117000\n",
      "117050\n",
      "117100\n",
      "117150\n",
      "117200\n",
      "117250\n",
      "117300\n",
      "117350\n",
      "117400\n",
      "117450\n",
      "117500\n",
      "117550\n",
      "117600\n",
      "117650\n",
      "117700\n",
      "117750\n",
      "117800\n",
      "117850\n",
      "117900\n",
      "117950\n",
      "118000\n",
      "118050\n",
      "118100\n",
      "118150\n",
      "118200\n",
      "118250\n",
      "118300\n",
      "118350\n",
      "118400\n",
      "118450\n",
      "118500\n",
      "118550\n",
      "118600\n",
      "118650\n",
      "118700\n",
      "118750\n",
      "118800\n",
      "118850\n",
      "118900\n",
      "118950\n",
      "119000\n",
      "119050\n",
      "119100\n",
      "119150\n",
      "119200\n",
      "119250\n",
      "119300\n",
      "119350\n",
      "119400\n",
      "119450\n",
      "Epoch: 4/20...  Training Step: 14300...  Training loss: 1.7112...  0.0876 sec/batch\n",
      "119500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119550\n",
      "119600\n",
      "119650\n",
      "119700\n",
      "119750\n",
      "119800\n",
      "119850\n",
      "119900\n",
      "119950\n",
      "120000\n",
      "120050\n",
      "120100\n",
      "120150\n",
      "120200\n",
      "120250\n",
      "120300\n",
      "120350\n",
      "120400\n",
      "120450\n",
      "120500\n",
      "120550\n",
      "120600\n",
      "120650\n",
      "120700\n",
      "120750\n",
      "120800\n",
      "120850\n",
      "120900\n",
      "120950\n",
      "121000\n",
      "121050\n",
      "121100\n",
      "121150\n",
      "121200\n",
      "121250\n",
      "121300\n",
      "121350\n",
      "121400\n",
      "121450\n",
      "121500\n",
      "121550\n",
      "121600\n",
      "121650\n",
      "121700\n",
      "121750\n",
      "121800\n",
      "121850\n",
      "121900\n",
      "121950\n",
      "Epoch: 4/20...  Training Step: 14350...  Training loss: 1.6615...  0.0793 sec/batch\n",
      "122000\n",
      "122050\n",
      "122100\n",
      "122150\n",
      "122200\n",
      "122250\n",
      "122300\n",
      "122350\n",
      "122400\n",
      "122450\n",
      "122500\n",
      "122550\n",
      "122600\n",
      "122650\n",
      "122700\n",
      "122750\n",
      "122800\n",
      "122850\n",
      "122900\n",
      "122950\n",
      "123000\n",
      "123050\n",
      "123100\n",
      "123150\n",
      "123200\n",
      "123250\n",
      "123300\n",
      "123350\n",
      "123400\n",
      "123450\n",
      "123500\n",
      "123550\n",
      "123600\n",
      "123650\n",
      "123700\n",
      "123750\n",
      "123800\n",
      "123850\n",
      "123900\n",
      "123950\n",
      "124000\n",
      "124050\n",
      "124100\n",
      "124150\n",
      "124200\n",
      "124250\n",
      "124300\n",
      "124350\n",
      "124400\n",
      "124450\n",
      "Epoch: 4/20...  Training Step: 14400...  Training loss: 1.5702...  0.1075 sec/batch\n",
      "124500\n",
      "124550\n",
      "124600\n",
      "124650\n",
      "124700\n",
      "124750\n",
      "124800\n",
      "124850\n",
      "124900\n",
      "124950\n",
      "125000\n",
      "125050\n",
      "125100\n",
      "125150\n",
      "125200\n",
      "125250\n",
      "125300\n",
      "125350\n",
      "125400\n",
      "125450\n",
      "125500\n",
      "125550\n",
      "125600\n",
      "125650\n",
      "125700\n",
      "125750\n",
      "125800\n",
      "125850\n",
      "125900\n",
      "125950\n",
      "126000\n",
      "126050\n",
      "126100\n",
      "126150\n",
      "126200\n",
      "126250\n",
      "126300\n",
      "126350\n",
      "126400\n",
      "126450\n",
      "126500\n",
      "126550\n",
      "126600\n",
      "126650\n",
      "126700\n",
      "126750\n",
      "126800\n",
      "126850\n",
      "126900\n",
      "126950\n",
      "Epoch: 4/20...  Training Step: 14450...  Training loss: 1.6870...  0.1076 sec/batch\n",
      "127000\n",
      "127050\n",
      "127100\n",
      "127150\n",
      "127200\n",
      "127250\n",
      "127300\n",
      "127350\n",
      "127400\n",
      "127450\n",
      "127500\n",
      "127550\n",
      "127600\n",
      "127650\n",
      "127700\n",
      "127750\n",
      "127800\n",
      "127850\n",
      "127900\n",
      "127950\n",
      "128000\n",
      "128050\n",
      "128100\n",
      "128150\n",
      "128200\n",
      "128250\n",
      "128300\n",
      "128350\n",
      "128400\n",
      "128450\n",
      "128500\n",
      "128550\n",
      "128600\n",
      "128650\n",
      "128700\n",
      "128750\n",
      "128800\n",
      "128850\n",
      "128900\n",
      "128950\n",
      "129000\n",
      "129050\n",
      "129100\n",
      "129150\n",
      "129200\n",
      "129250\n",
      "129300\n",
      "129350\n",
      "129400\n",
      "129450\n",
      "Epoch: 4/20...  Training Step: 14500...  Training loss: 1.5997...  0.0772 sec/batch\n",
      "129500\n",
      "129550\n",
      "129600\n",
      "129650\n",
      "129700\n",
      "129750\n",
      "129800\n",
      "129850\n",
      "129900\n",
      "129950\n",
      "130000\n",
      "130050\n",
      "130100\n",
      "130150\n",
      "130200\n",
      "130250\n",
      "130300\n",
      "130350\n",
      "130400\n",
      "130450\n",
      "130500\n",
      "130550\n",
      "130600\n",
      "130650\n",
      "130700\n",
      "130750\n",
      "130800\n",
      "130850\n",
      "130900\n",
      "130950\n",
      "131000\n",
      "131050\n",
      "131100\n",
      "131150\n",
      "131200\n",
      "131250\n",
      "131300\n",
      "131350\n",
      "131400\n",
      "131450\n",
      "131500\n",
      "131550\n",
      "131600\n",
      "131650\n",
      "131700\n",
      "131750\n",
      "131800\n",
      "131850\n",
      "131900\n",
      "131950\n",
      "Epoch: 4/20...  Training Step: 14550...  Training loss: 1.4796...  0.0739 sec/batch\n",
      "132000\n",
      "132050\n",
      "132100\n",
      "132150\n",
      "132200\n",
      "132250\n",
      "132300\n",
      "132350\n",
      "132400\n",
      "132450\n",
      "132500\n",
      "132550\n",
      "132600\n",
      "132650\n",
      "132700\n",
      "132750\n",
      "132800\n",
      "132850\n",
      "132900\n",
      "132950\n",
      "133000\n",
      "133050\n",
      "133100\n",
      "133150\n",
      "133200\n",
      "133250\n",
      "133300\n",
      "133350\n",
      "133400\n",
      "133450\n",
      "133500\n",
      "133550\n",
      "133600\n",
      "133650\n",
      "133700\n",
      "133750\n",
      "133800\n",
      "133850\n",
      "133900\n",
      "133950\n",
      "134000\n",
      "134050\n",
      "134100\n",
      "134150\n",
      "134200\n",
      "134250\n",
      "134300\n",
      "134350\n",
      "134400\n",
      "134450\n",
      "Epoch: 4/20...  Training Step: 14600...  Training loss: 1.6761...  0.0970 sec/batch\n",
      "134500\n",
      "134550\n",
      "134600\n",
      "134650\n",
      "134700\n",
      "134750\n",
      "134800\n",
      "134850\n",
      "134900\n",
      "134950\n",
      "135000\n",
      "135050\n",
      "135100\n",
      "135150\n",
      "135200\n",
      "135250\n",
      "135300\n",
      "135350\n",
      "135400\n",
      "135450\n",
      "135500\n",
      "135550\n",
      "135600\n",
      "135650\n",
      "135700\n",
      "135750\n",
      "135800\n",
      "135850\n",
      "135900\n",
      "135950\n",
      "136000\n",
      "136050\n",
      "136100\n",
      "136150\n",
      "136200\n",
      "136250\n",
      "136300\n",
      "136350\n",
      "136400\n",
      "136450\n",
      "136500\n",
      "136550\n",
      "136600\n",
      "136650\n",
      "136700\n",
      "136750\n",
      "136800\n",
      "136850\n",
      "136900\n",
      "136950\n",
      "Epoch: 4/20...  Training Step: 14650...  Training loss: 1.5600...  0.0797 sec/batch\n",
      "137000\n",
      "137050\n",
      "137100\n",
      "137150\n",
      "137200\n",
      "137250\n",
      "137300\n",
      "137350\n",
      "137400\n",
      "137450\n",
      "137500\n",
      "137550\n",
      "137600\n",
      "137650\n",
      "137700\n",
      "137750\n",
      "137800\n",
      "137850\n",
      "137900\n",
      "137950\n",
      "138000\n",
      "138050\n",
      "138100\n",
      "138150\n",
      "138200\n",
      "138250\n",
      "138300\n",
      "138350\n",
      "138400\n",
      "138450\n",
      "138500\n",
      "138550\n",
      "138600\n",
      "138650\n",
      "138700\n",
      "138750\n",
      "138800\n",
      "138850\n",
      "138900\n",
      "138950\n",
      "139000\n",
      "139050\n",
      "139100\n",
      "139150\n",
      "139200\n",
      "139250\n",
      "139300\n",
      "139350\n",
      "139400\n",
      "139450\n",
      "Epoch: 4/20...  Training Step: 14700...  Training loss: 1.5501...  0.0843 sec/batch\n",
      "139500\n",
      "139550\n",
      "139600\n",
      "139650\n",
      "139700\n",
      "139750\n",
      "139800\n",
      "139850\n",
      "139900\n",
      "139950\n",
      "140000\n",
      "140050\n",
      "140100\n",
      "140150\n",
      "140200\n",
      "140250\n",
      "140300\n",
      "140350\n",
      "140400\n",
      "140450\n",
      "140500\n",
      "140550\n",
      "140600\n",
      "140650\n",
      "140700\n",
      "140750\n",
      "140800\n",
      "140850\n",
      "140900\n",
      "140950\n",
      "141000\n",
      "141050\n",
      "141100\n",
      "141150\n",
      "141200\n",
      "141250\n",
      "141300\n",
      "141350\n",
      "141400\n",
      "141450\n",
      "141500\n",
      "141550\n",
      "141600\n",
      "141650\n",
      "141700\n",
      "141750\n",
      "141800\n",
      "141850\n",
      "141900\n",
      "141950\n",
      "Epoch: 4/20...  Training Step: 14750...  Training loss: 1.5673...  0.0928 sec/batch\n",
      "142000\n",
      "142050\n",
      "142100\n",
      "142150\n",
      "142200\n",
      "142250\n",
      "142300\n",
      "142350\n",
      "142400\n",
      "142450\n",
      "142500\n",
      "142550\n",
      "142600\n",
      "142650\n",
      "142700\n",
      "142750\n",
      "142800\n",
      "142850\n",
      "142900\n",
      "142950\n",
      "143000\n",
      "143050\n",
      "143100\n",
      "143150\n",
      "143200\n",
      "143250\n",
      "143300\n",
      "143350\n",
      "143400\n",
      "143450\n",
      "143500\n",
      "143550\n",
      "143600\n",
      "143650\n",
      "143700\n",
      "143750\n",
      "143800\n",
      "143850\n",
      "143900\n",
      "143950\n",
      "144000\n",
      "144050\n",
      "144100\n",
      "144150\n",
      "144200\n",
      "144250\n",
      "144300\n",
      "144350\n",
      "144400\n",
      "144450\n",
      "Epoch: 4/20...  Training Step: 14800...  Training loss: 1.4799...  0.0791 sec/batch\n",
      "144500\n",
      "144550\n",
      "144600\n",
      "144650\n",
      "144700\n",
      "144750\n",
      "144800\n",
      "144850\n",
      "144900\n",
      "144950\n",
      "145000\n",
      "145050\n",
      "145100\n",
      "145150\n",
      "145200\n",
      "145250\n",
      "145300\n",
      "145350\n",
      "145400\n",
      "145450\n",
      "145500\n",
      "145550\n",
      "145600\n",
      "145650\n",
      "145700\n",
      "145750\n",
      "145800\n",
      "145850\n",
      "145900\n",
      "145950\n",
      "146000\n",
      "146050\n",
      "146100\n",
      "146150\n",
      "146200\n",
      "146250\n",
      "146300\n",
      "146350\n",
      "146400\n",
      "146450\n",
      "146500\n",
      "146550\n",
      "146600\n",
      "146650\n",
      "146700\n",
      "146750\n",
      "146800\n",
      "146850\n",
      "146900\n",
      "146950\n",
      "Epoch: 4/20...  Training Step: 14850...  Training loss: 1.6290...  0.0789 sec/batch\n",
      "147000\n",
      "147050\n",
      "147100\n",
      "147150\n",
      "147200\n",
      "147250\n",
      "147300\n",
      "147350\n",
      "147400\n",
      "147450\n",
      "147500\n",
      "147550\n",
      "147600\n",
      "147650\n",
      "147700\n",
      "147750\n",
      "147800\n",
      "147850\n",
      "147900\n",
      "147950\n",
      "148000\n",
      "148050\n",
      "148100\n",
      "148150\n",
      "148200\n",
      "148250\n",
      "148300\n",
      "148350\n",
      "148400\n",
      "148450\n",
      "148500\n",
      "148550\n",
      "148600\n",
      "148650\n",
      "148700\n",
      "148750\n",
      "148800\n",
      "148850\n",
      "148900\n",
      "148950\n",
      "149000\n",
      "149050\n",
      "149100\n",
      "149150\n",
      "149200\n",
      "149250\n",
      "149300\n",
      "149350\n",
      "149400\n",
      "149450\n",
      "Epoch: 4/20...  Training Step: 14900...  Training loss: 1.6245...  0.0802 sec/batch\n",
      "149500\n",
      "149550\n",
      "149600\n",
      "149650\n",
      "149700\n",
      "149750\n",
      "149800\n",
      "149850\n",
      "149900\n",
      "149950\n",
      "150000\n",
      "150050\n",
      "150100\n",
      "150150\n",
      "150200\n",
      "150250\n",
      "150300\n",
      "150350\n",
      "150400\n",
      "150450\n",
      "150500\n",
      "150550\n",
      "150600\n",
      "150650\n",
      "150700\n",
      "150750\n",
      "150800\n",
      "150850\n",
      "150900\n",
      "150950\n",
      "151000\n",
      "151050\n",
      "151100\n",
      "151150\n",
      "151200\n",
      "151250\n",
      "151300\n",
      "151350\n",
      "151400\n",
      "151450\n",
      "151500\n",
      "151550\n",
      "151600\n",
      "151650\n",
      "151700\n",
      "151750\n",
      "151800\n",
      "151850\n",
      "151900\n",
      "151950\n",
      "Epoch: 4/20...  Training Step: 14950...  Training loss: 1.7255...  0.0896 sec/batch\n",
      "152000\n",
      "152050\n",
      "152100\n",
      "152150\n",
      "152200\n",
      "152250\n",
      "152300\n",
      "152350\n",
      "152400\n",
      "152450\n",
      "152500\n",
      "152550\n",
      "152600\n",
      "152650\n",
      "152700\n",
      "152750\n",
      "152800\n",
      "152850\n",
      "152900\n",
      "152950\n",
      "153000\n",
      "153050\n",
      "153100\n",
      "153150\n",
      "153200\n",
      "153250\n",
      "153300\n",
      "153350\n",
      "153400\n",
      "153450\n",
      "153500\n",
      "153550\n",
      "153600\n",
      "153650\n",
      "153700\n",
      "153750\n",
      "153800\n",
      "153850\n",
      "153900\n",
      "153950\n",
      "154000\n",
      "154050\n",
      "154100\n",
      "154150\n",
      "154200\n",
      "154250\n",
      "154300\n",
      "154350\n",
      "154400\n",
      "154450\n",
      "Epoch: 4/20...  Training Step: 15000...  Training loss: 1.5058...  0.0776 sec/batch\n",
      "154500\n",
      "154550\n",
      "154600\n",
      "154650\n",
      "154700\n",
      "154750\n",
      "154800\n",
      "154850\n",
      "154900\n",
      "154950\n",
      "155000\n",
      "155050\n",
      "155100\n",
      "155150\n",
      "155200\n",
      "155250\n",
      "155300\n",
      "155350\n",
      "155400\n",
      "155450\n",
      "155500\n",
      "155550\n",
      "155600\n",
      "155650\n",
      "155700\n",
      "155750\n",
      "155800\n",
      "155850\n",
      "155900\n",
      "155950\n",
      "156000\n",
      "156050\n",
      "156100\n",
      "156150\n",
      "156200\n",
      "156250\n",
      "156300\n",
      "156350\n",
      "156400\n",
      "156450\n",
      "156500\n",
      "156550\n",
      "156600\n",
      "156650\n",
      "156700\n",
      "156750\n",
      "156800\n",
      "156850\n",
      "156900\n",
      "156950\n",
      "Epoch: 4/20...  Training Step: 15050...  Training loss: 1.7669...  0.0963 sec/batch\n",
      "157000\n",
      "157050\n",
      "157100\n",
      "157150\n",
      "157200\n",
      "157250\n",
      "157300\n",
      "157350\n",
      "157400\n",
      "157450\n",
      "157500\n",
      "157550\n",
      "157600\n",
      "157650\n",
      "157700\n",
      "157750\n",
      "157800\n",
      "157850\n",
      "157900\n",
      "157950\n",
      "158000\n",
      "158050\n",
      "158100\n",
      "158150\n",
      "158200\n",
      "158250\n",
      "158300\n",
      "158350\n",
      "158400\n",
      "158450\n",
      "158500\n",
      "158550\n",
      "158600\n",
      "158650\n",
      "158700\n",
      "158750\n",
      "158800\n",
      "158850\n",
      "158900\n",
      "158950\n",
      "159000\n",
      "159050\n",
      "159100\n",
      "159150\n",
      "159200\n",
      "159250\n",
      "159300\n",
      "159350\n",
      "159400\n",
      "159450\n",
      "Epoch: 4/20...  Training Step: 15100...  Training loss: 1.6108...  0.1134 sec/batch\n",
      "159500\n",
      "159550\n",
      "159600\n",
      "159650\n",
      "159700\n",
      "159750\n",
      "159800\n",
      "159850\n",
      "159900\n",
      "159950\n",
      "160000\n",
      "160050\n",
      "160100\n",
      "160150\n",
      "160200\n",
      "160250\n",
      "160300\n",
      "160350\n",
      "160400\n",
      "160450\n",
      "160500\n",
      "160550\n",
      "160600\n",
      "160650\n",
      "160700\n",
      "160750\n",
      "160800\n",
      "160850\n",
      "160900\n",
      "160950\n",
      "161000\n",
      "161050\n",
      "161100\n",
      "161150\n",
      "161200\n",
      "161250\n",
      "161300\n",
      "161350\n",
      "161400\n",
      "161450\n",
      "161500\n",
      "161550\n",
      "161600\n",
      "161650\n",
      "161700\n",
      "161750\n",
      "161800\n",
      "161850\n",
      "161900\n",
      "161950\n",
      "Epoch: 4/20...  Training Step: 15150...  Training loss: 1.5465...  0.0823 sec/batch\n",
      "162000\n",
      "162050\n",
      "162100\n",
      "162150\n",
      "162200\n",
      "162250\n",
      "162300\n",
      "162350\n",
      "162400\n",
      "162450\n",
      "162500\n",
      "162550\n",
      "162600\n",
      "162650\n",
      "162700\n",
      "162750\n",
      "162800\n",
      "162850\n",
      "162900\n",
      "162950\n",
      "163000\n",
      "163050\n",
      "163100\n",
      "163150\n",
      "163200\n",
      "163250\n",
      "163300\n",
      "163350\n",
      "163400\n",
      "163450\n",
      "163500\n",
      "163550\n",
      "163600\n",
      "163650\n",
      "163700\n",
      "163750\n",
      "163800\n",
      "163850\n",
      "163900\n",
      "163950\n",
      "164000\n",
      "164050\n",
      "164100\n",
      "164150\n",
      "164200\n",
      "164250\n",
      "164300\n",
      "164350\n",
      "164400\n",
      "164450\n",
      "Epoch: 4/20...  Training Step: 15200...  Training loss: 1.7258...  0.0906 sec/batch\n",
      "164500\n",
      "164550\n",
      "164600\n",
      "164650\n",
      "164700\n",
      "164750\n",
      "164800\n",
      "164850\n",
      "164900\n",
      "164950\n",
      "165000\n",
      "165050\n",
      "165100\n",
      "165150\n",
      "165200\n",
      "165250\n",
      "165300\n",
      "165350\n",
      "165400\n",
      "165450\n",
      "165500\n",
      "165550\n",
      "165600\n",
      "165650\n",
      "165700\n",
      "165750\n",
      "165800\n",
      "165850\n",
      "165900\n",
      "165950\n",
      "166000\n",
      "166050\n",
      "166100\n",
      "166150\n",
      "166200\n",
      "166250\n",
      "166300\n",
      "166350\n",
      "166400\n",
      "166450\n",
      "166500\n",
      "166550\n",
      "166600\n",
      "166650\n",
      "166700\n",
      "166750\n",
      "166800\n",
      "166850\n",
      "166900\n",
      "166950\n",
      "Epoch: 4/20...  Training Step: 15250...  Training loss: 1.5964...  0.0754 sec/batch\n",
      "167000\n",
      "167050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "167100\n",
      "167150\n",
      "167200\n",
      "167250\n",
      "167300\n",
      "167350\n",
      "167400\n",
      "167450\n",
      "167500\n",
      "167550\n",
      "167600\n",
      "167650\n",
      "167700\n",
      "167750\n",
      "167800\n",
      "167850\n",
      "167900\n",
      "167950\n",
      "168000\n",
      "168050\n",
      "168100\n",
      "168150\n",
      "168200\n",
      "168250\n",
      "168300\n",
      "168350\n",
      "168400\n",
      "168450\n",
      "168500\n",
      "168550\n",
      "168600\n",
      "168650\n",
      "168700\n",
      "168750\n",
      "168800\n",
      "168850\n",
      "168900\n",
      "168950\n",
      "169000\n",
      "169050\n",
      "169100\n",
      "169150\n",
      "169200\n",
      "169250\n",
      "169300\n",
      "169350\n",
      "169400\n",
      "169450\n",
      "Epoch: 4/20...  Training Step: 15300...  Training loss: 1.6100...  0.0993 sec/batch\n",
      "169500\n",
      "169550\n",
      "169600\n",
      "169650\n",
      "169700\n",
      "169750\n",
      "169800\n",
      "169850\n",
      "169900\n",
      "169950\n",
      "170000\n",
      "170050\n",
      "170100\n",
      "170150\n",
      "170200\n",
      "170250\n",
      "170300\n",
      "170350\n",
      "170400\n",
      "170450\n",
      "170500\n",
      "170550\n",
      "170600\n",
      "170650\n",
      "170700\n",
      "170750\n",
      "170800\n",
      "170850\n",
      "170900\n",
      "170950\n",
      "171000\n",
      "171050\n",
      "171100\n",
      "171150\n",
      "171200\n",
      "171250\n",
      "171300\n",
      "171350\n",
      "171400\n",
      "171450\n",
      "171500\n",
      "171550\n",
      "171600\n",
      "171650\n",
      "171700\n",
      "171750\n",
      "171800\n",
      "171850\n",
      "171900\n",
      "171950\n",
      "Epoch: 4/20...  Training Step: 15350...  Training loss: 1.7263...  0.0805 sec/batch\n",
      "172000\n",
      "172050\n",
      "172100\n",
      "172150\n",
      "172200\n",
      "172250\n",
      "172300\n",
      "172350\n",
      "172400\n",
      "172450\n",
      "172500\n",
      "172550\n",
      "172600\n",
      "172650\n",
      "172700\n",
      "172750\n",
      "172800\n",
      "172850\n",
      "172900\n",
      "172950\n",
      "173000\n",
      "173050\n",
      "173100\n",
      "173150\n",
      "173200\n",
      "173250\n",
      "173300\n",
      "173350\n",
      "173400\n",
      "173450\n",
      "173500\n",
      "173550\n",
      "173600\n",
      "173650\n",
      "173700\n",
      "173750\n",
      "173800\n",
      "173850\n",
      "173900\n",
      "173950\n",
      "174000\n",
      "174050\n",
      "174100\n",
      "174150\n",
      "174200\n",
      "174250\n",
      "174300\n",
      "174350\n",
      "174400\n",
      "174450\n",
      "Epoch: 4/20...  Training Step: 15400...  Training loss: 1.7497...  0.0895 sec/batch\n",
      "174500\n",
      "174550\n",
      "174600\n",
      "174650\n",
      "174700\n",
      "174750\n",
      "174800\n",
      "174850\n",
      "174900\n",
      "174950\n",
      "175000\n",
      "175050\n",
      "175100\n",
      "175150\n",
      "175200\n",
      "175250\n",
      "175300\n",
      "175350\n",
      "175400\n",
      "175450\n",
      "175500\n",
      "175550\n",
      "175600\n",
      "175650\n",
      "175700\n",
      "175750\n",
      "175800\n",
      "175850\n",
      "175900\n",
      "175950\n",
      "176000\n",
      "176050\n",
      "176100\n",
      "176150\n",
      "176200\n",
      "176250\n",
      "176300\n",
      "176350\n",
      "176400\n",
      "176450\n",
      "176500\n",
      "176550\n",
      "176600\n",
      "176650\n",
      "176700\n",
      "176750\n",
      "176800\n",
      "176850\n",
      "176900\n",
      "176950\n",
      "Epoch: 4/20...  Training Step: 15450...  Training loss: 1.5347...  0.0789 sec/batch\n",
      "177000\n",
      "177050\n",
      "177100\n",
      "177150\n",
      "177200\n",
      "177250\n",
      "177300\n",
      "177350\n",
      "177400\n",
      "177450\n",
      "177500\n",
      "177550\n",
      "177600\n",
      "177650\n",
      "177700\n",
      "177750\n",
      "177800\n",
      "177850\n",
      "177900\n",
      "177950\n",
      "178000\n",
      "178050\n",
      "178100\n",
      "178150\n",
      "178200\n",
      "178250\n",
      "178300\n",
      "178350\n",
      "178400\n",
      "178450\n",
      "178500\n",
      "178550\n",
      "178600\n",
      "178650\n",
      "178700\n",
      "178750\n",
      "178800\n",
      "178850\n",
      "178900\n",
      "178950\n",
      "179000\n",
      "179050\n",
      "179100\n",
      "179150\n",
      "179200\n",
      "179250\n",
      "179300\n",
      "179350\n",
      "179400\n",
      "179450\n",
      "Epoch: 4/20...  Training Step: 15500...  Training loss: 1.5130...  0.0784 sec/batch\n",
      "179500\n",
      "179550\n",
      "179600\n",
      "179650\n",
      "179700\n",
      "179750\n",
      "179800\n",
      "179850\n",
      "179900\n",
      "179950\n",
      "180000\n",
      "180050\n",
      "180100\n",
      "180150\n",
      "180200\n",
      "180250\n",
      "180300\n",
      "180350\n",
      "180400\n",
      "180450\n",
      "180500\n",
      "180550\n",
      "180600\n",
      "180650\n",
      "180700\n",
      "180750\n",
      "180800\n",
      "180850\n",
      "180900\n",
      "180950\n",
      "181000\n",
      "181050\n",
      "181100\n",
      "181150\n",
      "181200\n",
      "181250\n",
      "181300\n",
      "181350\n",
      "181400\n",
      "181450\n",
      "181500\n",
      "181550\n",
      "181600\n",
      "181650\n",
      "181700\n",
      "181750\n",
      "181800\n",
      "181850\n",
      "181900\n",
      "181950\n",
      "Epoch: 4/20...  Training Step: 15550...  Training loss: 1.5234...  0.0797 sec/batch\n",
      "182000\n",
      "182050\n",
      "182100\n",
      "182150\n",
      "182200\n",
      "182250\n",
      "182300\n",
      "182350\n",
      "182400\n",
      "182450\n",
      "182500\n",
      "182550\n",
      "182600\n",
      "182650\n",
      "182700\n",
      "182750\n",
      "182800\n",
      "182850\n",
      "182900\n",
      "182950\n",
      "183000\n",
      "183050\n",
      "183100\n",
      "183150\n",
      "183200\n",
      "183250\n",
      "183300\n",
      "183350\n",
      "183400\n",
      "183450\n",
      "183500\n",
      "183550\n",
      "183600\n",
      "183650\n",
      "183700\n",
      "183750\n",
      "183800\n",
      "183850\n",
      "183900\n",
      "183950\n",
      "184000\n",
      "184050\n",
      "184100\n",
      "184150\n",
      "184200\n",
      "184250\n",
      "184300\n",
      "184350\n",
      "184400\n",
      "184450\n",
      "Epoch: 4/20...  Training Step: 15600...  Training loss: 1.5754...  0.0737 sec/batch\n",
      "184500\n",
      "184550\n",
      "184600\n",
      "184650\n",
      "184700\n",
      "184750\n",
      "184800\n",
      "184850\n",
      "184900\n",
      "184950\n",
      "185000\n",
      "185050\n",
      "185100\n",
      "185150\n",
      "185200\n",
      "185250\n",
      "185300\n",
      "185350\n",
      "185400\n",
      "185450\n",
      "185500\n",
      "185550\n",
      "185600\n",
      "185650\n",
      "185700\n",
      "185750\n",
      "185800\n",
      "185850\n",
      "185900\n",
      "185950\n",
      "186000\n",
      "186050\n",
      "186100\n",
      "186150\n",
      "186200\n",
      "186250\n",
      "186300\n",
      "186350\n",
      "186400\n",
      "186450\n",
      "186500\n",
      "186550\n",
      "186600\n",
      "186650\n",
      "186700\n",
      "186750\n",
      "186800\n",
      "186850\n",
      "186900\n",
      "186950\n",
      "Epoch: 4/20...  Training Step: 15650...  Training loss: 1.6139...  0.0814 sec/batch\n",
      "187000\n",
      "187050\n",
      "187100\n",
      "187150\n",
      "187200\n",
      "187250\n",
      "187300\n",
      "187350\n",
      "187400\n",
      "187450\n",
      "187500\n",
      "187550\n",
      "187600\n",
      "187650\n",
      "187700\n",
      "187750\n",
      "187800\n",
      "187850\n",
      "187900\n",
      "187950\n",
      "188000\n",
      "188050\n",
      "188100\n",
      "188150\n",
      "188200\n",
      "188250\n",
      "188300\n",
      "188350\n",
      "188400\n",
      "188450\n",
      "188500\n",
      "188550\n",
      "188600\n",
      "188650\n",
      "188700\n",
      "188750\n",
      "188800\n",
      "188850\n",
      "188900\n",
      "188950\n",
      "189000\n",
      "189050\n",
      "189100\n",
      "189150\n",
      "189200\n",
      "189250\n",
      "189300\n",
      "189350\n",
      "189400\n",
      "189450\n",
      "Epoch: 4/20...  Training Step: 15700...  Training loss: 1.5518...  0.0792 sec/batch\n",
      "189500\n",
      "189550\n",
      "189600\n",
      "189650\n",
      "189700\n",
      "189750\n",
      "189800\n",
      "189850\n",
      "189900\n",
      "189950\n",
      "190000\n",
      "190050\n",
      "190100\n",
      "190150\n",
      "190200\n",
      "190250\n",
      "190300\n",
      "190350\n",
      "190400\n",
      "190450\n",
      "190500\n",
      "190550\n",
      "190600\n",
      "190650\n",
      "190700\n",
      "190750\n",
      "190800\n",
      "190850\n",
      "190900\n",
      "190950\n",
      "191000\n",
      "191050\n",
      "191100\n",
      "191150\n",
      "191200\n",
      "191250\n",
      "191300\n",
      "191350\n",
      "191400\n",
      "191450\n",
      "191500\n",
      "191550\n",
      "191600\n",
      "191650\n",
      "191700\n",
      "191750\n",
      "191800\n",
      "191850\n",
      "191900\n",
      "191950\n",
      "Epoch: 4/20...  Training Step: 15750...  Training loss: 1.7720...  0.0833 sec/batch\n",
      "192000\n",
      "192050\n",
      "192100\n",
      "192150\n",
      "192200\n",
      "192250\n",
      "192300\n",
      "192350\n",
      "192400\n",
      "192450\n",
      "192500\n",
      "192550\n",
      "192600\n",
      "192650\n",
      "192700\n",
      "192750\n",
      "192800\n",
      "192850\n",
      "192900\n",
      "192950\n",
      "193000\n",
      "193050\n",
      "193100\n",
      "193150\n",
      "193200\n",
      "193250\n",
      "193300\n",
      "193350\n",
      "193400\n",
      "193450\n",
      "193500\n",
      "193550\n",
      "193600\n",
      "193650\n",
      "193700\n",
      "193750\n",
      "193800\n",
      "193850\n",
      "193900\n",
      "193950\n",
      "194000\n",
      "194050\n",
      "194100\n",
      "194150\n",
      "194200\n",
      "194250\n",
      "194300\n",
      "194350\n",
      "194400\n",
      "194450\n",
      "Epoch: 4/20...  Training Step: 15800...  Training loss: 1.6680...  0.0780 sec/batch\n",
      "194500\n",
      "194550\n",
      "194600\n",
      "194650\n",
      "194700\n",
      "194750\n",
      "194800\n",
      "194850\n",
      "194900\n",
      "194950\n",
      "195000\n",
      "195050\n",
      "195100\n",
      "195150\n",
      "195200\n",
      "195250\n",
      "195300\n",
      "195350\n",
      "195400\n",
      "195450\n",
      "195500\n",
      "195550\n",
      "195600\n",
      "195650\n",
      "195700\n",
      "195750\n",
      "195800\n",
      "195850\n",
      "195900\n",
      "195950\n",
      "196000\n",
      "196050\n",
      "196100\n",
      "196150\n",
      "196200\n",
      "196250\n",
      "196300\n",
      "196350\n",
      "196400\n",
      "196450\n",
      "196500\n",
      "196550\n",
      "196600\n",
      "196650\n",
      "196700\n",
      "196750\n",
      "196800\n",
      "196850\n",
      "196900\n",
      "196950\n",
      "Epoch: 4/20...  Training Step: 15850...  Training loss: 1.6473...  0.0842 sec/batch\n",
      "197000\n",
      "197050\n",
      "197100\n",
      "197150\n",
      "197200\n",
      "197250\n",
      "197300\n",
      "197350\n",
      "197400\n",
      "197450\n",
      "197500\n",
      "197550\n",
      "197600\n",
      "197650\n",
      "197700\n",
      "197750\n",
      "197800\n",
      "197850\n",
      "197900\n",
      "197950\n",
      "198000\n",
      "198050\n",
      "198100\n",
      "198150\n",
      "198200\n",
      "198250\n",
      "198300\n",
      "198350\n",
      "198400\n",
      "198450\n",
      "3970\n",
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "800\n",
      "850\n",
      "900\n",
      "950\n",
      "Epoch: 5/20...  Training Step: 15900...  Training loss: 1.6964...  0.0817 sec/batch\n",
      "1000\n",
      "1050\n",
      "1100\n",
      "1150\n",
      "1200\n",
      "1250\n",
      "1300\n",
      "1350\n",
      "1400\n",
      "1450\n",
      "1500\n",
      "1550\n",
      "1600\n",
      "1650\n",
      "1700\n",
      "1750\n",
      "1800\n",
      "1850\n",
      "1900\n",
      "1950\n",
      "2000\n",
      "2050\n",
      "2100\n",
      "2150\n",
      "2200\n",
      "2250\n",
      "2300\n",
      "2350\n",
      "2400\n",
      "2450\n",
      "2500\n",
      "2550\n",
      "2600\n",
      "2650\n",
      "2700\n",
      "2750\n",
      "2800\n",
      "2850\n",
      "2900\n",
      "2950\n",
      "3000\n",
      "3050\n",
      "3100\n",
      "3150\n",
      "3200\n",
      "3250\n",
      "3300\n",
      "3350\n",
      "3400\n",
      "3450\n",
      "Epoch: 5/20...  Training Step: 15950...  Training loss: 1.5736...  0.0737 sec/batch\n",
      "3500\n",
      "3550\n",
      "3600\n",
      "3650\n",
      "3700\n",
      "3750\n",
      "3800\n",
      "3850\n",
      "3900\n",
      "3950\n",
      "4000\n",
      "4050\n",
      "4100\n",
      "4150\n",
      "4200\n",
      "4250\n",
      "4300\n",
      "4350\n",
      "4400\n",
      "4450\n",
      "4500\n",
      "4550\n",
      "4600\n",
      "4650\n",
      "4700\n",
      "4750\n",
      "4800\n",
      "4850\n",
      "4900\n",
      "4950\n",
      "5000\n",
      "5050\n",
      "5100\n",
      "5150\n",
      "5200\n",
      "5250\n",
      "5300\n",
      "5350\n",
      "5400\n",
      "5450\n",
      "5500\n",
      "5550\n",
      "5600\n",
      "5650\n",
      "5700\n",
      "5750\n",
      "5800\n",
      "5850\n",
      "5900\n",
      "5950\n",
      "Epoch: 5/20...  Training Step: 16000...  Training loss: 1.5353...  0.0802 sec/batch\n",
      "6000\n",
      "6050\n",
      "6100\n",
      "6150\n",
      "6200\n",
      "6250\n",
      "6300\n",
      "6350\n",
      "6400\n",
      "6450\n",
      "6500\n",
      "6550\n",
      "6600\n",
      "6650\n",
      "6700\n",
      "6750\n",
      "6800\n",
      "6850\n",
      "6900\n",
      "6950\n",
      "7000\n",
      "7050\n",
      "7100\n",
      "7150\n",
      "7200\n",
      "7250\n",
      "7300\n",
      "7350\n",
      "7400\n",
      "7450\n",
      "7500\n",
      "7550\n",
      "7600\n",
      "7650\n",
      "7700\n",
      "7750\n",
      "7800\n",
      "7850\n",
      "7900\n",
      "7950\n",
      "8000\n",
      "8050\n",
      "8100\n",
      "8150\n",
      "8200\n",
      "8250\n",
      "8300\n",
      "8350\n",
      "8400\n",
      "8450\n",
      "Epoch: 5/20...  Training Step: 16050...  Training loss: 1.7353...  0.1016 sec/batch\n",
      "8500\n",
      "8550\n",
      "8600\n",
      "8650\n",
      "8700\n",
      "8750\n",
      "8800\n",
      "8850\n",
      "8900\n",
      "8950\n",
      "9000\n",
      "9050\n",
      "9100\n",
      "9150\n",
      "9200\n",
      "9250\n",
      "9300\n",
      "9350\n",
      "9400\n",
      "9450\n",
      "9500\n",
      "9550\n",
      "9600\n",
      "9650\n",
      "9700\n",
      "9750\n",
      "9800\n",
      "9850\n",
      "9900\n",
      "9950\n",
      "10000\n",
      "10050\n",
      "10100\n",
      "10150\n",
      "10200\n",
      "10250\n",
      "10300\n",
      "10350\n",
      "10400\n",
      "10450\n",
      "10500\n",
      "10550\n",
      "10600\n",
      "10650\n",
      "10700\n",
      "10750\n",
      "10800\n",
      "10850\n",
      "10900\n",
      "10950\n",
      "Epoch: 5/20...  Training Step: 16100...  Training loss: 1.6187...  0.1026 sec/batch\n",
      "11000\n",
      "11050\n",
      "11100\n",
      "11150\n",
      "11200\n",
      "11250\n",
      "11300\n",
      "11350\n",
      "11400\n",
      "11450\n",
      "11500\n",
      "11550\n",
      "11600\n",
      "11650\n",
      "11700\n",
      "11750\n",
      "11800\n",
      "11850\n",
      "11900\n",
      "11950\n",
      "12000\n",
      "12050\n",
      "12100\n",
      "12150\n",
      "12200\n",
      "12250\n",
      "12300\n",
      "12350\n",
      "12400\n",
      "12450\n",
      "12500\n",
      "12550\n",
      "12600\n",
      "12650\n",
      "12700\n",
      "12750\n",
      "12800\n",
      "12850\n",
      "12900\n",
      "12950\n",
      "13000\n",
      "13050\n",
      "13100\n",
      "13150\n",
      "13200\n",
      "13250\n",
      "13300\n",
      "13350\n",
      "13400\n",
      "13450\n",
      "Epoch: 5/20...  Training Step: 16150...  Training loss: 1.6272...  0.0868 sec/batch\n",
      "13500\n",
      "13550\n",
      "13600\n",
      "13650\n",
      "13700\n",
      "13750\n",
      "13800\n",
      "13850\n",
      "13900\n",
      "13950\n",
      "14000\n",
      "14050\n",
      "14100\n",
      "14150\n",
      "14200\n",
      "14250\n",
      "14300\n",
      "14350\n",
      "14400\n",
      "14450\n",
      "14500\n",
      "14550\n",
      "14600\n",
      "14650\n",
      "14700\n",
      "14750\n",
      "14800\n",
      "14850\n",
      "14900\n",
      "14950\n",
      "15000\n",
      "15050\n",
      "15100\n",
      "15150\n",
      "15200\n",
      "15250\n",
      "15300\n",
      "15350\n",
      "15400\n",
      "15450\n",
      "15500\n",
      "15550\n",
      "15600\n",
      "15650\n",
      "15700\n",
      "15750\n",
      "15800\n",
      "15850\n",
      "15900\n",
      "15950\n",
      "Epoch: 5/20...  Training Step: 16200...  Training loss: 1.6368...  0.1687 sec/batch\n",
      "16000\n",
      "16050\n",
      "16100\n",
      "16150\n",
      "16200\n",
      "16250\n",
      "16300\n",
      "16350\n",
      "16400\n",
      "16450\n",
      "16500\n",
      "16550\n",
      "16600\n",
      "16650\n",
      "16700\n",
      "16750\n",
      "16800\n",
      "16850\n",
      "16900\n",
      "16950\n",
      "17000\n",
      "17050\n",
      "17100\n",
      "17150\n",
      "17200\n",
      "17250\n",
      "17300\n",
      "17350\n",
      "17400\n",
      "17450\n",
      "17500\n",
      "17550\n",
      "17600\n",
      "17650\n",
      "17700\n",
      "17750\n",
      "17800\n",
      "17850\n",
      "17900\n",
      "17950\n",
      "18000\n",
      "18050\n",
      "18100\n",
      "18150\n",
      "18200\n",
      "18250\n",
      "18300\n",
      "18350\n",
      "18400\n",
      "18450\n",
      "Epoch: 5/20...  Training Step: 16250...  Training loss: 1.6720...  0.0929 sec/batch\n",
      "18500\n",
      "18550\n",
      "18600\n",
      "18650\n",
      "18700\n",
      "18750\n",
      "18800\n",
      "18850\n",
      "18900\n",
      "18950\n",
      "19000\n",
      "19050\n",
      "19100\n",
      "19150\n",
      "19200\n",
      "19250\n",
      "19300\n",
      "19350\n",
      "19400\n",
      "19450\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19500\n",
      "19550\n",
      "19600\n",
      "19650\n",
      "19700\n",
      "19750\n",
      "19800\n",
      "19850\n",
      "19900\n",
      "19950\n",
      "20000\n",
      "20050\n",
      "20100\n",
      "20150\n",
      "20200\n",
      "20250\n",
      "20300\n",
      "20350\n",
      "20400\n",
      "20450\n",
      "20500\n",
      "20550\n",
      "20600\n",
      "20650\n",
      "20700\n",
      "20750\n",
      "20800\n",
      "20850\n",
      "20900\n",
      "20950\n",
      "Epoch: 5/20...  Training Step: 16300...  Training loss: 1.6065...  0.0878 sec/batch\n",
      "21000\n",
      "21050\n",
      "21100\n",
      "21150\n",
      "21200\n",
      "21250\n",
      "21300\n",
      "21350\n",
      "21400\n",
      "21450\n",
      "21500\n",
      "21550\n",
      "21600\n",
      "21650\n",
      "21700\n",
      "21750\n",
      "21800\n",
      "21850\n",
      "21900\n",
      "21950\n",
      "22000\n",
      "22050\n",
      "22100\n",
      "22150\n",
      "22200\n",
      "22250\n",
      "22300\n",
      "22350\n",
      "22400\n",
      "22450\n",
      "22500\n",
      "22550\n",
      "22600\n",
      "22650\n",
      "22700\n",
      "22750\n",
      "22800\n",
      "22850\n",
      "22900\n",
      "22950\n",
      "23000\n",
      "23050\n",
      "23100\n",
      "23150\n",
      "23200\n",
      "23250\n",
      "23300\n",
      "23350\n",
      "23400\n",
      "23450\n",
      "Epoch: 5/20...  Training Step: 16350...  Training loss: 1.6408...  0.0843 sec/batch\n",
      "23500\n",
      "23550\n",
      "23600\n",
      "23650\n",
      "23700\n",
      "23750\n",
      "23800\n",
      "23850\n",
      "23900\n",
      "23950\n",
      "24000\n",
      "24050\n",
      "24100\n",
      "24150\n",
      "24200\n",
      "24250\n",
      "24300\n",
      "24350\n",
      "24400\n",
      "24450\n",
      "24500\n",
      "24550\n",
      "24600\n",
      "24650\n",
      "24700\n",
      "24750\n",
      "24800\n",
      "24850\n",
      "24900\n",
      "24950\n",
      "25000\n",
      "25050\n",
      "25100\n",
      "25150\n",
      "25200\n",
      "25250\n",
      "25300\n",
      "25350\n",
      "25400\n",
      "25450\n",
      "25500\n",
      "25550\n",
      "25600\n",
      "25650\n",
      "25700\n",
      "25750\n",
      "25800\n",
      "25850\n",
      "25900\n",
      "25950\n",
      "Epoch: 5/20...  Training Step: 16400...  Training loss: 1.5499...  0.0774 sec/batch\n",
      "26000\n",
      "26050\n",
      "26100\n",
      "26150\n",
      "26200\n",
      "26250\n",
      "26300\n",
      "26350\n",
      "26400\n",
      "26450\n",
      "26500\n",
      "26550\n",
      "26600\n",
      "26650\n",
      "26700\n",
      "26750\n",
      "26800\n",
      "26850\n",
      "26900\n",
      "26950\n",
      "27000\n",
      "27050\n",
      "27100\n",
      "27150\n",
      "27200\n",
      "27250\n",
      "27300\n",
      "27350\n",
      "27400\n",
      "27450\n",
      "27500\n",
      "27550\n",
      "27600\n",
      "27650\n",
      "27700\n",
      "27750\n",
      "27800\n",
      "27850\n",
      "27900\n",
      "27950\n",
      "28000\n",
      "28050\n",
      "28100\n",
      "28150\n",
      "28200\n",
      "28250\n",
      "28300\n",
      "28350\n",
      "28400\n",
      "28450\n",
      "Epoch: 5/20...  Training Step: 16450...  Training loss: 1.6227...  0.0772 sec/batch\n",
      "28500\n",
      "28550\n",
      "28600\n",
      "28650\n",
      "28700\n",
      "28750\n",
      "28800\n",
      "28850\n",
      "28900\n",
      "28950\n",
      "29000\n",
      "29050\n",
      "29100\n",
      "29150\n",
      "29200\n",
      "29250\n",
      "29300\n",
      "29350\n",
      "29400\n",
      "29450\n",
      "29500\n",
      "29550\n",
      "29600\n",
      "29650\n",
      "29700\n",
      "29750\n",
      "29800\n",
      "29850\n",
      "29900\n",
      "29950\n",
      "30000\n",
      "30050\n",
      "30100\n",
      "30150\n",
      "30200\n",
      "30250\n",
      "30300\n",
      "30350\n",
      "30400\n",
      "30450\n",
      "30500\n",
      "30550\n",
      "30600\n",
      "30650\n",
      "30700\n",
      "30750\n",
      "30800\n",
      "30850\n",
      "30900\n",
      "30950\n",
      "Epoch: 5/20...  Training Step: 16500...  Training loss: 1.4518...  0.0796 sec/batch\n",
      "31000\n",
      "31050\n",
      "31100\n",
      "31150\n",
      "31200\n",
      "31250\n",
      "31300\n",
      "31350\n",
      "31400\n",
      "31450\n",
      "31500\n",
      "31550\n",
      "31600\n",
      "31650\n",
      "31700\n",
      "31750\n",
      "31800\n",
      "31850\n",
      "31900\n",
      "31950\n",
      "32000\n",
      "32050\n",
      "32100\n",
      "32150\n",
      "32200\n",
      "32250\n",
      "32300\n",
      "32350\n",
      "32400\n",
      "32450\n",
      "32500\n",
      "32550\n",
      "32600\n",
      "32650\n",
      "32700\n",
      "32750\n",
      "32800\n",
      "32850\n",
      "32900\n",
      "32950\n",
      "33000\n",
      "33050\n",
      "33100\n",
      "33150\n",
      "33200\n",
      "33250\n",
      "33300\n",
      "33350\n",
      "33400\n",
      "33450\n",
      "Epoch: 5/20...  Training Step: 16550...  Training loss: 1.6477...  0.0856 sec/batch\n",
      "33500\n",
      "33550\n",
      "33600\n",
      "33650\n",
      "33700\n",
      "33750\n",
      "33800\n",
      "33850\n",
      "33900\n",
      "33950\n",
      "34000\n",
      "34050\n",
      "34100\n",
      "34150\n",
      "34200\n",
      "34250\n",
      "34300\n",
      "34350\n",
      "34400\n",
      "34450\n",
      "34500\n",
      "34550\n",
      "34600\n",
      "34650\n",
      "34700\n",
      "34750\n",
      "34800\n",
      "34850\n",
      "34900\n",
      "34950\n",
      "35000\n",
      "35050\n",
      "35100\n",
      "35150\n",
      "35200\n",
      "35250\n",
      "35300\n",
      "35350\n",
      "35400\n",
      "35450\n",
      "35500\n",
      "35550\n",
      "35600\n",
      "35650\n",
      "35700\n",
      "35750\n",
      "35800\n",
      "35850\n",
      "35900\n",
      "35950\n",
      "Epoch: 5/20...  Training Step: 16600...  Training loss: 1.5827...  0.0792 sec/batch\n",
      "36000\n",
      "36050\n",
      "36100\n",
      "36150\n",
      "36200\n",
      "36250\n",
      "36300\n",
      "36350\n",
      "36400\n",
      "36450\n",
      "36500\n",
      "36550\n",
      "36600\n",
      "36650\n",
      "36700\n",
      "36750\n",
      "36800\n",
      "36850\n",
      "36900\n",
      "36950\n",
      "37000\n",
      "37050\n",
      "37100\n",
      "37150\n",
      "37200\n",
      "37250\n",
      "37300\n",
      "37350\n",
      "37400\n",
      "37450\n",
      "37500\n",
      "37550\n",
      "37600\n",
      "37650\n",
      "37700\n",
      "37750\n",
      "37800\n",
      "37850\n",
      "37900\n",
      "37950\n",
      "38000\n",
      "38050\n",
      "38100\n",
      "38150\n",
      "38200\n",
      "38250\n",
      "38300\n",
      "38350\n",
      "38400\n",
      "38450\n",
      "Epoch: 5/20...  Training Step: 16650...  Training loss: 1.6526...  0.0853 sec/batch\n",
      "38500\n",
      "38550\n",
      "38600\n",
      "38650\n",
      "38700\n",
      "38750\n",
      "38800\n",
      "38850\n",
      "38900\n",
      "38950\n",
      "39000\n",
      "39050\n",
      "39100\n",
      "39150\n",
      "39200\n",
      "39250\n",
      "39300\n",
      "39350\n",
      "39400\n",
      "39450\n",
      "39500\n",
      "39550\n",
      "39600\n",
      "39650\n",
      "39700\n",
      "39750\n",
      "39800\n",
      "39850\n",
      "39900\n",
      "39950\n",
      "40000\n",
      "40050\n",
      "40100\n",
      "40150\n",
      "40200\n",
      "40250\n",
      "40300\n",
      "40350\n",
      "40400\n",
      "40450\n",
      "40500\n",
      "40550\n",
      "40600\n",
      "40650\n",
      "40700\n",
      "40750\n",
      "40800\n",
      "40850\n",
      "40900\n",
      "40950\n",
      "Epoch: 5/20...  Training Step: 16700...  Training loss: 1.6166...  0.0911 sec/batch\n",
      "41000\n",
      "41050\n",
      "41100\n",
      "41150\n",
      "41200\n",
      "41250\n",
      "41300\n",
      "41350\n",
      "41400\n",
      "41450\n",
      "41500\n",
      "41550\n",
      "41600\n",
      "41650\n",
      "41700\n",
      "41750\n",
      "41800\n",
      "41850\n",
      "41900\n",
      "41950\n",
      "42000\n",
      "42050\n",
      "42100\n",
      "42150\n",
      "42200\n",
      "42250\n",
      "42300\n",
      "42350\n",
      "42400\n",
      "42450\n",
      "42500\n",
      "42550\n",
      "42600\n",
      "42650\n",
      "42700\n",
      "42750\n",
      "42800\n",
      "42850\n",
      "42900\n",
      "42950\n",
      "43000\n",
      "43050\n",
      "43100\n",
      "43150\n",
      "43200\n",
      "43250\n",
      "43300\n",
      "43350\n",
      "43400\n",
      "43450\n",
      "Epoch: 5/20...  Training Step: 16750...  Training loss: 1.6442...  0.0820 sec/batch\n",
      "43500\n",
      "43550\n",
      "43600\n",
      "43650\n",
      "43700\n",
      "43750\n",
      "43800\n",
      "43850\n",
      "43900\n",
      "43950\n",
      "44000\n",
      "44050\n",
      "44100\n",
      "44150\n",
      "44200\n",
      "44250\n",
      "44300\n",
      "44350\n",
      "44400\n",
      "44450\n",
      "44500\n",
      "44550\n",
      "44600\n",
      "44650\n",
      "44700\n",
      "44750\n",
      "44800\n",
      "44850\n",
      "44900\n",
      "44950\n",
      "45000\n",
      "45050\n",
      "45100\n",
      "45150\n",
      "45200\n",
      "45250\n",
      "45300\n",
      "45350\n",
      "45400\n",
      "45450\n",
      "45500\n",
      "45550\n",
      "45600\n",
      "45650\n",
      "45700\n",
      "45750\n",
      "45800\n",
      "45850\n",
      "45900\n",
      "45950\n",
      "Epoch: 5/20...  Training Step: 16800...  Training loss: 1.5045...  0.0775 sec/batch\n",
      "46000\n",
      "46050\n",
      "46100\n",
      "46150\n",
      "46200\n",
      "46250\n",
      "46300\n",
      "46350\n",
      "46400\n",
      "46450\n",
      "46500\n",
      "46550\n",
      "46600\n",
      "46650\n",
      "46700\n",
      "46750\n",
      "46800\n",
      "46850\n",
      "46900\n",
      "46950\n",
      "47000\n",
      "47050\n",
      "47100\n",
      "47150\n",
      "47200\n",
      "47250\n",
      "47300\n",
      "47350\n",
      "47400\n",
      "47450\n",
      "47500\n",
      "47550\n",
      "47600\n",
      "47650\n",
      "47700\n",
      "47750\n",
      "47800\n",
      "47850\n",
      "47900\n",
      "47950\n",
      "48000\n",
      "48050\n",
      "48100\n",
      "48150\n",
      "48200\n",
      "48250\n",
      "48300\n",
      "48350\n",
      "48400\n",
      "48450\n",
      "Epoch: 5/20...  Training Step: 16850...  Training loss: 1.4567...  0.1008 sec/batch\n",
      "48500\n",
      "48550\n",
      "48600\n",
      "48650\n",
      "48700\n",
      "48750\n",
      "48800\n",
      "48850\n",
      "48900\n",
      "48950\n",
      "49000\n",
      "49050\n",
      "49100\n",
      "49150\n",
      "49200\n",
      "49250\n",
      "49300\n",
      "49350\n",
      "49400\n",
      "49450\n",
      "49500\n",
      "49550\n",
      "49600\n",
      "49650\n",
      "49700\n",
      "49750\n",
      "49800\n",
      "49850\n",
      "49900\n",
      "49950\n",
      "50000\n",
      "50050\n",
      "50100\n",
      "50150\n",
      "50200\n",
      "50250\n",
      "50300\n",
      "50350\n",
      "50400\n",
      "50450\n",
      "50500\n",
      "50550\n",
      "50600\n",
      "50650\n",
      "50700\n",
      "50750\n",
      "50800\n",
      "50850\n",
      "50900\n",
      "50950\n",
      "Epoch: 5/20...  Training Step: 16900...  Training loss: 1.4339...  0.0939 sec/batch\n",
      "51000\n",
      "51050\n",
      "51100\n",
      "51150\n",
      "51200\n",
      "51250\n",
      "51300\n",
      "51350\n",
      "51400\n",
      "51450\n",
      "51500\n",
      "51550\n",
      "51600\n",
      "51650\n",
      "51700\n",
      "51750\n",
      "51800\n",
      "51850\n",
      "51900\n",
      "51950\n",
      "52000\n",
      "52050\n",
      "52100\n",
      "52150\n",
      "52200\n",
      "52250\n",
      "52300\n",
      "52350\n",
      "52400\n",
      "52450\n",
      "52500\n",
      "52550\n",
      "52600\n",
      "52650\n",
      "52700\n",
      "52750\n",
      "52800\n",
      "52850\n",
      "52900\n",
      "52950\n",
      "53000\n",
      "53050\n",
      "53100\n",
      "53150\n",
      "53200\n",
      "53250\n",
      "53300\n",
      "53350\n",
      "53400\n",
      "53450\n",
      "Epoch: 5/20...  Training Step: 16950...  Training loss: 1.5224...  0.1149 sec/batch\n",
      "53500\n",
      "53550\n",
      "53600\n",
      "53650\n",
      "53700\n",
      "53750\n",
      "53800\n",
      "53850\n",
      "53900\n",
      "53950\n",
      "54000\n",
      "54050\n",
      "54100\n",
      "54150\n",
      "54200\n",
      "54250\n",
      "54300\n",
      "54350\n",
      "54400\n",
      "54450\n",
      "54500\n",
      "54550\n",
      "54600\n",
      "54650\n",
      "54700\n",
      "54750\n",
      "54800\n",
      "54850\n",
      "54900\n",
      "54950\n",
      "55000\n",
      "55050\n",
      "55100\n",
      "55150\n",
      "55200\n",
      "55250\n",
      "55300\n",
      "55350\n",
      "55400\n",
      "55450\n",
      "55500\n",
      "55550\n",
      "55600\n",
      "55650\n",
      "55700\n",
      "55750\n",
      "55800\n",
      "55850\n",
      "55900\n",
      "55950\n",
      "Epoch: 5/20...  Training Step: 17000...  Training loss: 1.4298...  0.0932 sec/batch\n",
      "56000\n",
      "56050\n",
      "56100\n",
      "56150\n",
      "56200\n",
      "56250\n",
      "56300\n",
      "56350\n",
      "56400\n",
      "56450\n",
      "56500\n",
      "56550\n",
      "56600\n",
      "56650\n",
      "56700\n",
      "56750\n",
      "56800\n",
      "56850\n",
      "56900\n",
      "56950\n",
      "57000\n",
      "57050\n",
      "57100\n",
      "57150\n",
      "57200\n",
      "57250\n",
      "57300\n",
      "57350\n",
      "57400\n",
      "57450\n",
      "57500\n",
      "57550\n",
      "57600\n",
      "57650\n",
      "57700\n",
      "57750\n",
      "57800\n",
      "57850\n",
      "57900\n",
      "57950\n",
      "58000\n",
      "58050\n",
      "58100\n",
      "58150\n",
      "58200\n",
      "58250\n",
      "58300\n",
      "58350\n",
      "58400\n",
      "58450\n",
      "Epoch: 5/20...  Training Step: 17050...  Training loss: 1.6049...  0.0826 sec/batch\n",
      "58500\n",
      "58550\n",
      "58600\n",
      "58650\n",
      "58700\n",
      "58750\n",
      "58800\n",
      "58850\n",
      "58900\n",
      "58950\n",
      "59000\n",
      "59050\n",
      "59100\n",
      "59150\n",
      "59200\n",
      "59250\n",
      "59300\n",
      "59350\n",
      "59400\n",
      "59450\n",
      "59500\n",
      "59550\n",
      "59600\n",
      "59650\n",
      "59700\n",
      "59750\n",
      "59800\n",
      "59850\n",
      "59900\n",
      "59950\n",
      "60000\n",
      "60050\n",
      "60100\n",
      "60150\n",
      "60200\n",
      "60250\n",
      "60300\n",
      "60350\n",
      "60400\n",
      "60450\n",
      "60500\n",
      "60550\n",
      "60600\n",
      "60650\n",
      "60700\n",
      "60750\n",
      "60800\n",
      "60850\n",
      "60900\n",
      "60950\n",
      "Epoch: 5/20...  Training Step: 17100...  Training loss: 1.4991...  0.0793 sec/batch\n",
      "61000\n",
      "61050\n",
      "61100\n",
      "61150\n",
      "61200\n",
      "61250\n",
      "61300\n",
      "61350\n",
      "61400\n",
      "61450\n",
      "61500\n",
      "61550\n",
      "61600\n",
      "61650\n",
      "61700\n",
      "61750\n",
      "61800\n",
      "61850\n",
      "61900\n",
      "61950\n",
      "62000\n",
      "62050\n",
      "62100\n",
      "62150\n",
      "62200\n",
      "62250\n",
      "62300\n",
      "62350\n",
      "62400\n",
      "62450\n",
      "62500\n",
      "62550\n",
      "62600\n",
      "62650\n",
      "62700\n",
      "62750\n",
      "62800\n",
      "62850\n",
      "62900\n",
      "62950\n",
      "63000\n",
      "63050\n",
      "63100\n",
      "63150\n",
      "63200\n",
      "63250\n",
      "63300\n",
      "63350\n",
      "63400\n",
      "63450\n",
      "Epoch: 5/20...  Training Step: 17150...  Training loss: 1.5744...  0.0846 sec/batch\n",
      "63500\n",
      "63550\n",
      "63600\n",
      "63650\n",
      "63700\n",
      "63750\n",
      "63800\n",
      "63850\n",
      "63900\n",
      "63950\n",
      "64000\n",
      "64050\n",
      "64100\n",
      "64150\n",
      "64200\n",
      "64250\n",
      "64300\n",
      "64350\n",
      "64400\n",
      "64450\n",
      "64500\n",
      "64550\n",
      "64600\n",
      "64650\n",
      "64700\n",
      "64750\n",
      "64800\n",
      "64850\n",
      "64900\n",
      "64950\n",
      "65000\n",
      "65050\n",
      "65100\n",
      "65150\n",
      "65200\n",
      "65250\n",
      "65300\n",
      "65350\n",
      "65400\n",
      "65450\n",
      "65500\n",
      "65550\n",
      "65600\n",
      "65650\n",
      "65700\n",
      "65750\n",
      "65800\n",
      "65850\n",
      "65900\n",
      "65950\n",
      "Epoch: 5/20...  Training Step: 17200...  Training loss: 1.6297...  0.1080 sec/batch\n",
      "66000\n",
      "66050\n",
      "66100\n",
      "66150\n",
      "66200\n",
      "66250\n",
      "66300\n",
      "66350\n",
      "66400\n",
      "66450\n",
      "66500\n",
      "66550\n",
      "66600\n",
      "66650\n",
      "66700\n",
      "66750\n",
      "66800\n",
      "66850\n",
      "66900\n",
      "66950\n",
      "67000\n",
      "67050\n",
      "67100\n",
      "67150\n",
      "67200\n",
      "67250\n",
      "67300\n",
      "67350\n",
      "67400\n",
      "67450\n",
      "67500\n",
      "67550\n",
      "67600\n",
      "67650\n",
      "67700\n",
      "67750\n",
      "67800\n",
      "67850\n",
      "67900\n",
      "67950\n",
      "68000\n",
      "68050\n",
      "68100\n",
      "68150\n",
      "68200\n",
      "68250\n",
      "68300\n",
      "68350\n",
      "68400\n",
      "68450\n",
      "Epoch: 5/20...  Training Step: 17250...  Training loss: 1.6437...  0.0899 sec/batch\n",
      "68500\n",
      "68550\n",
      "68600\n",
      "68650\n",
      "68700\n",
      "68750\n",
      "68800\n",
      "68850\n",
      "68900\n",
      "68950\n",
      "69000\n",
      "69050\n",
      "69100\n",
      "69150\n",
      "69200\n",
      "69250\n",
      "69300\n",
      "69350\n",
      "69400\n",
      "69450\n",
      "69500\n",
      "69550\n",
      "69600\n",
      "69650\n",
      "69700\n",
      "69750\n",
      "69800\n",
      "69850\n",
      "69900\n",
      "69950\n",
      "70000\n",
      "70050\n",
      "70100\n",
      "70150\n",
      "70200\n",
      "70250\n",
      "70300\n",
      "70350\n",
      "70400\n",
      "70450\n",
      "70500\n",
      "70550\n",
      "70600\n",
      "70650\n",
      "70700\n",
      "70750\n",
      "70800\n",
      "70850\n",
      "70900\n",
      "70950\n",
      "Epoch: 5/20...  Training Step: 17300...  Training loss: 1.5431...  0.0806 sec/batch\n",
      "71000\n",
      "71050\n",
      "71100\n",
      "71150\n",
      "71200\n",
      "71250\n",
      "71300\n",
      "71350\n",
      "71400\n",
      "71450\n",
      "71500\n",
      "71550\n",
      "71600\n",
      "71650\n",
      "71700\n",
      "71750\n",
      "71800\n",
      "71850\n",
      "71900\n",
      "71950\n",
      "72000\n",
      "72050\n",
      "72100\n",
      "72150\n",
      "72200\n",
      "72250\n",
      "72300\n",
      "72350\n",
      "72400\n",
      "72450\n",
      "72500\n",
      "72550\n",
      "72600\n",
      "72650\n",
      "72700\n",
      "72750\n",
      "72800\n",
      "72850\n",
      "72900\n",
      "72950\n",
      "73000\n",
      "73050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73100\n",
      "73150\n",
      "73200\n",
      "73250\n",
      "73300\n",
      "73350\n",
      "73400\n",
      "73450\n",
      "Epoch: 5/20...  Training Step: 17350...  Training loss: 1.5241...  0.0810 sec/batch\n",
      "73500\n",
      "73550\n",
      "73600\n",
      "73650\n",
      "73700\n",
      "73750\n",
      "73800\n",
      "73850\n",
      "73900\n",
      "73950\n",
      "74000\n",
      "74050\n",
      "74100\n",
      "74150\n",
      "74200\n",
      "74250\n",
      "74300\n",
      "74350\n",
      "74400\n",
      "74450\n",
      "74500\n",
      "74550\n",
      "74600\n",
      "74650\n",
      "74700\n",
      "74750\n",
      "74800\n",
      "74850\n",
      "74900\n",
      "74950\n",
      "75000\n",
      "75050\n",
      "75100\n",
      "75150\n",
      "75200\n",
      "75250\n",
      "75300\n",
      "75350\n",
      "75400\n",
      "75450\n",
      "75500\n",
      "75550\n",
      "75600\n",
      "75650\n",
      "75700\n",
      "75750\n",
      "75800\n",
      "75850\n",
      "75900\n",
      "75950\n",
      "Epoch: 5/20...  Training Step: 17400...  Training loss: 1.5574...  0.0816 sec/batch\n",
      "76000\n",
      "76050\n",
      "76100\n",
      "76150\n",
      "76200\n",
      "76250\n",
      "76300\n",
      "76350\n",
      "76400\n",
      "76450\n",
      "76500\n",
      "76550\n",
      "76600\n",
      "76650\n",
      "76700\n",
      "76750\n",
      "76800\n",
      "76850\n",
      "76900\n",
      "76950\n",
      "77000\n",
      "77050\n",
      "77100\n",
      "77150\n",
      "77200\n",
      "77250\n",
      "77300\n",
      "77350\n",
      "77400\n",
      "77450\n",
      "77500\n",
      "77550\n",
      "77600\n",
      "77650\n",
      "77700\n",
      "77750\n",
      "77800\n",
      "77850\n",
      "77900\n",
      "77950\n",
      "78000\n",
      "78050\n",
      "78100\n",
      "78150\n",
      "78200\n",
      "78250\n",
      "78300\n",
      "78350\n",
      "78400\n",
      "78450\n",
      "Epoch: 5/20...  Training Step: 17450...  Training loss: 1.5970...  0.0828 sec/batch\n",
      "78500\n",
      "78550\n",
      "78600\n",
      "78650\n",
      "78700\n",
      "78750\n",
      "78800\n",
      "78850\n",
      "78900\n",
      "78950\n",
      "79000\n",
      "79050\n",
      "79100\n",
      "79150\n",
      "79200\n",
      "79250\n",
      "79300\n",
      "79350\n",
      "79400\n",
      "79450\n",
      "79500\n",
      "79550\n",
      "79600\n",
      "79650\n",
      "79700\n",
      "79750\n",
      "79800\n",
      "79850\n",
      "79900\n",
      "79950\n",
      "80000\n",
      "80050\n",
      "80100\n",
      "80150\n",
      "80200\n",
      "80250\n",
      "80300\n",
      "80350\n",
      "80400\n",
      "80450\n",
      "80500\n",
      "80550\n",
      "80600\n",
      "80650\n",
      "80700\n",
      "80750\n",
      "80800\n",
      "80850\n",
      "80900\n",
      "80950\n",
      "Epoch: 5/20...  Training Step: 17500...  Training loss: 1.7045...  0.0917 sec/batch\n",
      "81000\n",
      "81050\n",
      "81100\n",
      "81150\n",
      "81200\n",
      "81250\n",
      "81300\n",
      "81350\n",
      "81400\n",
      "81450\n",
      "81500\n",
      "81550\n",
      "81600\n",
      "81650\n",
      "81700\n",
      "81750\n",
      "81800\n",
      "81850\n",
      "81900\n",
      "81950\n",
      "82000\n",
      "82050\n",
      "82100\n",
      "82150\n",
      "82200\n",
      "82250\n",
      "82300\n",
      "82350\n",
      "82400\n",
      "82450\n",
      "82500\n",
      "82550\n",
      "82600\n",
      "82650\n",
      "82700\n",
      "82750\n",
      "82800\n",
      "82850\n",
      "82900\n",
      "82950\n",
      "83000\n",
      "83050\n",
      "83100\n",
      "83150\n",
      "83200\n",
      "83250\n",
      "83300\n",
      "83350\n",
      "83400\n",
      "83450\n",
      "Epoch: 5/20...  Training Step: 17550...  Training loss: 1.6892...  0.0783 sec/batch\n",
      "83500\n",
      "83550\n",
      "83600\n",
      "83650\n",
      "83700\n",
      "83750\n",
      "83800\n",
      "83850\n",
      "83900\n",
      "83950\n",
      "84000\n",
      "84050\n",
      "84100\n",
      "84150\n",
      "84200\n",
      "84250\n",
      "84300\n",
      "84350\n",
      "84400\n",
      "84450\n",
      "84500\n",
      "84550\n",
      "84600\n",
      "84650\n",
      "84700\n",
      "84750\n",
      "84800\n",
      "84850\n",
      "84900\n",
      "84950\n",
      "85000\n",
      "85050\n",
      "85100\n",
      "85150\n",
      "85200\n",
      "85250\n",
      "85300\n",
      "85350\n",
      "85400\n",
      "85450\n",
      "85500\n",
      "85550\n",
      "85600\n",
      "85650\n",
      "85700\n",
      "85750\n",
      "85800\n",
      "85850\n",
      "85900\n",
      "85950\n",
      "Epoch: 5/20...  Training Step: 17600...  Training loss: 1.5317...  0.1078 sec/batch\n",
      "86000\n",
      "86050\n",
      "86100\n",
      "86150\n",
      "86200\n",
      "86250\n",
      "86300\n",
      "86350\n",
      "86400\n",
      "86450\n",
      "86500\n",
      "86550\n",
      "86600\n",
      "86650\n",
      "86700\n",
      "86750\n",
      "86800\n",
      "86850\n",
      "86900\n",
      "86950\n",
      "87000\n",
      "87050\n",
      "87100\n",
      "87150\n",
      "87200\n",
      "87250\n",
      "87300\n",
      "87350\n",
      "87400\n",
      "87450\n",
      "87500\n",
      "87550\n",
      "87600\n",
      "87650\n",
      "87700\n",
      "87750\n",
      "87800\n",
      "87850\n",
      "87900\n",
      "87950\n",
      "88000\n",
      "88050\n",
      "88100\n",
      "88150\n",
      "88200\n",
      "88250\n",
      "88300\n",
      "88350\n",
      "88400\n",
      "88450\n",
      "Epoch: 5/20...  Training Step: 17650...  Training loss: 1.6565...  0.0829 sec/batch\n",
      "88500\n",
      "88550\n",
      "88600\n",
      "88650\n",
      "88700\n",
      "88750\n",
      "88800\n",
      "88850\n",
      "88900\n",
      "88950\n",
      "89000\n",
      "89050\n",
      "89100\n",
      "89150\n",
      "89200\n",
      "89250\n",
      "89300\n",
      "89350\n",
      "89400\n",
      "89450\n",
      "89500\n",
      "89550\n",
      "89600\n",
      "89650\n",
      "89700\n",
      "89750\n",
      "89800\n",
      "89850\n",
      "89900\n",
      "89950\n",
      "90000\n",
      "90050\n",
      "90100\n",
      "90150\n",
      "90200\n",
      "90250\n",
      "90300\n",
      "90350\n",
      "90400\n",
      "90450\n",
      "90500\n",
      "90550\n",
      "90600\n",
      "90650\n",
      "90700\n",
      "90750\n",
      "90800\n",
      "90850\n",
      "90900\n",
      "90950\n",
      "Epoch: 5/20...  Training Step: 17700...  Training loss: 1.5524...  0.0763 sec/batch\n",
      "91000\n",
      "91050\n",
      "91100\n",
      "91150\n",
      "91200\n",
      "91250\n",
      "91300\n",
      "91350\n",
      "91400\n",
      "91450\n",
      "91500\n",
      "91550\n",
      "91600\n",
      "91650\n",
      "91700\n",
      "91750\n",
      "91800\n",
      "91850\n",
      "91900\n",
      "91950\n",
      "92000\n",
      "92050\n",
      "92100\n",
      "92150\n",
      "92200\n",
      "92250\n",
      "92300\n",
      "92350\n",
      "92400\n",
      "92450\n",
      "92500\n",
      "92550\n",
      "92600\n",
      "92650\n",
      "92700\n",
      "92750\n",
      "92800\n",
      "92850\n",
      "92900\n",
      "92950\n",
      "93000\n",
      "93050\n",
      "93100\n",
      "93150\n",
      "93200\n",
      "93250\n",
      "93300\n",
      "93350\n",
      "93400\n",
      "93450\n",
      "Epoch: 5/20...  Training Step: 17750...  Training loss: 1.3901...  0.0817 sec/batch\n",
      "93500\n",
      "93550\n",
      "93600\n",
      "93650\n",
      "93700\n",
      "93750\n",
      "93800\n",
      "93850\n",
      "93900\n",
      "93950\n",
      "94000\n",
      "94050\n",
      "94100\n",
      "94150\n",
      "94200\n",
      "94250\n",
      "94300\n",
      "94350\n",
      "94400\n",
      "94450\n",
      "94500\n",
      "94550\n",
      "94600\n",
      "94650\n",
      "94700\n",
      "94750\n",
      "94800\n",
      "94850\n",
      "94900\n",
      "94950\n",
      "95000\n",
      "95050\n",
      "95100\n",
      "95150\n",
      "95200\n",
      "95250\n",
      "95300\n",
      "95350\n",
      "95400\n",
      "95450\n",
      "95500\n",
      "95550\n",
      "95600\n",
      "95650\n",
      "95700\n",
      "95750\n",
      "95800\n",
      "95850\n",
      "95900\n",
      "95950\n",
      "Epoch: 5/20...  Training Step: 17800...  Training loss: 1.5853...  0.0821 sec/batch\n",
      "96000\n",
      "96050\n",
      "96100\n",
      "96150\n",
      "96200\n",
      "96250\n",
      "96300\n",
      "96350\n",
      "96400\n",
      "96450\n",
      "96500\n",
      "96550\n",
      "96600\n",
      "96650\n",
      "96700\n",
      "96750\n",
      "96800\n",
      "96850\n",
      "96900\n",
      "96950\n",
      "97000\n",
      "97050\n",
      "97100\n",
      "97150\n",
      "97200\n",
      "97250\n",
      "97300\n",
      "97350\n",
      "97400\n",
      "97450\n",
      "97500\n",
      "97550\n",
      "97600\n",
      "97650\n",
      "97700\n",
      "97750\n",
      "97800\n",
      "97850\n",
      "97900\n",
      "97950\n",
      "98000\n",
      "98050\n",
      "98100\n",
      "98150\n",
      "98200\n",
      "98250\n",
      "98300\n",
      "98350\n",
      "98400\n",
      "98450\n",
      "Epoch: 5/20...  Training Step: 17850...  Training loss: 1.5564...  0.1027 sec/batch\n",
      "98500\n",
      "98550\n",
      "98600\n",
      "98650\n",
      "98700\n",
      "98750\n",
      "98800\n",
      "98850\n",
      "98900\n",
      "98950\n",
      "99000\n",
      "99050\n",
      "99100\n",
      "99150\n",
      "99200\n",
      "99250\n",
      "99300\n",
      "99350\n",
      "99400\n",
      "99450\n",
      "99500\n",
      "99550\n",
      "99600\n",
      "99650\n",
      "99700\n",
      "99750\n",
      "99800\n",
      "99850\n",
      "99900\n",
      "99950\n",
      "100000\n",
      "100050\n",
      "100100\n",
      "100150\n",
      "100200\n",
      "100250\n",
      "100300\n",
      "100350\n",
      "100400\n",
      "100450\n",
      "100500\n",
      "100550\n",
      "100600\n",
      "100650\n",
      "100700\n",
      "100750\n",
      "100800\n",
      "100850\n",
      "100900\n",
      "100950\n",
      "Epoch: 5/20...  Training Step: 17900...  Training loss: 1.4560...  0.0783 sec/batch\n",
      "101000\n",
      "101050\n",
      "101100\n",
      "101150\n",
      "101200\n",
      "101250\n",
      "101300\n",
      "101350\n",
      "101400\n",
      "101450\n",
      "101500\n",
      "101550\n",
      "101600\n",
      "101650\n",
      "101700\n",
      "101750\n",
      "101800\n",
      "101850\n",
      "101900\n",
      "101950\n",
      "102000\n",
      "102050\n",
      "102100\n",
      "102150\n",
      "102200\n",
      "102250\n",
      "102300\n",
      "102350\n",
      "102400\n",
      "102450\n",
      "102500\n",
      "102550\n",
      "102600\n",
      "102650\n",
      "102700\n",
      "102750\n",
      "102800\n",
      "102850\n",
      "102900\n",
      "102950\n",
      "103000\n",
      "103050\n",
      "103100\n",
      "103150\n",
      "103200\n",
      "103250\n",
      "103300\n",
      "103350\n",
      "103400\n",
      "103450\n",
      "Epoch: 5/20...  Training Step: 17950...  Training loss: 1.4931...  0.0825 sec/batch\n",
      "103500\n",
      "103550\n",
      "103600\n",
      "103650\n",
      "103700\n",
      "103750\n",
      "103800\n",
      "103850\n",
      "103900\n",
      "103950\n",
      "104000\n",
      "104050\n",
      "104100\n",
      "104150\n",
      "104200\n",
      "104250\n",
      "104300\n",
      "104350\n",
      "104400\n",
      "104450\n",
      "104500\n",
      "104550\n",
      "104600\n",
      "104650\n",
      "104700\n",
      "104750\n",
      "104800\n",
      "104850\n",
      "104900\n",
      "104950\n",
      "105000\n",
      "105050\n",
      "105100\n",
      "105150\n",
      "105200\n",
      "105250\n",
      "105300\n",
      "105350\n",
      "105400\n",
      "105450\n",
      "105500\n",
      "105550\n",
      "105600\n",
      "105650\n",
      "105700\n",
      "105750\n",
      "105800\n",
      "105850\n",
      "105900\n",
      "105950\n",
      "Epoch: 5/20...  Training Step: 18000...  Training loss: 1.3895...  0.0884 sec/batch\n",
      "106000\n",
      "106050\n",
      "106100\n",
      "106150\n",
      "106200\n",
      "106250\n",
      "106300\n",
      "106350\n",
      "106400\n",
      "106450\n",
      "106500\n",
      "106550\n",
      "106600\n",
      "106650\n",
      "106700\n",
      "106750\n",
      "106800\n",
      "106850\n",
      "106900\n",
      "106950\n",
      "107000\n",
      "107050\n",
      "107100\n",
      "107150\n",
      "107200\n",
      "107250\n",
      "107300\n",
      "107350\n",
      "107400\n",
      "107450\n",
      "107500\n",
      "107550\n",
      "107600\n",
      "107650\n",
      "107700\n",
      "107750\n",
      "107800\n",
      "107850\n",
      "107900\n",
      "107950\n",
      "108000\n",
      "108050\n",
      "108100\n",
      "108150\n",
      "108200\n",
      "108250\n",
      "108300\n",
      "108350\n",
      "108400\n",
      "108450\n",
      "Epoch: 5/20...  Training Step: 18050...  Training loss: 1.5761...  0.0794 sec/batch\n",
      "108500\n",
      "108550\n",
      "108600\n",
      "108650\n",
      "108700\n",
      "108750\n",
      "108800\n",
      "108850\n",
      "108900\n",
      "108950\n",
      "109000\n",
      "109050\n",
      "109100\n",
      "109150\n",
      "109200\n",
      "109250\n",
      "109300\n",
      "109350\n",
      "109400\n",
      "109450\n",
      "109500\n",
      "109550\n",
      "109600\n",
      "109650\n",
      "109700\n",
      "109750\n",
      "109800\n",
      "109850\n",
      "109900\n",
      "109950\n",
      "110000\n",
      "110050\n",
      "110100\n",
      "110150\n",
      "110200\n",
      "110250\n",
      "110300\n",
      "110350\n",
      "110400\n",
      "110450\n",
      "110500\n",
      "110550\n",
      "110600\n",
      "110650\n",
      "110700\n",
      "110750\n",
      "110800\n",
      "110850\n",
      "110900\n",
      "110950\n",
      "Epoch: 5/20...  Training Step: 18100...  Training loss: 1.6166...  0.0766 sec/batch\n",
      "111000\n",
      "111050\n",
      "111100\n",
      "111150\n",
      "111200\n",
      "111250\n",
      "111300\n",
      "111350\n",
      "111400\n",
      "111450\n",
      "111500\n",
      "111550\n",
      "111600\n",
      "111650\n",
      "111700\n",
      "111750\n",
      "111800\n",
      "111850\n",
      "111900\n",
      "111950\n",
      "112000\n",
      "112050\n",
      "112100\n",
      "112150\n",
      "112200\n",
      "112250\n",
      "112300\n",
      "112350\n",
      "112400\n",
      "112450\n",
      "112500\n",
      "112550\n",
      "112600\n",
      "112650\n",
      "112700\n",
      "112750\n",
      "112800\n",
      "112850\n",
      "112900\n",
      "112950\n",
      "113000\n",
      "113050\n",
      "113100\n",
      "113150\n",
      "113200\n",
      "113250\n",
      "113300\n",
      "113350\n",
      "113400\n",
      "113450\n",
      "Epoch: 5/20...  Training Step: 18150...  Training loss: 1.5127...  0.0861 sec/batch\n",
      "113500\n",
      "113550\n",
      "113600\n",
      "113650\n",
      "113700\n",
      "113750\n",
      "113800\n",
      "113850\n",
      "113900\n",
      "113950\n",
      "114000\n",
      "114050\n",
      "114100\n",
      "114150\n",
      "114200\n",
      "114250\n",
      "114300\n",
      "114350\n",
      "114400\n",
      "114450\n",
      "114500\n",
      "114550\n",
      "114600\n",
      "114650\n",
      "114700\n",
      "114750\n",
      "114800\n",
      "114850\n",
      "114900\n",
      "114950\n",
      "115000\n",
      "115050\n",
      "115100\n",
      "115150\n",
      "115200\n",
      "115250\n",
      "115300\n",
      "115350\n",
      "115400\n",
      "115450\n",
      "115500\n",
      "115550\n",
      "115600\n",
      "115650\n",
      "115700\n",
      "115750\n",
      "115800\n",
      "115850\n",
      "115900\n",
      "115950\n",
      "Epoch: 5/20...  Training Step: 18200...  Training loss: 1.4633...  0.0922 sec/batch\n",
      "116000\n",
      "116050\n",
      "116100\n",
      "116150\n",
      "116200\n",
      "116250\n",
      "116300\n",
      "116350\n",
      "116400\n",
      "116450\n",
      "116500\n",
      "116550\n",
      "116600\n",
      "116650\n",
      "116700\n",
      "116750\n",
      "116800\n",
      "116850\n",
      "116900\n",
      "116950\n",
      "117000\n",
      "117050\n",
      "117100\n",
      "117150\n",
      "117200\n",
      "117250\n",
      "117300\n",
      "117350\n",
      "117400\n",
      "117450\n",
      "117500\n",
      "117550\n",
      "117600\n",
      "117650\n",
      "117700\n",
      "117750\n",
      "117800\n",
      "117850\n",
      "117900\n",
      "117950\n",
      "118000\n",
      "118050\n",
      "118100\n",
      "118150\n",
      "118200\n",
      "118250\n",
      "118300\n",
      "118350\n",
      "118400\n",
      "118450\n",
      "Epoch: 5/20...  Training Step: 18250...  Training loss: 1.5270...  0.0852 sec/batch\n",
      "118500\n",
      "118550\n",
      "118600\n",
      "118650\n",
      "118700\n",
      "118750\n",
      "118800\n",
      "118850\n",
      "118900\n",
      "118950\n",
      "119000\n",
      "119050\n",
      "119100\n",
      "119150\n",
      "119200\n",
      "119250\n",
      "119300\n",
      "119350\n",
      "119400\n",
      "119450\n",
      "119500\n",
      "119550\n",
      "119600\n",
      "119650\n",
      "119700\n",
      "119750\n",
      "119800\n",
      "119850\n",
      "119900\n",
      "119950\n",
      "120000\n",
      "120050\n",
      "120100\n",
      "120150\n",
      "120200\n",
      "120250\n",
      "120300\n",
      "120350\n",
      "120400\n",
      "120450\n",
      "120500\n",
      "120550\n",
      "120600\n",
      "120650\n",
      "120700\n",
      "120750\n",
      "120800\n",
      "120850\n",
      "120900\n",
      "120950\n",
      "Epoch: 5/20...  Training Step: 18300...  Training loss: 1.7301...  0.1765 sec/batch\n",
      "121000\n",
      "121050\n",
      "121100\n",
      "121150\n",
      "121200\n",
      "121250\n",
      "121300\n",
      "121350\n",
      "121400\n",
      "121450\n",
      "121500\n",
      "121550\n",
      "121600\n",
      "121650\n",
      "121700\n",
      "121750\n",
      "121800\n",
      "121850\n",
      "121900\n",
      "121950\n",
      "122000\n",
      "122050\n",
      "122100\n",
      "122150\n",
      "122200\n",
      "122250\n",
      "122300\n",
      "122350\n",
      "122400\n",
      "122450\n",
      "122500\n",
      "122550\n",
      "122600\n",
      "122650\n",
      "122700\n",
      "122750\n",
      "122800\n",
      "122850\n",
      "122900\n",
      "122950\n",
      "123000\n",
      "123050\n",
      "123100\n",
      "123150\n",
      "123200\n",
      "123250\n",
      "123300\n",
      "123350\n",
      "123400\n",
      "123450\n",
      "Epoch: 5/20...  Training Step: 18350...  Training loss: 1.6019...  0.0800 sec/batch\n",
      "123500\n",
      "123550\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123600\n",
      "123650\n",
      "123700\n",
      "123750\n",
      "123800\n",
      "123850\n",
      "123900\n",
      "123950\n",
      "124000\n",
      "124050\n",
      "124100\n",
      "124150\n",
      "124200\n",
      "124250\n",
      "124300\n",
      "124350\n",
      "124400\n",
      "124450\n",
      "124500\n",
      "124550\n",
      "124600\n",
      "124650\n",
      "124700\n",
      "124750\n",
      "124800\n",
      "124850\n",
      "124900\n",
      "124950\n",
      "125000\n",
      "125050\n",
      "125100\n",
      "125150\n",
      "125200\n",
      "125250\n",
      "125300\n",
      "125350\n",
      "125400\n",
      "125450\n",
      "125500\n",
      "125550\n",
      "125600\n",
      "125650\n",
      "125700\n",
      "125750\n",
      "125800\n",
      "125850\n",
      "125900\n",
      "125950\n",
      "Epoch: 5/20...  Training Step: 18400...  Training loss: 1.6586...  0.0762 sec/batch\n",
      "126000\n",
      "126050\n",
      "126100\n",
      "126150\n",
      "126200\n",
      "126250\n",
      "126300\n",
      "126350\n",
      "126400\n",
      "126450\n",
      "126500\n",
      "126550\n",
      "126600\n",
      "126650\n",
      "126700\n",
      "126750\n",
      "126800\n",
      "126850\n",
      "126900\n",
      "126950\n",
      "127000\n",
      "127050\n",
      "127100\n",
      "127150\n",
      "127200\n",
      "127250\n",
      "127300\n",
      "127350\n",
      "127400\n",
      "127450\n",
      "127500\n",
      "127550\n",
      "127600\n",
      "127650\n",
      "127700\n",
      "127750\n",
      "127800\n",
      "127850\n",
      "127900\n",
      "127950\n",
      "128000\n",
      "128050\n",
      "128100\n",
      "128150\n",
      "128200\n",
      "128250\n",
      "128300\n",
      "128350\n",
      "128400\n",
      "128450\n",
      "Epoch: 5/20...  Training Step: 18450...  Training loss: 1.4510...  0.0874 sec/batch\n",
      "128500\n",
      "128550\n",
      "128600\n",
      "128650\n",
      "128700\n",
      "128750\n",
      "128800\n",
      "128850\n",
      "128900\n",
      "128950\n",
      "129000\n",
      "129050\n",
      "129100\n",
      "129150\n",
      "129200\n",
      "129250\n",
      "129300\n",
      "129350\n",
      "129400\n",
      "129450\n",
      "129500\n",
      "129550\n",
      "129600\n",
      "129650\n",
      "129700\n",
      "129750\n",
      "129800\n",
      "129850\n",
      "129900\n",
      "129950\n",
      "130000\n",
      "130050\n",
      "130100\n",
      "130150\n",
      "130200\n",
      "130250\n",
      "130300\n",
      "130350\n",
      "130400\n",
      "130450\n",
      "130500\n",
      "130550\n",
      "130600\n",
      "130650\n",
      "130700\n",
      "130750\n",
      "130800\n",
      "130850\n",
      "130900\n",
      "130950\n",
      "Epoch: 5/20...  Training Step: 18500...  Training loss: 1.4558...  0.1080 sec/batch\n",
      "131000\n",
      "131050\n",
      "131100\n",
      "131150\n",
      "131200\n",
      "131250\n",
      "131300\n",
      "131350\n",
      "131400\n",
      "131450\n",
      "131500\n",
      "131550\n",
      "131600\n",
      "131650\n",
      "131700\n",
      "131750\n",
      "131800\n",
      "131850\n",
      "131900\n",
      "131950\n",
      "132000\n",
      "132050\n",
      "132100\n",
      "132150\n",
      "132200\n",
      "132250\n",
      "132300\n",
      "132350\n",
      "132400\n",
      "132450\n",
      "132500\n",
      "132550\n",
      "132600\n",
      "132650\n",
      "132700\n",
      "132750\n",
      "132800\n",
      "132850\n",
      "132900\n",
      "132950\n",
      "133000\n",
      "133050\n",
      "133100\n",
      "133150\n",
      "133200\n",
      "133250\n",
      "133300\n",
      "133350\n",
      "133400\n",
      "133450\n",
      "Epoch: 5/20...  Training Step: 18550...  Training loss: 1.6426...  0.0865 sec/batch\n",
      "133500\n",
      "133550\n",
      "133600\n",
      "133650\n",
      "133700\n",
      "133750\n",
      "133800\n",
      "133850\n",
      "133900\n",
      "133950\n",
      "134000\n",
      "134050\n",
      "134100\n",
      "134150\n",
      "134200\n",
      "134250\n",
      "134300\n",
      "134350\n",
      "134400\n",
      "134450\n",
      "134500\n",
      "134550\n",
      "134600\n",
      "134650\n",
      "134700\n",
      "134750\n",
      "134800\n",
      "134850\n",
      "134900\n",
      "134950\n",
      "135000\n",
      "135050\n",
      "135100\n",
      "135150\n",
      "135200\n",
      "135250\n",
      "135300\n",
      "135350\n",
      "135400\n",
      "135450\n",
      "135500\n",
      "135550\n",
      "135600\n",
      "135650\n",
      "135700\n",
      "135750\n",
      "135800\n",
      "135850\n",
      "135900\n",
      "135950\n",
      "Epoch: 5/20...  Training Step: 18600...  Training loss: 1.4811...  0.0849 sec/batch\n",
      "136000\n",
      "136050\n",
      "136100\n",
      "136150\n",
      "136200\n",
      "136250\n",
      "136300\n",
      "136350\n",
      "136400\n",
      "136450\n",
      "136500\n",
      "136550\n",
      "136600\n",
      "136650\n",
      "136700\n",
      "136750\n",
      "136800\n",
      "136850\n",
      "136900\n",
      "136950\n",
      "137000\n",
      "137050\n",
      "137100\n",
      "137150\n",
      "137200\n",
      "137250\n",
      "137300\n",
      "137350\n",
      "137400\n",
      "137450\n",
      "137500\n",
      "137550\n",
      "137600\n",
      "137650\n",
      "137700\n",
      "137750\n",
      "137800\n",
      "137850\n",
      "137900\n",
      "137950\n",
      "138000\n",
      "138050\n",
      "138100\n",
      "138150\n",
      "138200\n",
      "138250\n",
      "138300\n",
      "138350\n",
      "138400\n",
      "138450\n",
      "Epoch: 5/20...  Training Step: 18650...  Training loss: 1.6287...  0.0968 sec/batch\n",
      "138500\n",
      "138550\n",
      "138600\n",
      "138650\n",
      "138700\n",
      "138750\n",
      "138800\n",
      "138850\n",
      "138900\n",
      "138950\n",
      "139000\n",
      "139050\n",
      "139100\n",
      "139150\n",
      "139200\n",
      "139250\n",
      "139300\n",
      "139350\n",
      "139400\n",
      "139450\n",
      "139500\n",
      "139550\n",
      "139600\n",
      "139650\n",
      "139700\n",
      "139750\n",
      "139800\n",
      "139850\n",
      "139900\n",
      "139950\n",
      "140000\n",
      "140050\n",
      "140100\n",
      "140150\n",
      "140200\n",
      "140250\n",
      "140300\n",
      "140350\n",
      "140400\n",
      "140450\n",
      "140500\n",
      "140550\n",
      "140600\n",
      "140650\n",
      "140700\n",
      "140750\n",
      "140800\n",
      "140850\n",
      "140900\n",
      "140950\n",
      "Epoch: 5/20...  Training Step: 18700...  Training loss: 1.6681...  0.0904 sec/batch\n",
      "141000\n",
      "141050\n",
      "141100\n",
      "141150\n",
      "141200\n",
      "141250\n",
      "141300\n",
      "141350\n",
      "141400\n",
      "141450\n",
      "141500\n",
      "141550\n",
      "141600\n",
      "141650\n",
      "141700\n",
      "141750\n",
      "141800\n",
      "141850\n",
      "141900\n",
      "141950\n",
      "142000\n",
      "142050\n",
      "142100\n",
      "142150\n",
      "142200\n",
      "142250\n",
      "142300\n",
      "142350\n",
      "142400\n",
      "142450\n",
      "142500\n",
      "142550\n",
      "142600\n",
      "142650\n",
      "142700\n",
      "142750\n",
      "142800\n",
      "142850\n",
      "142900\n",
      "142950\n",
      "143000\n",
      "143050\n",
      "143100\n",
      "143150\n",
      "143200\n",
      "143250\n",
      "143300\n",
      "143350\n",
      "143400\n",
      "143450\n",
      "Epoch: 5/20...  Training Step: 18750...  Training loss: 1.5309...  0.0967 sec/batch\n",
      "143500\n",
      "143550\n",
      "143600\n",
      "143650\n",
      "143700\n",
      "143750\n",
      "143800\n",
      "143850\n",
      "143900\n",
      "143950\n",
      "144000\n",
      "144050\n",
      "144100\n",
      "144150\n",
      "144200\n",
      "144250\n",
      "144300\n",
      "144350\n",
      "144400\n",
      "144450\n",
      "144500\n",
      "144550\n",
      "144600\n",
      "144650\n",
      "144700\n",
      "144750\n",
      "144800\n",
      "144850\n",
      "144900\n",
      "144950\n",
      "145000\n",
      "145050\n",
      "145100\n",
      "145150\n",
      "145200\n",
      "145250\n",
      "145300\n",
      "145350\n",
      "145400\n",
      "145450\n",
      "145500\n",
      "145550\n",
      "145600\n",
      "145650\n",
      "145700\n",
      "145750\n",
      "145800\n",
      "145850\n",
      "145900\n",
      "145950\n",
      "Epoch: 5/20...  Training Step: 18800...  Training loss: 1.6930...  0.0911 sec/batch\n",
      "146000\n",
      "146050\n",
      "146100\n",
      "146150\n",
      "146200\n",
      "146250\n",
      "146300\n",
      "146350\n",
      "146400\n",
      "146450\n",
      "146500\n",
      "146550\n",
      "146600\n",
      "146650\n",
      "146700\n",
      "146750\n",
      "146800\n",
      "146850\n",
      "146900\n",
      "146950\n",
      "147000\n",
      "147050\n",
      "147100\n",
      "147150\n",
      "147200\n",
      "147250\n",
      "147300\n",
      "147350\n",
      "147400\n",
      "147450\n",
      "147500\n",
      "147550\n",
      "147600\n",
      "147650\n",
      "147700\n",
      "147750\n",
      "147800\n",
      "147850\n",
      "147900\n",
      "147950\n",
      "148000\n",
      "148050\n",
      "148100\n",
      "148150\n",
      "148200\n",
      "148250\n",
      "148300\n",
      "148350\n",
      "148400\n",
      "148450\n",
      "Epoch: 5/20...  Training Step: 18850...  Training loss: 1.5035...  0.0790 sec/batch\n",
      "148500\n",
      "148550\n",
      "148600\n",
      "148650\n",
      "148700\n",
      "148750\n",
      "148800\n",
      "148850\n",
      "148900\n",
      "148950\n",
      "149000\n",
      "149050\n",
      "149100\n",
      "149150\n",
      "149200\n",
      "149250\n",
      "149300\n",
      "149350\n",
      "149400\n",
      "149450\n",
      "149500\n",
      "149550\n",
      "149600\n",
      "149650\n",
      "149700\n",
      "149750\n",
      "149800\n",
      "149850\n",
      "149900\n",
      "149950\n",
      "150000\n",
      "150050\n",
      "150100\n",
      "150150\n",
      "150200\n",
      "150250\n",
      "150300\n",
      "150350\n",
      "150400\n",
      "150450\n",
      "150500\n",
      "150550\n",
      "150600\n",
      "150650\n",
      "150700\n",
      "150750\n",
      "150800\n",
      "150850\n",
      "150900\n",
      "150950\n",
      "Epoch: 5/20...  Training Step: 18900...  Training loss: 1.6834...  0.0812 sec/batch\n",
      "151000\n",
      "151050\n",
      "151100\n",
      "151150\n",
      "151200\n",
      "151250\n",
      "151300\n",
      "151350\n",
      "151400\n",
      "151450\n",
      "151500\n",
      "151550\n",
      "151600\n",
      "151650\n",
      "151700\n",
      "151750\n",
      "151800\n",
      "151850\n",
      "151900\n",
      "151950\n",
      "152000\n",
      "152050\n",
      "152100\n",
      "152150\n",
      "152200\n",
      "152250\n",
      "152300\n",
      "152350\n",
      "152400\n",
      "152450\n",
      "152500\n",
      "152550\n",
      "152600\n",
      "152650\n",
      "152700\n",
      "152750\n",
      "152800\n",
      "152850\n",
      "152900\n",
      "152950\n",
      "153000\n",
      "153050\n",
      "153100\n",
      "153150\n",
      "153200\n",
      "153250\n",
      "153300\n",
      "153350\n",
      "153400\n",
      "153450\n",
      "Epoch: 5/20...  Training Step: 18950...  Training loss: 1.5023...  0.0860 sec/batch\n",
      "153500\n",
      "153550\n",
      "153600\n",
      "153650\n",
      "153700\n",
      "153750\n",
      "153800\n",
      "153850\n",
      "153900\n",
      "153950\n",
      "154000\n",
      "154050\n",
      "154100\n",
      "154150\n",
      "154200\n",
      "154250\n",
      "154300\n",
      "154350\n",
      "154400\n",
      "154450\n",
      "154500\n",
      "154550\n",
      "154600\n",
      "154650\n",
      "154700\n",
      "154750\n",
      "154800\n",
      "154850\n",
      "154900\n",
      "154950\n",
      "155000\n",
      "155050\n",
      "155100\n",
      "155150\n",
      "155200\n",
      "155250\n",
      "155300\n",
      "155350\n",
      "155400\n",
      "155450\n",
      "155500\n",
      "155550\n",
      "155600\n",
      "155650\n",
      "155700\n",
      "155750\n",
      "155800\n",
      "155850\n",
      "155900\n",
      "155950\n",
      "Epoch: 5/20...  Training Step: 19000...  Training loss: 1.4588...  0.0866 sec/batch\n",
      "156000\n",
      "156050\n",
      "156100\n",
      "156150\n",
      "156200\n",
      "156250\n",
      "156300\n",
      "156350\n",
      "156400\n",
      "156450\n",
      "156500\n",
      "156550\n",
      "156600\n",
      "156650\n",
      "156700\n",
      "156750\n",
      "156800\n",
      "156850\n",
      "156900\n",
      "156950\n",
      "157000\n",
      "157050\n",
      "157100\n",
      "157150\n",
      "157200\n",
      "157250\n",
      "157300\n",
      "157350\n",
      "157400\n",
      "157450\n",
      "157500\n",
      "157550\n",
      "157600\n",
      "157650\n",
      "157700\n",
      "157750\n",
      "157800\n",
      "157850\n",
      "157900\n",
      "157950\n",
      "158000\n",
      "158050\n",
      "158100\n",
      "158150\n",
      "158200\n",
      "158250\n",
      "158300\n",
      "158350\n",
      "158400\n",
      "158450\n",
      "Epoch: 5/20...  Training Step: 19050...  Training loss: 1.6099...  0.0772 sec/batch\n",
      "158500\n",
      "158550\n",
      "158600\n",
      "158650\n",
      "158700\n",
      "158750\n",
      "158800\n",
      "158850\n",
      "158900\n",
      "158950\n",
      "159000\n",
      "159050\n",
      "159100\n",
      "159150\n",
      "159200\n",
      "159250\n",
      "159300\n",
      "159350\n",
      "159400\n",
      "159450\n",
      "159500\n",
      "159550\n",
      "159600\n",
      "159650\n",
      "159700\n",
      "159750\n",
      "159800\n",
      "159850\n",
      "159900\n",
      "159950\n",
      "160000\n",
      "160050\n",
      "160100\n",
      "160150\n",
      "160200\n",
      "160250\n",
      "160300\n",
      "160350\n",
      "160400\n",
      "160450\n",
      "160500\n",
      "160550\n",
      "160600\n",
      "160650\n",
      "160700\n",
      "160750\n",
      "160800\n",
      "160850\n",
      "160900\n",
      "160950\n",
      "Epoch: 5/20...  Training Step: 19100...  Training loss: 1.5966...  0.0822 sec/batch\n",
      "161000\n",
      "161050\n",
      "161100\n",
      "161150\n",
      "161200\n",
      "161250\n",
      "161300\n",
      "161350\n",
      "161400\n",
      "161450\n",
      "161500\n",
      "161550\n",
      "161600\n",
      "161650\n",
      "161700\n",
      "161750\n",
      "161800\n",
      "161850\n",
      "161900\n",
      "161950\n",
      "162000\n",
      "162050\n",
      "162100\n",
      "162150\n",
      "162200\n",
      "162250\n",
      "162300\n",
      "162350\n",
      "162400\n",
      "162450\n",
      "162500\n",
      "162550\n",
      "162600\n",
      "162650\n",
      "162700\n",
      "162750\n",
      "162800\n",
      "162850\n",
      "162900\n",
      "162950\n",
      "163000\n",
      "163050\n",
      "163100\n",
      "163150\n",
      "163200\n",
      "163250\n",
      "163300\n",
      "163350\n",
      "163400\n",
      "163450\n",
      "Epoch: 5/20...  Training Step: 19150...  Training loss: 1.6533...  0.0780 sec/batch\n",
      "163500\n",
      "163550\n",
      "163600\n",
      "163650\n",
      "163700\n",
      "163750\n",
      "163800\n",
      "163850\n",
      "163900\n",
      "163950\n",
      "164000\n",
      "164050\n",
      "164100\n",
      "164150\n",
      "164200\n",
      "164250\n",
      "164300\n",
      "164350\n",
      "164400\n",
      "164450\n",
      "164500\n",
      "164550\n",
      "164600\n",
      "164650\n",
      "164700\n",
      "164750\n",
      "164800\n",
      "164850\n",
      "164900\n",
      "164950\n",
      "165000\n",
      "165050\n",
      "165100\n",
      "165150\n",
      "165200\n",
      "165250\n",
      "165300\n",
      "165350\n",
      "165400\n",
      "165450\n",
      "165500\n",
      "165550\n",
      "165600\n",
      "165650\n",
      "165700\n",
      "165750\n",
      "165800\n",
      "165850\n",
      "165900\n",
      "165950\n",
      "Epoch: 5/20...  Training Step: 19200...  Training loss: 1.5562...  0.0912 sec/batch\n",
      "166000\n",
      "166050\n",
      "166100\n",
      "166150\n",
      "166200\n",
      "166250\n",
      "166300\n",
      "166350\n",
      "166400\n",
      "166450\n",
      "166500\n",
      "166550\n",
      "166600\n",
      "166650\n",
      "166700\n",
      "166750\n",
      "166800\n",
      "166850\n",
      "166900\n",
      "166950\n",
      "167000\n",
      "167050\n",
      "167100\n",
      "167150\n",
      "167200\n",
      "167250\n",
      "167300\n",
      "167350\n",
      "167400\n",
      "167450\n",
      "167500\n",
      "167550\n",
      "167600\n",
      "167650\n",
      "167700\n",
      "167750\n",
      "167800\n",
      "167850\n",
      "167900\n",
      "167950\n",
      "168000\n",
      "168050\n",
      "168100\n",
      "168150\n",
      "168200\n",
      "168250\n",
      "168300\n",
      "168350\n",
      "168400\n",
      "168450\n",
      "Epoch: 5/20...  Training Step: 19250...  Training loss: 1.5077...  0.0799 sec/batch\n",
      "168500\n",
      "168550\n",
      "168600\n",
      "168650\n",
      "168700\n",
      "168750\n",
      "168800\n",
      "168850\n",
      "168900\n",
      "168950\n",
      "169000\n",
      "169050\n",
      "169100\n",
      "169150\n",
      "169200\n",
      "169250\n",
      "169300\n",
      "169350\n",
      "169400\n",
      "169450\n",
      "169500\n",
      "169550\n",
      "169600\n",
      "169650\n",
      "169700\n",
      "169750\n",
      "169800\n",
      "169850\n",
      "169900\n",
      "169950\n",
      "170000\n",
      "170050\n",
      "170100\n",
      "170150\n",
      "170200\n",
      "170250\n",
      "170300\n",
      "170350\n",
      "170400\n",
      "170450\n",
      "170500\n",
      "170550\n",
      "170600\n",
      "170650\n",
      "170700\n",
      "170750\n",
      "170800\n",
      "170850\n",
      "170900\n",
      "170950\n",
      "Epoch: 5/20...  Training Step: 19300...  Training loss: 1.7175...  0.0757 sec/batch\n",
      "171000\n",
      "171050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "171100\n",
      "171150\n",
      "171200\n",
      "171250\n",
      "171300\n",
      "171350\n",
      "171400\n",
      "171450\n",
      "171500\n",
      "171550\n",
      "171600\n",
      "171650\n",
      "171700\n",
      "171750\n",
      "171800\n",
      "171850\n",
      "171900\n",
      "171950\n",
      "172000\n",
      "172050\n",
      "172100\n",
      "172150\n",
      "172200\n",
      "172250\n",
      "172300\n",
      "172350\n",
      "172400\n",
      "172450\n",
      "172500\n",
      "172550\n",
      "172600\n",
      "172650\n",
      "172700\n",
      "172750\n",
      "172800\n",
      "172850\n",
      "172900\n",
      "172950\n",
      "173000\n",
      "173050\n",
      "173100\n",
      "173150\n",
      "173200\n",
      "173250\n",
      "173300\n",
      "173350\n",
      "173400\n",
      "173450\n",
      "Epoch: 5/20...  Training Step: 19350...  Training loss: 1.5752...  0.0828 sec/batch\n",
      "173500\n",
      "173550\n",
      "173600\n",
      "173650\n",
      "173700\n",
      "173750\n",
      "173800\n",
      "173850\n",
      "173900\n",
      "173950\n",
      "174000\n",
      "174050\n",
      "174100\n",
      "174150\n",
      "174200\n",
      "174250\n",
      "174300\n",
      "174350\n",
      "174400\n",
      "174450\n",
      "174500\n",
      "174550\n",
      "174600\n",
      "174650\n",
      "174700\n",
      "174750\n",
      "174800\n",
      "174850\n",
      "174900\n",
      "174950\n",
      "175000\n",
      "175050\n",
      "175100\n",
      "175150\n",
      "175200\n",
      "175250\n",
      "175300\n",
      "175350\n",
      "175400\n",
      "175450\n",
      "175500\n",
      "175550\n",
      "175600\n",
      "175650\n",
      "175700\n",
      "175750\n",
      "175800\n",
      "175850\n",
      "175900\n",
      "175950\n",
      "Epoch: 5/20...  Training Step: 19400...  Training loss: 1.5748...  0.0853 sec/batch\n",
      "176000\n",
      "176050\n",
      "176100\n",
      "176150\n",
      "176200\n",
      "176250\n",
      "176300\n",
      "176350\n",
      "176400\n",
      "176450\n",
      "176500\n",
      "176550\n",
      "176600\n",
      "176650\n",
      "176700\n",
      "176750\n",
      "176800\n",
      "176850\n",
      "176900\n",
      "176950\n",
      "177000\n",
      "177050\n",
      "177100\n",
      "177150\n",
      "177200\n",
      "177250\n",
      "177300\n",
      "177350\n",
      "177400\n",
      "177450\n",
      "177500\n",
      "177550\n",
      "177600\n",
      "177650\n",
      "177700\n",
      "177750\n",
      "177800\n",
      "177850\n",
      "177900\n",
      "177950\n",
      "178000\n",
      "178050\n",
      "178100\n",
      "178150\n",
      "178200\n",
      "178250\n",
      "178300\n",
      "178350\n",
      "178400\n",
      "178450\n",
      "Epoch: 5/20...  Training Step: 19450...  Training loss: 1.3264...  0.0752 sec/batch\n",
      "178500\n",
      "178550\n",
      "178600\n",
      "178650\n",
      "178700\n",
      "178750\n",
      "178800\n",
      "178850\n",
      "178900\n",
      "178950\n",
      "179000\n",
      "179050\n",
      "179100\n",
      "179150\n",
      "179200\n",
      "179250\n",
      "179300\n",
      "179350\n",
      "179400\n",
      "179450\n",
      "179500\n",
      "179550\n",
      "179600\n",
      "179650\n",
      "179700\n",
      "179750\n",
      "179800\n",
      "179850\n",
      "179900\n",
      "179950\n",
      "180000\n",
      "180050\n",
      "180100\n",
      "180150\n",
      "180200\n",
      "180250\n",
      "180300\n",
      "180350\n",
      "180400\n",
      "180450\n",
      "180500\n",
      "180550\n",
      "180600\n",
      "180650\n",
      "180700\n",
      "180750\n",
      "180800\n",
      "180850\n",
      "180900\n",
      "180950\n",
      "Epoch: 5/20...  Training Step: 19500...  Training loss: 1.4827...  0.1085 sec/batch\n",
      "181000\n",
      "181050\n",
      "181100\n",
      "181150\n",
      "181200\n",
      "181250\n",
      "181300\n",
      "181350\n",
      "181400\n",
      "181450\n",
      "181500\n",
      "181550\n",
      "181600\n",
      "181650\n",
      "181700\n",
      "181750\n",
      "181800\n",
      "181850\n",
      "181900\n",
      "181950\n",
      "182000\n",
      "182050\n",
      "182100\n",
      "182150\n",
      "182200\n",
      "182250\n",
      "182300\n",
      "182350\n",
      "182400\n",
      "182450\n",
      "182500\n",
      "182550\n",
      "182600\n",
      "182650\n",
      "182700\n",
      "182750\n",
      "182800\n",
      "182850\n",
      "182900\n",
      "182950\n",
      "183000\n",
      "183050\n",
      "183100\n",
      "183150\n",
      "183200\n",
      "183250\n",
      "183300\n",
      "183350\n",
      "183400\n",
      "183450\n",
      "Epoch: 5/20...  Training Step: 19550...  Training loss: 1.6026...  0.0882 sec/batch\n",
      "183500\n",
      "183550\n",
      "183600\n",
      "183650\n",
      "183700\n",
      "183750\n",
      "183800\n",
      "183850\n",
      "183900\n",
      "183950\n",
      "184000\n",
      "184050\n",
      "184100\n",
      "184150\n",
      "184200\n",
      "184250\n",
      "184300\n",
      "184350\n",
      "184400\n",
      "184450\n",
      "184500\n",
      "184550\n",
      "184600\n",
      "184650\n",
      "184700\n",
      "184750\n",
      "184800\n",
      "184850\n",
      "184900\n",
      "184950\n",
      "185000\n",
      "185050\n",
      "185100\n",
      "185150\n",
      "185200\n",
      "185250\n",
      "185300\n",
      "185350\n",
      "185400\n",
      "185450\n",
      "185500\n",
      "185550\n",
      "185600\n",
      "185650\n",
      "185700\n",
      "185750\n",
      "185800\n",
      "185850\n",
      "185900\n",
      "185950\n",
      "Epoch: 5/20...  Training Step: 19600...  Training loss: 1.6101...  0.0798 sec/batch\n",
      "186000\n",
      "186050\n",
      "186100\n",
      "186150\n",
      "186200\n",
      "186250\n",
      "186300\n",
      "186350\n",
      "186400\n",
      "186450\n",
      "186500\n",
      "186550\n",
      "186600\n",
      "186650\n",
      "186700\n",
      "186750\n",
      "186800\n",
      "186850\n",
      "186900\n",
      "186950\n",
      "187000\n",
      "187050\n",
      "187100\n",
      "187150\n",
      "187200\n",
      "187250\n",
      "187300\n",
      "187350\n",
      "187400\n",
      "187450\n",
      "187500\n",
      "187550\n",
      "187600\n",
      "187650\n",
      "187700\n",
      "187750\n",
      "187800\n",
      "187850\n",
      "187900\n",
      "187950\n",
      "188000\n",
      "188050\n",
      "188100\n",
      "188150\n",
      "188200\n",
      "188250\n",
      "188300\n",
      "188350\n",
      "188400\n",
      "188450\n",
      "Epoch: 5/20...  Training Step: 19650...  Training loss: 1.6838...  0.0855 sec/batch\n",
      "188500\n",
      "188550\n",
      "188600\n",
      "188650\n",
      "188700\n",
      "188750\n",
      "188800\n",
      "188850\n",
      "188900\n",
      "188950\n",
      "189000\n",
      "189050\n",
      "189100\n",
      "189150\n",
      "189200\n",
      "189250\n",
      "189300\n",
      "189350\n",
      "189400\n",
      "189450\n",
      "189500\n",
      "189550\n",
      "189600\n",
      "189650\n",
      "189700\n",
      "189750\n",
      "189800\n",
      "189850\n",
      "189900\n",
      "189950\n",
      "190000\n",
      "190050\n",
      "190100\n",
      "190150\n",
      "190200\n",
      "190250\n",
      "190300\n",
      "190350\n",
      "190400\n",
      "190450\n",
      "190500\n",
      "190550\n",
      "190600\n",
      "190650\n",
      "190700\n",
      "190750\n",
      "190800\n",
      "190850\n",
      "190900\n",
      "190950\n",
      "Epoch: 5/20...  Training Step: 19700...  Training loss: 1.8931...  0.1249 sec/batch\n",
      "191000\n",
      "191050\n",
      "191100\n",
      "191150\n",
      "191200\n",
      "191250\n",
      "191300\n",
      "191350\n",
      "191400\n",
      "191450\n",
      "191500\n",
      "191550\n",
      "191600\n",
      "191650\n",
      "191700\n",
      "191750\n",
      "191800\n",
      "191850\n",
      "191900\n",
      "191950\n",
      "192000\n",
      "192050\n",
      "192100\n",
      "192150\n",
      "192200\n",
      "192250\n",
      "192300\n",
      "192350\n",
      "192400\n",
      "192450\n",
      "192500\n",
      "192550\n",
      "192600\n",
      "192650\n",
      "192700\n",
      "192750\n",
      "192800\n",
      "192850\n",
      "192900\n",
      "192950\n",
      "193000\n",
      "193050\n",
      "193100\n",
      "193150\n",
      "193200\n",
      "193250\n",
      "193300\n",
      "193350\n",
      "193400\n",
      "193450\n",
      "Epoch: 5/20...  Training Step: 19750...  Training loss: 1.6839...  0.1188 sec/batch\n",
      "193500\n",
      "193550\n",
      "193600\n",
      "193650\n",
      "193700\n",
      "193750\n",
      "193800\n",
      "193850\n",
      "193900\n",
      "193950\n",
      "194000\n",
      "194050\n",
      "194100\n",
      "194150\n",
      "194200\n",
      "194250\n",
      "194300\n",
      "194350\n",
      "194400\n",
      "194450\n",
      "194500\n",
      "194550\n",
      "194600\n",
      "194650\n",
      "194700\n",
      "194750\n",
      "194800\n",
      "194850\n",
      "194900\n",
      "194950\n",
      "195000\n",
      "195050\n",
      "195100\n",
      "195150\n",
      "195200\n",
      "195250\n",
      "195300\n",
      "195350\n",
      "195400\n",
      "195450\n",
      "195500\n",
      "195550\n",
      "195600\n",
      "195650\n",
      "195700\n",
      "195750\n",
      "195800\n",
      "195850\n",
      "195900\n",
      "195950\n",
      "Epoch: 5/20...  Training Step: 19800...  Training loss: 1.8576...  0.0912 sec/batch\n",
      "196000\n",
      "196050\n",
      "196100\n",
      "196150\n",
      "196200\n",
      "196250\n",
      "196300\n",
      "196350\n",
      "196400\n",
      "196450\n",
      "196500\n",
      "196550\n",
      "196600\n",
      "196650\n",
      "196700\n",
      "196750\n",
      "196800\n",
      "196850\n",
      "196900\n",
      "196950\n",
      "197000\n",
      "197050\n",
      "197100\n",
      "197150\n",
      "197200\n",
      "197250\n",
      "197300\n",
      "197350\n",
      "197400\n",
      "197450\n",
      "197500\n",
      "197550\n",
      "197600\n",
      "197650\n",
      "197700\n",
      "197750\n",
      "197800\n",
      "197850\n",
      "197900\n",
      "197950\n",
      "198000\n",
      "198050\n",
      "198100\n",
      "198150\n",
      "198200\n",
      "198250\n",
      "198300\n",
      "198350\n",
      "198400\n",
      "198450\n",
      "Epoch: 5/20...  Training Step: 19850...  Training loss: 1.8686...  0.0932 sec/batch\n",
      "3970\n",
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "800\n",
      "850\n",
      "900\n",
      "950\n",
      "1000\n",
      "1050\n",
      "1100\n",
      "1150\n",
      "1200\n",
      "1250\n",
      "1300\n",
      "1350\n",
      "1400\n",
      "1450\n",
      "1500\n",
      "1550\n",
      "1600\n",
      "1650\n",
      "1700\n",
      "1750\n",
      "1800\n",
      "1850\n",
      "1900\n",
      "1950\n",
      "2000\n",
      "2050\n",
      "2100\n",
      "2150\n",
      "2200\n",
      "2250\n",
      "2300\n",
      "2350\n",
      "2400\n",
      "2450\n",
      "Epoch: 6/20...  Training Step: 19900...  Training loss: 1.7943...  0.0883 sec/batch\n",
      "2500\n",
      "2550\n",
      "2600\n",
      "2650\n",
      "2700\n",
      "2750\n",
      "2800\n",
      "2850\n",
      "2900\n",
      "2950\n",
      "3000\n",
      "3050\n",
      "3100\n",
      "3150\n",
      "3200\n",
      "3250\n",
      "3300\n",
      "3350\n",
      "3400\n",
      "3450\n",
      "3500\n",
      "3550\n",
      "3600\n",
      "3650\n",
      "3700\n",
      "3750\n",
      "3800\n",
      "3850\n",
      "3900\n",
      "3950\n",
      "4000\n",
      "4050\n",
      "4100\n",
      "4150\n",
      "4200\n",
      "4250\n",
      "4300\n",
      "4350\n",
      "4400\n",
      "4450\n",
      "4500\n",
      "4550\n",
      "4600\n",
      "4650\n",
      "4700\n",
      "4750\n",
      "4800\n",
      "4850\n",
      "4900\n",
      "4950\n",
      "Epoch: 6/20...  Training Step: 19950...  Training loss: 1.5170...  0.0792 sec/batch\n",
      "5000\n",
      "5050\n",
      "5100\n",
      "5150\n",
      "5200\n",
      "5250\n",
      "5300\n",
      "5350\n",
      "5400\n",
      "5450\n",
      "5500\n",
      "5550\n",
      "5600\n",
      "5650\n",
      "5700\n",
      "5750\n",
      "5800\n",
      "5850\n",
      "5900\n",
      "5950\n",
      "6000\n",
      "6050\n",
      "6100\n",
      "6150\n",
      "6200\n",
      "6250\n",
      "6300\n",
      "6350\n",
      "6400\n",
      "6450\n",
      "6500\n",
      "6550\n",
      "6600\n",
      "6650\n",
      "6700\n",
      "6750\n",
      "6800\n",
      "6850\n",
      "6900\n",
      "6950\n",
      "7000\n",
      "7050\n",
      "7100\n",
      "7150\n",
      "7200\n",
      "7250\n",
      "7300\n",
      "7350\n",
      "7400\n",
      "7450\n",
      "Epoch: 6/20...  Training Step: 20000...  Training loss: 1.4149...  0.0840 sec/batch\n",
      "7500\n",
      "7550\n",
      "7600\n",
      "7650\n",
      "7700\n",
      "7750\n",
      "7800\n",
      "7850\n",
      "7900\n",
      "7950\n",
      "8000\n",
      "8050\n",
      "8100\n",
      "8150\n",
      "8200\n",
      "8250\n",
      "8300\n",
      "8350\n",
      "8400\n",
      "8450\n",
      "8500\n",
      "8550\n",
      "8600\n",
      "8650\n",
      "8700\n",
      "8750\n",
      "8800\n",
      "8850\n",
      "8900\n",
      "8950\n",
      "9000\n",
      "9050\n",
      "9100\n",
      "9150\n",
      "9200\n",
      "9250\n",
      "9300\n",
      "9350\n",
      "9400\n",
      "9450\n",
      "9500\n",
      "9550\n",
      "9600\n",
      "9650\n",
      "9700\n",
      "9750\n",
      "9800\n",
      "9850\n",
      "9900\n",
      "9950\n",
      "Epoch: 6/20...  Training Step: 20050...  Training loss: 1.5853...  0.0747 sec/batch\n",
      "10000\n",
      "10050\n",
      "10100\n",
      "10150\n",
      "10200\n",
      "10250\n",
      "10300\n",
      "10350\n",
      "10400\n",
      "10450\n",
      "10500\n",
      "10550\n",
      "10600\n",
      "10650\n",
      "10700\n",
      "10750\n",
      "10800\n",
      "10850\n",
      "10900\n",
      "10950\n",
      "11000\n",
      "11050\n",
      "11100\n",
      "11150\n",
      "11200\n",
      "11250\n",
      "11300\n",
      "11350\n",
      "11400\n",
      "11450\n",
      "11500\n",
      "11550\n",
      "11600\n",
      "11650\n",
      "11700\n",
      "11750\n",
      "11800\n",
      "11850\n",
      "11900\n",
      "11950\n",
      "12000\n",
      "12050\n",
      "12100\n",
      "12150\n",
      "12200\n",
      "12250\n",
      "12300\n",
      "12350\n",
      "12400\n",
      "12450\n",
      "Epoch: 6/20...  Training Step: 20100...  Training loss: 1.5099...  0.0862 sec/batch\n",
      "12500\n",
      "12550\n",
      "12600\n",
      "12650\n",
      "12700\n",
      "12750\n",
      "12800\n",
      "12850\n",
      "12900\n",
      "12950\n",
      "13000\n",
      "13050\n",
      "13100\n",
      "13150\n",
      "13200\n",
      "13250\n",
      "13300\n",
      "13350\n",
      "13400\n",
      "13450\n",
      "13500\n",
      "13550\n",
      "13600\n",
      "13650\n",
      "13700\n",
      "13750\n",
      "13800\n",
      "13850\n",
      "13900\n",
      "13950\n",
      "14000\n",
      "14050\n",
      "14100\n",
      "14150\n",
      "14200\n",
      "14250\n",
      "14300\n",
      "14350\n",
      "14400\n",
      "14450\n",
      "14500\n",
      "14550\n",
      "14600\n",
      "14650\n",
      "14700\n",
      "14750\n",
      "14800\n",
      "14850\n",
      "14900\n",
      "14950\n",
      "Epoch: 6/20...  Training Step: 20150...  Training loss: 1.4408...  0.0816 sec/batch\n",
      "15000\n",
      "15050\n",
      "15100\n",
      "15150\n",
      "15200\n",
      "15250\n",
      "15300\n",
      "15350\n",
      "15400\n",
      "15450\n",
      "15500\n",
      "15550\n",
      "15600\n",
      "15650\n",
      "15700\n",
      "15750\n",
      "15800\n",
      "15850\n",
      "15900\n",
      "15950\n",
      "16000\n",
      "16050\n",
      "16100\n",
      "16150\n",
      "16200\n",
      "16250\n",
      "16300\n",
      "16350\n",
      "16400\n",
      "16450\n",
      "16500\n",
      "16550\n",
      "16600\n",
      "16650\n",
      "16700\n",
      "16750\n",
      "16800\n",
      "16850\n",
      "16900\n",
      "16950\n",
      "17000\n",
      "17050\n",
      "17100\n",
      "17150\n",
      "17200\n",
      "17250\n",
      "17300\n",
      "17350\n",
      "17400\n",
      "17450\n",
      "Epoch: 6/20...  Training Step: 20200...  Training loss: 1.5818...  0.0805 sec/batch\n",
      "17500\n",
      "17550\n",
      "17600\n",
      "17650\n",
      "17700\n",
      "17750\n",
      "17800\n",
      "17850\n",
      "17900\n",
      "17950\n",
      "18000\n",
      "18050\n",
      "18100\n",
      "18150\n",
      "18200\n",
      "18250\n",
      "18300\n",
      "18350\n",
      "18400\n",
      "18450\n",
      "18500\n",
      "18550\n",
      "18600\n",
      "18650\n",
      "18700\n",
      "18750\n",
      "18800\n",
      "18850\n",
      "18900\n",
      "18950\n",
      "19000\n",
      "19050\n",
      "19100\n",
      "19150\n",
      "19200\n",
      "19250\n",
      "19300\n",
      "19350\n",
      "19400\n",
      "19450\n",
      "19500\n",
      "19550\n",
      "19600\n",
      "19650\n",
      "19700\n",
      "19750\n",
      "19800\n",
      "19850\n",
      "19900\n",
      "19950\n",
      "Epoch: 6/20...  Training Step: 20250...  Training loss: 1.6130...  0.0885 sec/batch\n",
      "20000\n",
      "20050\n",
      "20100\n",
      "20150\n",
      "20200\n",
      "20250\n",
      "20300\n",
      "20350\n",
      "20400\n",
      "20450\n",
      "20500\n",
      "20550\n",
      "20600\n",
      "20650\n",
      "20700\n",
      "20750\n",
      "20800\n",
      "20850\n",
      "20900\n",
      "20950\n",
      "21000\n",
      "21050\n",
      "21100\n",
      "21150\n",
      "21200\n",
      "21250\n",
      "21300\n",
      "21350\n",
      "21400\n",
      "21450\n",
      "21500\n",
      "21550\n",
      "21600\n",
      "21650\n",
      "21700\n",
      "21750\n",
      "21800\n",
      "21850\n",
      "21900\n",
      "21950\n",
      "22000\n",
      "22050\n",
      "22100\n",
      "22150\n",
      "22200\n",
      "22250\n",
      "22300\n",
      "22350\n",
      "22400\n",
      "22450\n",
      "Epoch: 6/20...  Training Step: 20300...  Training loss: 1.6305...  0.0850 sec/batch\n",
      "22500\n",
      "22550\n",
      "22600\n",
      "22650\n",
      "22700\n",
      "22750\n",
      "22800\n",
      "22850\n",
      "22900\n",
      "22950\n",
      "23000\n",
      "23050\n",
      "23100\n",
      "23150\n",
      "23200\n",
      "23250\n",
      "23300\n",
      "23350\n",
      "23400\n",
      "23450\n",
      "23500\n",
      "23550\n",
      "23600\n",
      "23650\n",
      "23700\n",
      "23750\n",
      "23800\n",
      "23850\n",
      "23900\n",
      "23950\n",
      "24000\n",
      "24050\n",
      "24100\n",
      "24150\n",
      "24200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24250\n",
      "24300\n",
      "24350\n",
      "24400\n",
      "24450\n",
      "24500\n",
      "24550\n",
      "24600\n",
      "24650\n",
      "24700\n",
      "24750\n",
      "24800\n",
      "24850\n",
      "24900\n",
      "24950\n",
      "Epoch: 6/20...  Training Step: 20350...  Training loss: 1.7231...  0.0826 sec/batch\n",
      "25000\n",
      "25050\n",
      "25100\n",
      "25150\n",
      "25200\n",
      "25250\n",
      "25300\n",
      "25350\n",
      "25400\n",
      "25450\n",
      "25500\n",
      "25550\n",
      "25600\n",
      "25650\n",
      "25700\n",
      "25750\n",
      "25800\n",
      "25850\n",
      "25900\n",
      "25950\n",
      "26000\n",
      "26050\n",
      "26100\n",
      "26150\n",
      "26200\n",
      "26250\n",
      "26300\n",
      "26350\n",
      "26400\n",
      "26450\n",
      "26500\n",
      "26550\n",
      "26600\n",
      "26650\n",
      "26700\n",
      "26750\n",
      "26800\n",
      "26850\n",
      "26900\n",
      "26950\n",
      "27000\n",
      "27050\n",
      "27100\n",
      "27150\n",
      "27200\n",
      "27250\n",
      "27300\n",
      "27350\n",
      "27400\n",
      "27450\n",
      "Epoch: 6/20...  Training Step: 20400...  Training loss: 1.4277...  0.0832 sec/batch\n",
      "27500\n",
      "27550\n",
      "27600\n",
      "27650\n",
      "27700\n",
      "27750\n",
      "27800\n",
      "27850\n",
      "27900\n",
      "27950\n",
      "28000\n",
      "28050\n",
      "28100\n",
      "28150\n",
      "28200\n",
      "28250\n",
      "28300\n",
      "28350\n",
      "28400\n",
      "28450\n",
      "28500\n",
      "28550\n",
      "28600\n",
      "28650\n",
      "28700\n",
      "28750\n",
      "28800\n",
      "28850\n",
      "28900\n",
      "28950\n",
      "29000\n",
      "29050\n",
      "29100\n",
      "29150\n",
      "29200\n",
      "29250\n",
      "29300\n",
      "29350\n",
      "29400\n",
      "29450\n",
      "29500\n",
      "29550\n",
      "29600\n",
      "29650\n",
      "29700\n",
      "29750\n",
      "29800\n",
      "29850\n",
      "29900\n",
      "29950\n",
      "Epoch: 6/20...  Training Step: 20450...  Training loss: 1.5011...  0.0788 sec/batch\n",
      "30000\n",
      "30050\n",
      "30100\n",
      "30150\n",
      "30200\n",
      "30250\n",
      "30300\n",
      "30350\n",
      "30400\n",
      "30450\n",
      "30500\n",
      "30550\n",
      "30600\n",
      "30650\n",
      "30700\n",
      "30750\n",
      "30800\n",
      "30850\n",
      "30900\n",
      "30950\n",
      "31000\n",
      "31050\n",
      "31100\n",
      "31150\n",
      "31200\n",
      "31250\n",
      "31300\n",
      "31350\n",
      "31400\n",
      "31450\n",
      "31500\n",
      "31550\n",
      "31600\n",
      "31650\n",
      "31700\n",
      "31750\n",
      "31800\n",
      "31850\n",
      "31900\n",
      "31950\n",
      "32000\n",
      "32050\n",
      "32100\n",
      "32150\n",
      "32200\n",
      "32250\n",
      "32300\n",
      "32350\n",
      "32400\n",
      "32450\n",
      "Epoch: 6/20...  Training Step: 20500...  Training loss: 1.4244...  0.1003 sec/batch\n",
      "32500\n",
      "32550\n",
      "32600\n",
      "32650\n",
      "32700\n",
      "32750\n",
      "32800\n",
      "32850\n",
      "32900\n",
      "32950\n",
      "33000\n",
      "33050\n",
      "33100\n",
      "33150\n",
      "33200\n",
      "33250\n",
      "33300\n",
      "33350\n",
      "33400\n",
      "33450\n",
      "33500\n",
      "33550\n",
      "33600\n",
      "33650\n",
      "33700\n",
      "33750\n",
      "33800\n",
      "33850\n",
      "33900\n",
      "33950\n",
      "34000\n",
      "34050\n",
      "34100\n",
      "34150\n",
      "34200\n",
      "34250\n",
      "34300\n",
      "34350\n",
      "34400\n",
      "34450\n",
      "34500\n",
      "34550\n",
      "34600\n",
      "34650\n",
      "34700\n",
      "34750\n",
      "34800\n",
      "34850\n",
      "34900\n",
      "34950\n",
      "Epoch: 6/20...  Training Step: 20550...  Training loss: 1.4781...  0.0952 sec/batch\n",
      "35000\n",
      "35050\n",
      "35100\n",
      "35150\n",
      "35200\n",
      "35250\n",
      "35300\n",
      "35350\n",
      "35400\n",
      "35450\n",
      "35500\n",
      "35550\n",
      "35600\n",
      "35650\n",
      "35700\n",
      "35750\n",
      "35800\n",
      "35850\n",
      "35900\n",
      "35950\n",
      "36000\n",
      "36050\n",
      "36100\n",
      "36150\n",
      "36200\n",
      "36250\n",
      "36300\n",
      "36350\n",
      "36400\n",
      "36450\n",
      "36500\n",
      "36550\n",
      "36600\n",
      "36650\n",
      "36700\n",
      "36750\n",
      "36800\n",
      "36850\n",
      "36900\n",
      "36950\n",
      "37000\n",
      "37050\n",
      "37100\n",
      "37150\n",
      "37200\n",
      "37250\n",
      "37300\n",
      "37350\n",
      "37400\n",
      "37450\n",
      "Epoch: 6/20...  Training Step: 20600...  Training loss: 1.5315...  0.1976 sec/batch\n",
      "37500\n",
      "37550\n",
      "37600\n",
      "37650\n",
      "37700\n",
      "37750\n",
      "37800\n",
      "37850\n",
      "37900\n",
      "37950\n",
      "38000\n",
      "38050\n",
      "38100\n",
      "38150\n",
      "38200\n",
      "38250\n",
      "38300\n",
      "38350\n",
      "38400\n",
      "38450\n",
      "38500\n",
      "38550\n",
      "38600\n",
      "38650\n",
      "38700\n",
      "38750\n",
      "38800\n",
      "38850\n",
      "38900\n",
      "38950\n",
      "39000\n",
      "39050\n",
      "39100\n",
      "39150\n",
      "39200\n",
      "39250\n",
      "39300\n",
      "39350\n",
      "39400\n",
      "39450\n",
      "39500\n",
      "39550\n",
      "39600\n",
      "39650\n",
      "39700\n",
      "39750\n",
      "39800\n",
      "39850\n",
      "39900\n",
      "39950\n",
      "Epoch: 6/20...  Training Step: 20650...  Training loss: 1.5764...  0.0935 sec/batch\n",
      "40000\n",
      "40050\n",
      "40100\n",
      "40150\n",
      "40200\n",
      "40250\n",
      "40300\n",
      "40350\n",
      "40400\n",
      "40450\n",
      "40500\n",
      "40550\n",
      "40600\n",
      "40650\n",
      "40700\n",
      "40750\n",
      "40800\n",
      "40850\n",
      "40900\n",
      "40950\n",
      "41000\n",
      "41050\n",
      "41100\n",
      "41150\n",
      "41200\n",
      "41250\n",
      "41300\n",
      "41350\n",
      "41400\n",
      "41450\n",
      "41500\n",
      "41550\n",
      "41600\n",
      "41650\n",
      "41700\n",
      "41750\n",
      "41800\n",
      "41850\n",
      "41900\n",
      "41950\n",
      "42000\n",
      "42050\n",
      "42100\n",
      "42150\n",
      "42200\n",
      "42250\n",
      "42300\n",
      "42350\n",
      "42400\n",
      "42450\n",
      "Epoch: 6/20...  Training Step: 20700...  Training loss: 1.6853...  0.0818 sec/batch\n",
      "42500\n",
      "42550\n",
      "42600\n",
      "42650\n",
      "42700\n",
      "42750\n",
      "42800\n",
      "42850\n",
      "42900\n",
      "42950\n",
      "43000\n",
      "43050\n",
      "43100\n",
      "43150\n",
      "43200\n",
      "43250\n",
      "43300\n",
      "43350\n",
      "43400\n",
      "43450\n",
      "43500\n",
      "43550\n",
      "43600\n",
      "43650\n",
      "43700\n",
      "43750\n",
      "43800\n",
      "43850\n",
      "43900\n",
      "43950\n",
      "44000\n",
      "44050\n",
      "44100\n",
      "44150\n",
      "44200\n",
      "44250\n",
      "44300\n",
      "44350\n",
      "44400\n",
      "44450\n",
      "44500\n",
      "44550\n",
      "44600\n",
      "44650\n",
      "44700\n",
      "44750\n",
      "44800\n",
      "44850\n",
      "44900\n",
      "44950\n",
      "Epoch: 6/20...  Training Step: 20750...  Training loss: 1.5777...  0.0849 sec/batch\n",
      "45000\n",
      "45050\n",
      "45100\n",
      "45150\n",
      "45200\n",
      "45250\n",
      "45300\n",
      "45350\n",
      "45400\n",
      "45450\n",
      "45500\n",
      "45550\n",
      "45600\n",
      "45650\n",
      "45700\n",
      "45750\n",
      "45800\n",
      "45850\n",
      "45900\n",
      "45950\n",
      "46000\n",
      "46050\n",
      "46100\n",
      "46150\n",
      "46200\n",
      "46250\n",
      "46300\n",
      "46350\n",
      "46400\n",
      "46450\n",
      "46500\n",
      "46550\n",
      "46600\n",
      "46650\n",
      "46700\n",
      "46750\n",
      "46800\n",
      "46850\n",
      "46900\n",
      "46950\n",
      "47000\n",
      "47050\n",
      "47100\n",
      "47150\n",
      "47200\n",
      "47250\n",
      "47300\n",
      "47350\n",
      "47400\n",
      "47450\n",
      "Epoch: 6/20...  Training Step: 20800...  Training loss: 1.6717...  0.0914 sec/batch\n",
      "47500\n",
      "47550\n",
      "47600\n",
      "47650\n",
      "47700\n",
      "47750\n",
      "47800\n",
      "47850\n",
      "47900\n",
      "47950\n",
      "48000\n",
      "48050\n",
      "48100\n",
      "48150\n",
      "48200\n",
      "48250\n",
      "48300\n",
      "48350\n",
      "48400\n",
      "48450\n",
      "48500\n",
      "48550\n",
      "48600\n",
      "48650\n",
      "48700\n",
      "48750\n",
      "48800\n",
      "48850\n",
      "48900\n",
      "48950\n",
      "49000\n",
      "49050\n",
      "49100\n",
      "49150\n",
      "49200\n",
      "49250\n",
      "49300\n",
      "49350\n",
      "49400\n",
      "49450\n",
      "49500\n",
      "49550\n",
      "49600\n",
      "49650\n",
      "49700\n",
      "49750\n",
      "49800\n",
      "49850\n",
      "49900\n",
      "49950\n",
      "Epoch: 6/20...  Training Step: 20850...  Training loss: 1.5426...  0.0753 sec/batch\n",
      "50000\n",
      "50050\n",
      "50100\n",
      "50150\n",
      "50200\n",
      "50250\n",
      "50300\n",
      "50350\n",
      "50400\n",
      "50450\n",
      "50500\n",
      "50550\n",
      "50600\n",
      "50650\n",
      "50700\n",
      "50750\n",
      "50800\n",
      "50850\n",
      "50900\n",
      "50950\n",
      "51000\n",
      "51050\n",
      "51100\n",
      "51150\n",
      "51200\n",
      "51250\n",
      "51300\n",
      "51350\n",
      "51400\n",
      "51450\n",
      "51500\n",
      "51550\n",
      "51600\n",
      "51650\n",
      "51700\n",
      "51750\n",
      "51800\n",
      "51850\n",
      "51900\n",
      "51950\n",
      "52000\n",
      "52050\n",
      "52100\n",
      "52150\n",
      "52200\n",
      "52250\n",
      "52300\n",
      "52350\n",
      "52400\n",
      "52450\n",
      "Epoch: 6/20...  Training Step: 20900...  Training loss: 1.6328...  0.1206 sec/batch\n",
      "52500\n",
      "52550\n",
      "52600\n",
      "52650\n",
      "52700\n",
      "52750\n",
      "52800\n",
      "52850\n",
      "52900\n",
      "52950\n",
      "53000\n",
      "53050\n",
      "53100\n",
      "53150\n",
      "53200\n",
      "53250\n",
      "53300\n",
      "53350\n",
      "53400\n",
      "53450\n",
      "53500\n",
      "53550\n",
      "53600\n",
      "53650\n",
      "53700\n",
      "53750\n",
      "53800\n",
      "53850\n",
      "53900\n",
      "53950\n",
      "54000\n",
      "54050\n",
      "54100\n",
      "54150\n",
      "54200\n",
      "54250\n",
      "54300\n",
      "54350\n",
      "54400\n",
      "54450\n",
      "54500\n",
      "54550\n",
      "54600\n",
      "54650\n",
      "54700\n",
      "54750\n",
      "54800\n",
      "54850\n",
      "54900\n",
      "54950\n",
      "Epoch: 6/20...  Training Step: 20950...  Training loss: 1.5926...  0.0853 sec/batch\n",
      "55000\n",
      "55050\n",
      "55100\n",
      "55150\n",
      "55200\n",
      "55250\n",
      "55300\n",
      "55350\n",
      "55400\n",
      "55450\n",
      "55500\n",
      "55550\n",
      "55600\n",
      "55650\n",
      "55700\n",
      "55750\n",
      "55800\n",
      "55850\n",
      "55900\n",
      "55950\n",
      "56000\n",
      "56050\n",
      "56100\n",
      "56150\n",
      "56200\n",
      "56250\n",
      "56300\n",
      "56350\n",
      "56400\n",
      "56450\n",
      "56500\n",
      "56550\n",
      "56600\n",
      "56650\n",
      "56700\n",
      "56750\n",
      "56800\n",
      "56850\n",
      "56900\n",
      "56950\n",
      "57000\n",
      "57050\n",
      "57100\n",
      "57150\n",
      "57200\n",
      "57250\n",
      "57300\n",
      "57350\n",
      "57400\n",
      "57450\n",
      "Epoch: 6/20...  Training Step: 21000...  Training loss: 1.5242...  0.0778 sec/batch\n",
      "57500\n",
      "57550\n",
      "57600\n",
      "57650\n",
      "57700\n",
      "57750\n",
      "57800\n",
      "57850\n",
      "57900\n",
      "57950\n",
      "58000\n",
      "58050\n",
      "58100\n",
      "58150\n",
      "58200\n",
      "58250\n",
      "58300\n",
      "58350\n",
      "58400\n",
      "58450\n",
      "58500\n",
      "58550\n",
      "58600\n",
      "58650\n",
      "58700\n",
      "58750\n",
      "58800\n",
      "58850\n",
      "58900\n",
      "58950\n",
      "59000\n",
      "59050\n",
      "59100\n",
      "59150\n",
      "59200\n",
      "59250\n",
      "59300\n",
      "59350\n",
      "59400\n",
      "59450\n",
      "59500\n",
      "59550\n",
      "59600\n",
      "59650\n",
      "59700\n",
      "59750\n",
      "59800\n",
      "59850\n",
      "59900\n",
      "59950\n",
      "Epoch: 6/20...  Training Step: 21050...  Training loss: 1.4897...  0.0815 sec/batch\n",
      "60000\n",
      "60050\n",
      "60100\n",
      "60150\n",
      "60200\n",
      "60250\n",
      "60300\n",
      "60350\n",
      "60400\n",
      "60450\n",
      "60500\n",
      "60550\n",
      "60600\n",
      "60650\n",
      "60700\n",
      "60750\n",
      "60800\n",
      "60850\n",
      "60900\n",
      "60950\n",
      "61000\n",
      "61050\n",
      "61100\n",
      "61150\n",
      "61200\n",
      "61250\n",
      "61300\n",
      "61350\n",
      "61400\n",
      "61450\n",
      "61500\n",
      "61550\n",
      "61600\n",
      "61650\n",
      "61700\n",
      "61750\n",
      "61800\n",
      "61850\n",
      "61900\n",
      "61950\n",
      "62000\n",
      "62050\n",
      "62100\n",
      "62150\n",
      "62200\n",
      "62250\n",
      "62300\n",
      "62350\n",
      "62400\n",
      "62450\n",
      "Epoch: 6/20...  Training Step: 21100...  Training loss: 1.4190...  0.0848 sec/batch\n",
      "62500\n",
      "62550\n",
      "62600\n",
      "62650\n",
      "62700\n",
      "62750\n",
      "62800\n",
      "62850\n",
      "62900\n",
      "62950\n",
      "63000\n",
      "63050\n",
      "63100\n",
      "63150\n",
      "63200\n",
      "63250\n",
      "63300\n",
      "63350\n",
      "63400\n",
      "63450\n",
      "63500\n",
      "63550\n",
      "63600\n",
      "63650\n",
      "63700\n",
      "63750\n",
      "63800\n",
      "63850\n",
      "63900\n",
      "63950\n",
      "64000\n",
      "64050\n",
      "64100\n",
      "64150\n",
      "64200\n",
      "64250\n",
      "64300\n",
      "64350\n",
      "64400\n",
      "64450\n",
      "64500\n",
      "64550\n",
      "64600\n",
      "64650\n",
      "64700\n",
      "64750\n",
      "64800\n",
      "64850\n",
      "64900\n",
      "64950\n",
      "Epoch: 6/20...  Training Step: 21150...  Training loss: 1.5169...  0.0949 sec/batch\n",
      "65000\n",
      "65050\n",
      "65100\n",
      "65150\n",
      "65200\n",
      "65250\n",
      "65300\n",
      "65350\n",
      "65400\n",
      "65450\n",
      "65500\n",
      "65550\n",
      "65600\n",
      "65650\n",
      "65700\n",
      "65750\n",
      "65800\n",
      "65850\n",
      "65900\n",
      "65950\n",
      "66000\n",
      "66050\n",
      "66100\n",
      "66150\n",
      "66200\n",
      "66250\n",
      "66300\n",
      "66350\n",
      "66400\n",
      "66450\n",
      "66500\n",
      "66550\n",
      "66600\n",
      "66650\n",
      "66700\n",
      "66750\n",
      "66800\n",
      "66850\n",
      "66900\n",
      "66950\n",
      "67000\n",
      "67050\n",
      "67100\n",
      "67150\n",
      "67200\n",
      "67250\n",
      "67300\n",
      "67350\n",
      "67400\n",
      "67450\n",
      "Epoch: 6/20...  Training Step: 21200...  Training loss: 1.4867...  0.0842 sec/batch\n",
      "67500\n",
      "67550\n",
      "67600\n",
      "67650\n",
      "67700\n",
      "67750\n",
      "67800\n",
      "67850\n",
      "67900\n",
      "67950\n",
      "68000\n",
      "68050\n",
      "68100\n",
      "68150\n",
      "68200\n",
      "68250\n",
      "68300\n",
      "68350\n",
      "68400\n",
      "68450\n",
      "68500\n",
      "68550\n",
      "68600\n",
      "68650\n",
      "68700\n",
      "68750\n",
      "68800\n",
      "68850\n",
      "68900\n",
      "68950\n",
      "69000\n",
      "69050\n",
      "69100\n",
      "69150\n",
      "69200\n",
      "69250\n",
      "69300\n",
      "69350\n",
      "69400\n",
      "69450\n",
      "69500\n",
      "69550\n",
      "69600\n",
      "69650\n",
      "69700\n",
      "69750\n",
      "69800\n",
      "69850\n",
      "69900\n",
      "69950\n",
      "Epoch: 6/20...  Training Step: 21250...  Training loss: 1.7236...  0.0790 sec/batch\n",
      "70000\n",
      "70050\n",
      "70100\n",
      "70150\n",
      "70200\n",
      "70250\n",
      "70300\n",
      "70350\n",
      "70400\n",
      "70450\n",
      "70500\n",
      "70550\n",
      "70600\n",
      "70650\n",
      "70700\n",
      "70750\n",
      "70800\n",
      "70850\n",
      "70900\n",
      "70950\n",
      "71000\n",
      "71050\n",
      "71100\n",
      "71150\n",
      "71200\n",
      "71250\n",
      "71300\n",
      "71350\n",
      "71400\n",
      "71450\n",
      "71500\n",
      "71550\n",
      "71600\n",
      "71650\n",
      "71700\n",
      "71750\n",
      "71800\n",
      "71850\n",
      "71900\n",
      "71950\n",
      "72000\n",
      "72050\n",
      "72100\n",
      "72150\n",
      "72200\n",
      "72250\n",
      "72300\n",
      "72350\n",
      "72400\n",
      "72450\n",
      "Epoch: 6/20...  Training Step: 21300...  Training loss: 1.5919...  0.0769 sec/batch\n",
      "72500\n",
      "72550\n",
      "72600\n",
      "72650\n",
      "72700\n",
      "72750\n",
      "72800\n",
      "72850\n",
      "72900\n",
      "72950\n",
      "73000\n",
      "73050\n",
      "73100\n",
      "73150\n",
      "73200\n",
      "73250\n",
      "73300\n",
      "73350\n",
      "73400\n",
      "73450\n",
      "73500\n",
      "73550\n",
      "73600\n",
      "73650\n",
      "73700\n",
      "73750\n",
      "73800\n",
      "73850\n",
      "73900\n",
      "73950\n",
      "74000\n",
      "74050\n",
      "74100\n",
      "74150\n",
      "74200\n",
      "74250\n",
      "74300\n",
      "74350\n",
      "74400\n",
      "74450\n",
      "74500\n",
      "74550\n",
      "74600\n",
      "74650\n",
      "74700\n",
      "74750\n",
      "74800\n",
      "74850\n",
      "74900\n",
      "74950\n",
      "Epoch: 6/20...  Training Step: 21350...  Training loss: 1.6110...  0.0853 sec/batch\n",
      "75000\n",
      "75050\n",
      "75100\n",
      "75150\n",
      "75200\n",
      "75250\n",
      "75300\n",
      "75350\n",
      "75400\n",
      "75450\n",
      "75500\n",
      "75550\n",
      "75600\n",
      "75650\n",
      "75700\n",
      "75750\n",
      "75800\n",
      "75850\n",
      "75900\n",
      "75950\n",
      "76000\n",
      "76050\n",
      "76100\n",
      "76150\n",
      "76200\n",
      "76250\n",
      "76300\n",
      "76350\n",
      "76400\n",
      "76450\n",
      "76500\n",
      "76550\n",
      "76600\n",
      "76650\n",
      "76700\n",
      "76750\n",
      "76800\n",
      "76850\n",
      "76900\n",
      "76950\n",
      "77000\n",
      "77050\n",
      "77100\n",
      "77150\n",
      "77200\n",
      "77250\n",
      "77300\n",
      "77350\n",
      "77400\n",
      "77450\n",
      "Epoch: 6/20...  Training Step: 21400...  Training loss: 1.4470...  0.0809 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77500\n",
      "77550\n",
      "77600\n",
      "77650\n",
      "77700\n",
      "77750\n",
      "77800\n",
      "77850\n",
      "77900\n",
      "77950\n",
      "78000\n",
      "78050\n",
      "78100\n",
      "78150\n",
      "78200\n",
      "78250\n",
      "78300\n",
      "78350\n",
      "78400\n",
      "78450\n",
      "78500\n",
      "78550\n",
      "78600\n",
      "78650\n",
      "78700\n",
      "78750\n",
      "78800\n",
      "78850\n",
      "78900\n",
      "78950\n",
      "79000\n",
      "79050\n",
      "79100\n",
      "79150\n",
      "79200\n",
      "79250\n",
      "79300\n",
      "79350\n",
      "79400\n",
      "79450\n",
      "79500\n",
      "79550\n",
      "79600\n",
      "79650\n",
      "79700\n",
      "79750\n",
      "79800\n",
      "79850\n",
      "79900\n",
      "79950\n",
      "Epoch: 6/20...  Training Step: 21450...  Training loss: 1.5783...  0.0751 sec/batch\n",
      "80000\n",
      "80050\n",
      "80100\n",
      "80150\n",
      "80200\n",
      "80250\n",
      "80300\n",
      "80350\n",
      "80400\n",
      "80450\n",
      "80500\n",
      "80550\n",
      "80600\n",
      "80650\n",
      "80700\n",
      "80750\n",
      "80800\n",
      "80850\n",
      "80900\n",
      "80950\n",
      "81000\n",
      "81050\n",
      "81100\n",
      "81150\n",
      "81200\n",
      "81250\n",
      "81300\n",
      "81350\n",
      "81400\n",
      "81450\n",
      "81500\n",
      "81550\n",
      "81600\n",
      "81650\n",
      "81700\n",
      "81750\n",
      "81800\n",
      "81850\n",
      "81900\n",
      "81950\n",
      "82000\n",
      "82050\n",
      "82100\n",
      "82150\n",
      "82200\n",
      "82250\n",
      "82300\n",
      "82350\n",
      "82400\n",
      "82450\n",
      "Epoch: 6/20...  Training Step: 21500...  Training loss: 1.5087...  0.0966 sec/batch\n",
      "82500\n",
      "82550\n",
      "82600\n",
      "82650\n",
      "82700\n",
      "82750\n",
      "82800\n",
      "82850\n",
      "82900\n",
      "82950\n",
      "83000\n",
      "83050\n",
      "83100\n",
      "83150\n",
      "83200\n",
      "83250\n",
      "83300\n",
      "83350\n",
      "83400\n",
      "83450\n",
      "83500\n",
      "83550\n",
      "83600\n",
      "83650\n",
      "83700\n",
      "83750\n",
      "83800\n",
      "83850\n",
      "83900\n",
      "83950\n",
      "84000\n",
      "84050\n",
      "84100\n",
      "84150\n",
      "84200\n",
      "84250\n",
      "84300\n",
      "84350\n",
      "84400\n",
      "84450\n",
      "84500\n",
      "84550\n",
      "84600\n",
      "84650\n",
      "84700\n",
      "84750\n",
      "84800\n",
      "84850\n",
      "84900\n",
      "84950\n",
      "Epoch: 6/20...  Training Step: 21550...  Training loss: 1.6227...  0.0767 sec/batch\n",
      "85000\n",
      "85050\n",
      "85100\n",
      "85150\n",
      "85200\n",
      "85250\n",
      "85300\n",
      "85350\n",
      "85400\n",
      "85450\n",
      "85500\n",
      "85550\n",
      "85600\n",
      "85650\n",
      "85700\n",
      "85750\n",
      "85800\n",
      "85850\n",
      "85900\n",
      "85950\n",
      "86000\n",
      "86050\n",
      "86100\n",
      "86150\n",
      "86200\n",
      "86250\n",
      "86300\n",
      "86350\n",
      "86400\n",
      "86450\n",
      "86500\n",
      "86550\n",
      "86600\n",
      "86650\n",
      "86700\n",
      "86750\n",
      "86800\n",
      "86850\n",
      "86900\n",
      "86950\n",
      "87000\n",
      "87050\n",
      "87100\n",
      "87150\n",
      "87200\n",
      "87250\n",
      "87300\n",
      "87350\n",
      "87400\n",
      "87450\n",
      "Epoch: 6/20...  Training Step: 21600...  Training loss: 1.5915...  0.0780 sec/batch\n",
      "87500\n",
      "87550\n",
      "87600\n",
      "87650\n",
      "87700\n",
      "87750\n",
      "87800\n",
      "87850\n",
      "87900\n",
      "87950\n",
      "88000\n",
      "88050\n",
      "88100\n",
      "88150\n",
      "88200\n",
      "88250\n",
      "88300\n",
      "88350\n",
      "88400\n",
      "88450\n",
      "88500\n",
      "88550\n",
      "88600\n",
      "88650\n",
      "88700\n",
      "88750\n",
      "88800\n",
      "88850\n",
      "88900\n",
      "88950\n",
      "89000\n",
      "89050\n",
      "89100\n",
      "89150\n",
      "89200\n",
      "89250\n",
      "89300\n",
      "89350\n",
      "89400\n",
      "89450\n",
      "89500\n",
      "89550\n",
      "89600\n",
      "89650\n",
      "89700\n",
      "89750\n",
      "89800\n",
      "89850\n",
      "89900\n",
      "89950\n",
      "Epoch: 6/20...  Training Step: 21650...  Training loss: 1.5273...  0.0874 sec/batch\n",
      "90000\n",
      "90050\n",
      "90100\n",
      "90150\n",
      "90200\n",
      "90250\n",
      "90300\n",
      "90350\n",
      "90400\n",
      "90450\n",
      "90500\n",
      "90550\n",
      "90600\n",
      "90650\n",
      "90700\n",
      "90750\n",
      "90800\n",
      "90850\n",
      "90900\n",
      "90950\n",
      "91000\n",
      "91050\n",
      "91100\n",
      "91150\n",
      "91200\n",
      "91250\n",
      "91300\n",
      "91350\n",
      "91400\n",
      "91450\n",
      "91500\n",
      "91550\n",
      "91600\n",
      "91650\n",
      "91700\n",
      "91750\n",
      "91800\n",
      "91850\n",
      "91900\n",
      "91950\n",
      "92000\n",
      "92050\n",
      "92100\n",
      "92150\n",
      "92200\n",
      "92250\n",
      "92300\n",
      "92350\n",
      "92400\n",
      "92450\n",
      "Epoch: 6/20...  Training Step: 21700...  Training loss: 1.6780...  0.0830 sec/batch\n",
      "92500\n",
      "92550\n",
      "92600\n",
      "92650\n",
      "92700\n",
      "92750\n",
      "92800\n",
      "92850\n",
      "92900\n",
      "92950\n",
      "93000\n",
      "93050\n",
      "93100\n",
      "93150\n",
      "93200\n",
      "93250\n",
      "93300\n",
      "93350\n",
      "93400\n",
      "93450\n",
      "93500\n",
      "93550\n",
      "93600\n",
      "93650\n",
      "93700\n",
      "93750\n",
      "93800\n",
      "93850\n",
      "93900\n",
      "93950\n",
      "94000\n",
      "94050\n",
      "94100\n",
      "94150\n",
      "94200\n",
      "94250\n",
      "94300\n",
      "94350\n",
      "94400\n",
      "94450\n",
      "94500\n",
      "94550\n",
      "94600\n",
      "94650\n",
      "94700\n",
      "94750\n",
      "94800\n",
      "94850\n",
      "94900\n",
      "94950\n",
      "Epoch: 6/20...  Training Step: 21750...  Training loss: 1.3821...  0.0764 sec/batch\n",
      "95000\n",
      "95050\n",
      "95100\n",
      "95150\n",
      "95200\n",
      "95250\n",
      "95300\n",
      "95350\n",
      "95400\n",
      "95450\n",
      "95500\n",
      "95550\n",
      "95600\n",
      "95650\n",
      "95700\n",
      "95750\n",
      "95800\n",
      "95850\n",
      "95900\n",
      "95950\n",
      "96000\n",
      "96050\n",
      "96100\n",
      "96150\n",
      "96200\n",
      "96250\n",
      "96300\n",
      "96350\n",
      "96400\n",
      "96450\n",
      "96500\n",
      "96550\n",
      "96600\n",
      "96650\n",
      "96700\n",
      "96750\n",
      "96800\n",
      "96850\n",
      "96900\n",
      "96950\n",
      "97000\n",
      "97050\n",
      "97100\n",
      "97150\n",
      "97200\n",
      "97250\n",
      "97300\n",
      "97350\n",
      "97400\n",
      "97450\n",
      "Epoch: 6/20...  Training Step: 21800...  Training loss: 1.4834...  0.0760 sec/batch\n",
      "97500\n",
      "97550\n",
      "97600\n",
      "97650\n",
      "97700\n",
      "97750\n",
      "97800\n",
      "97850\n",
      "97900\n",
      "97950\n",
      "98000\n",
      "98050\n",
      "98100\n",
      "98150\n",
      "98200\n",
      "98250\n",
      "98300\n",
      "98350\n",
      "98400\n",
      "98450\n",
      "98500\n",
      "98550\n",
      "98600\n",
      "98650\n",
      "98700\n",
      "98750\n",
      "98800\n",
      "98850\n",
      "98900\n",
      "98950\n",
      "99000\n",
      "99050\n",
      "99100\n",
      "99150\n",
      "99200\n",
      "99250\n",
      "99300\n",
      "99350\n",
      "99400\n",
      "99450\n",
      "99500\n",
      "99550\n",
      "99600\n",
      "99650\n",
      "99700\n",
      "99750\n",
      "99800\n",
      "99850\n",
      "99900\n",
      "99950\n",
      "Epoch: 6/20...  Training Step: 21850...  Training loss: 1.5454...  0.0952 sec/batch\n",
      "100000\n",
      "100050\n",
      "100100\n",
      "100150\n",
      "100200\n",
      "100250\n",
      "100300\n",
      "100350\n",
      "100400\n",
      "100450\n",
      "100500\n",
      "100550\n",
      "100600\n",
      "100650\n",
      "100700\n",
      "100750\n",
      "100800\n",
      "100850\n",
      "100900\n",
      "100950\n",
      "101000\n",
      "101050\n",
      "101100\n",
      "101150\n",
      "101200\n",
      "101250\n",
      "101300\n",
      "101350\n",
      "101400\n",
      "101450\n",
      "101500\n",
      "101550\n",
      "101600\n",
      "101650\n",
      "101700\n",
      "101750\n",
      "101800\n",
      "101850\n",
      "101900\n",
      "101950\n",
      "102000\n",
      "102050\n",
      "102100\n",
      "102150\n",
      "102200\n",
      "102250\n",
      "102300\n",
      "102350\n",
      "102400\n",
      "102450\n",
      "Epoch: 6/20...  Training Step: 21900...  Training loss: 1.6557...  0.0819 sec/batch\n",
      "102500\n",
      "102550\n",
      "102600\n",
      "102650\n",
      "102700\n",
      "102750\n",
      "102800\n",
      "102850\n",
      "102900\n",
      "102950\n",
      "103000\n",
      "103050\n",
      "103100\n",
      "103150\n",
      "103200\n",
      "103250\n",
      "103300\n",
      "103350\n",
      "103400\n",
      "103450\n",
      "103500\n",
      "103550\n",
      "103600\n",
      "103650\n",
      "103700\n",
      "103750\n",
      "103800\n",
      "103850\n",
      "103900\n",
      "103950\n",
      "104000\n",
      "104050\n",
      "104100\n",
      "104150\n",
      "104200\n",
      "104250\n",
      "104300\n",
      "104350\n",
      "104400\n",
      "104450\n",
      "104500\n",
      "104550\n",
      "104600\n",
      "104650\n",
      "104700\n",
      "104750\n",
      "104800\n",
      "104850\n",
      "104900\n",
      "104950\n",
      "Epoch: 6/20...  Training Step: 21950...  Training loss: 1.5938...  0.0974 sec/batch\n",
      "105000\n",
      "105050\n",
      "105100\n",
      "105150\n",
      "105200\n",
      "105250\n",
      "105300\n",
      "105350\n",
      "105400\n",
      "105450\n",
      "105500\n",
      "105550\n",
      "105600\n",
      "105650\n",
      "105700\n",
      "105750\n",
      "105800\n",
      "105850\n",
      "105900\n",
      "105950\n",
      "106000\n",
      "106050\n",
      "106100\n",
      "106150\n",
      "106200\n",
      "106250\n",
      "106300\n",
      "106350\n",
      "106400\n",
      "106450\n",
      "106500\n",
      "106550\n",
      "106600\n",
      "106650\n",
      "106700\n",
      "106750\n",
      "106800\n",
      "106850\n",
      "106900\n",
      "106950\n",
      "107000\n",
      "107050\n",
      "107100\n",
      "107150\n",
      "107200\n",
      "107250\n",
      "107300\n",
      "107350\n",
      "107400\n",
      "107450\n",
      "Epoch: 6/20...  Training Step: 22000...  Training loss: 1.5171...  0.0748 sec/batch\n",
      "107500\n",
      "107550\n",
      "107600\n",
      "107650\n",
      "107700\n",
      "107750\n",
      "107800\n",
      "107850\n",
      "107900\n",
      "107950\n",
      "108000\n",
      "108050\n",
      "108100\n",
      "108150\n",
      "108200\n",
      "108250\n",
      "108300\n",
      "108350\n",
      "108400\n",
      "108450\n",
      "108500\n",
      "108550\n",
      "108600\n",
      "108650\n",
      "108700\n",
      "108750\n",
      "108800\n",
      "108850\n",
      "108900\n",
      "108950\n",
      "109000\n",
      "109050\n",
      "109100\n",
      "109150\n",
      "109200\n",
      "109250\n",
      "109300\n",
      "109350\n",
      "109400\n",
      "109450\n",
      "109500\n",
      "109550\n",
      "109600\n",
      "109650\n",
      "109700\n",
      "109750\n",
      "109800\n",
      "109850\n",
      "109900\n",
      "109950\n",
      "Epoch: 6/20...  Training Step: 22050...  Training loss: 1.5236...  0.0816 sec/batch\n",
      "110000\n",
      "110050\n",
      "110100\n",
      "110150\n",
      "110200\n",
      "110250\n",
      "110300\n",
      "110350\n",
      "110400\n",
      "110450\n",
      "110500\n",
      "110550\n",
      "110600\n",
      "110650\n",
      "110700\n",
      "110750\n",
      "110800\n",
      "110850\n",
      "110900\n",
      "110950\n",
      "111000\n",
      "111050\n",
      "111100\n",
      "111150\n",
      "111200\n",
      "111250\n",
      "111300\n",
      "111350\n",
      "111400\n",
      "111450\n",
      "111500\n",
      "111550\n",
      "111600\n",
      "111650\n",
      "111700\n",
      "111750\n",
      "111800\n",
      "111850\n",
      "111900\n",
      "111950\n",
      "112000\n",
      "112050\n",
      "112100\n",
      "112150\n",
      "112200\n",
      "112250\n",
      "112300\n",
      "112350\n",
      "112400\n",
      "112450\n",
      "Epoch: 6/20...  Training Step: 22100...  Training loss: 1.5584...  0.0761 sec/batch\n",
      "112500\n",
      "112550\n",
      "112600\n",
      "112650\n",
      "112700\n",
      "112750\n",
      "112800\n",
      "112850\n",
      "112900\n",
      "112950\n",
      "113000\n",
      "113050\n",
      "113100\n",
      "113150\n",
      "113200\n",
      "113250\n",
      "113300\n",
      "113350\n",
      "113400\n",
      "113450\n",
      "113500\n",
      "113550\n",
      "113600\n",
      "113650\n",
      "113700\n",
      "113750\n",
      "113800\n",
      "113850\n",
      "113900\n",
      "113950\n",
      "114000\n",
      "114050\n",
      "114100\n",
      "114150\n",
      "114200\n",
      "114250\n",
      "114300\n",
      "114350\n",
      "114400\n",
      "114450\n",
      "114500\n",
      "114550\n",
      "114600\n",
      "114650\n",
      "114700\n",
      "114750\n",
      "114800\n",
      "114850\n",
      "114900\n",
      "114950\n",
      "Epoch: 6/20...  Training Step: 22150...  Training loss: 1.4780...  0.0932 sec/batch\n",
      "115000\n",
      "115050\n",
      "115100\n",
      "115150\n",
      "115200\n",
      "115250\n",
      "115300\n",
      "115350\n",
      "115400\n",
      "115450\n",
      "115500\n",
      "115550\n",
      "115600\n",
      "115650\n",
      "115700\n",
      "115750\n",
      "115800\n",
      "115850\n",
      "115900\n",
      "115950\n",
      "116000\n",
      "116050\n",
      "116100\n",
      "116150\n",
      "116200\n",
      "116250\n",
      "116300\n",
      "116350\n",
      "116400\n",
      "116450\n",
      "116500\n",
      "116550\n",
      "116600\n",
      "116650\n",
      "116700\n",
      "116750\n",
      "116800\n",
      "116850\n",
      "116900\n",
      "116950\n",
      "117000\n",
      "117050\n",
      "117100\n",
      "117150\n",
      "117200\n",
      "117250\n",
      "117300\n",
      "117350\n",
      "117400\n",
      "117450\n",
      "Epoch: 6/20...  Training Step: 22200...  Training loss: 1.6860...  0.0791 sec/batch\n",
      "117500\n",
      "117550\n",
      "117600\n",
      "117650\n",
      "117700\n",
      "117750\n",
      "117800\n",
      "117850\n",
      "117900\n",
      "117950\n",
      "118000\n",
      "118050\n",
      "118100\n",
      "118150\n",
      "118200\n",
      "118250\n",
      "118300\n",
      "118350\n",
      "118400\n",
      "118450\n",
      "118500\n",
      "118550\n",
      "118600\n",
      "118650\n",
      "118700\n",
      "118750\n",
      "118800\n",
      "118850\n",
      "118900\n",
      "118950\n",
      "119000\n",
      "119050\n",
      "119100\n",
      "119150\n",
      "119200\n",
      "119250\n",
      "119300\n",
      "119350\n",
      "119400\n",
      "119450\n",
      "119500\n",
      "119550\n",
      "119600\n",
      "119650\n",
      "119700\n",
      "119750\n",
      "119800\n",
      "119850\n",
      "119900\n",
      "119950\n",
      "Epoch: 6/20...  Training Step: 22250...  Training loss: 1.6608...  0.0787 sec/batch\n",
      "120000\n",
      "120050\n",
      "120100\n",
      "120150\n",
      "120200\n",
      "120250\n",
      "120300\n",
      "120350\n",
      "120400\n",
      "120450\n",
      "120500\n",
      "120550\n",
      "120600\n",
      "120650\n",
      "120700\n",
      "120750\n",
      "120800\n",
      "120850\n",
      "120900\n",
      "120950\n",
      "121000\n",
      "121050\n",
      "121100\n",
      "121150\n",
      "121200\n",
      "121250\n",
      "121300\n",
      "121350\n",
      "121400\n",
      "121450\n",
      "121500\n",
      "121550\n",
      "121600\n",
      "121650\n",
      "121700\n",
      "121750\n",
      "121800\n",
      "121850\n",
      "121900\n",
      "121950\n",
      "122000\n",
      "122050\n",
      "122100\n",
      "122150\n",
      "122200\n",
      "122250\n",
      "122300\n",
      "122350\n",
      "122400\n",
      "122450\n",
      "Epoch: 6/20...  Training Step: 22300...  Training loss: 1.6574...  0.0810 sec/batch\n",
      "122500\n",
      "122550\n",
      "122600\n",
      "122650\n",
      "122700\n",
      "122750\n",
      "122800\n",
      "122850\n",
      "122900\n",
      "122950\n",
      "123000\n",
      "123050\n",
      "123100\n",
      "123150\n",
      "123200\n",
      "123250\n",
      "123300\n",
      "123350\n",
      "123400\n",
      "123450\n",
      "123500\n",
      "123550\n",
      "123600\n",
      "123650\n",
      "123700\n",
      "123750\n",
      "123800\n",
      "123850\n",
      "123900\n",
      "123950\n",
      "124000\n",
      "124050\n",
      "124100\n",
      "124150\n",
      "124200\n",
      "124250\n",
      "124300\n",
      "124350\n",
      "124400\n",
      "124450\n",
      "124500\n",
      "124550\n",
      "124600\n",
      "124650\n",
      "124700\n",
      "124750\n",
      "124800\n",
      "124850\n",
      "124900\n",
      "124950\n",
      "Epoch: 6/20...  Training Step: 22350...  Training loss: 1.4711...  0.0736 sec/batch\n",
      "125000\n",
      "125050\n",
      "125100\n",
      "125150\n",
      "125200\n",
      "125250\n",
      "125300\n",
      "125350\n",
      "125400\n",
      "125450\n",
      "125500\n",
      "125550\n",
      "125600\n",
      "125650\n",
      "125700\n",
      "125750\n",
      "125800\n",
      "125850\n",
      "125900\n",
      "125950\n",
      "126000\n",
      "126050\n",
      "126100\n",
      "126150\n",
      "126200\n",
      "126250\n",
      "126300\n",
      "126350\n",
      "126400\n",
      "126450\n",
      "126500\n",
      "126550\n",
      "126600\n",
      "126650\n",
      "126700\n",
      "126750\n",
      "126800\n",
      "126850\n",
      "126900\n",
      "126950\n",
      "127000\n",
      "127050\n",
      "127100\n",
      "127150\n",
      "127200\n",
      "127250\n",
      "127300\n",
      "127350\n",
      "127400\n",
      "127450\n",
      "Epoch: 6/20...  Training Step: 22400...  Training loss: 1.5581...  0.0757 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127500\n",
      "127550\n",
      "127600\n",
      "127650\n",
      "127700\n",
      "127750\n",
      "127800\n",
      "127850\n",
      "127900\n",
      "127950\n",
      "128000\n",
      "128050\n",
      "128100\n",
      "128150\n",
      "128200\n",
      "128250\n",
      "128300\n",
      "128350\n",
      "128400\n",
      "128450\n",
      "128500\n",
      "128550\n",
      "128600\n",
      "128650\n",
      "128700\n",
      "128750\n",
      "128800\n",
      "128850\n",
      "128900\n",
      "128950\n",
      "129000\n",
      "129050\n",
      "129100\n",
      "129150\n",
      "129200\n",
      "129250\n",
      "129300\n",
      "129350\n",
      "129400\n",
      "129450\n",
      "129500\n",
      "129550\n",
      "129600\n",
      "129650\n",
      "129700\n",
      "129750\n",
      "129800\n",
      "129850\n",
      "129900\n",
      "129950\n",
      "Epoch: 6/20...  Training Step: 22450...  Training loss: 1.5403...  0.0819 sec/batch\n",
      "130000\n",
      "130050\n",
      "130100\n",
      "130150\n",
      "130200\n",
      "130250\n",
      "130300\n",
      "130350\n",
      "130400\n",
      "130450\n",
      "130500\n",
      "130550\n",
      "130600\n",
      "130650\n",
      "130700\n",
      "130750\n",
      "130800\n",
      "130850\n",
      "130900\n",
      "130950\n",
      "131000\n",
      "131050\n",
      "131100\n",
      "131150\n",
      "131200\n",
      "131250\n",
      "131300\n",
      "131350\n",
      "131400\n",
      "131450\n",
      "131500\n",
      "131550\n",
      "131600\n",
      "131650\n",
      "131700\n",
      "131750\n",
      "131800\n",
      "131850\n",
      "131900\n",
      "131950\n",
      "132000\n",
      "132050\n",
      "132100\n",
      "132150\n",
      "132200\n",
      "132250\n",
      "132300\n",
      "132350\n",
      "132400\n",
      "132450\n",
      "Epoch: 6/20...  Training Step: 22500...  Training loss: 1.7994...  0.0783 sec/batch\n",
      "132500\n",
      "132550\n",
      "132600\n",
      "132650\n",
      "132700\n",
      "132750\n",
      "132800\n",
      "132850\n",
      "132900\n",
      "132950\n",
      "133000\n",
      "133050\n",
      "133100\n",
      "133150\n",
      "133200\n",
      "133250\n",
      "133300\n",
      "133350\n",
      "133400\n",
      "133450\n",
      "133500\n",
      "133550\n",
      "133600\n",
      "133650\n",
      "133700\n",
      "133750\n",
      "133800\n",
      "133850\n",
      "133900\n",
      "133950\n",
      "134000\n",
      "134050\n",
      "134100\n",
      "134150\n",
      "134200\n",
      "134250\n",
      "134300\n",
      "134350\n",
      "134400\n",
      "134450\n",
      "134500\n",
      "134550\n",
      "134600\n",
      "134650\n",
      "134700\n",
      "134750\n",
      "134800\n",
      "134850\n",
      "134900\n",
      "134950\n",
      "Epoch: 6/20...  Training Step: 22550...  Training loss: 1.6186...  0.0767 sec/batch\n",
      "135000\n",
      "135050\n",
      "135100\n",
      "135150\n",
      "135200\n",
      "135250\n",
      "135300\n",
      "135350\n",
      "135400\n",
      "135450\n",
      "135500\n",
      "135550\n",
      "135600\n",
      "135650\n",
      "135700\n",
      "135750\n",
      "135800\n",
      "135850\n",
      "135900\n",
      "135950\n",
      "136000\n",
      "136050\n",
      "136100\n",
      "136150\n",
      "136200\n",
      "136250\n",
      "136300\n",
      "136350\n",
      "136400\n",
      "136450\n",
      "136500\n",
      "136550\n",
      "136600\n",
      "136650\n",
      "136700\n",
      "136750\n",
      "136800\n",
      "136850\n",
      "136900\n",
      "136950\n",
      "137000\n",
      "137050\n",
      "137100\n",
      "137150\n",
      "137200\n",
      "137250\n",
      "137300\n",
      "137350\n",
      "137400\n",
      "137450\n",
      "Epoch: 6/20...  Training Step: 22600...  Training loss: 1.5071...  0.0824 sec/batch\n",
      "137500\n",
      "137550\n",
      "137600\n",
      "137650\n",
      "137700\n",
      "137750\n",
      "137800\n",
      "137850\n",
      "137900\n",
      "137950\n",
      "138000\n",
      "138050\n",
      "138100\n",
      "138150\n",
      "138200\n",
      "138250\n",
      "138300\n",
      "138350\n",
      "138400\n",
      "138450\n",
      "138500\n",
      "138550\n",
      "138600\n",
      "138650\n",
      "138700\n",
      "138750\n",
      "138800\n",
      "138850\n",
      "138900\n",
      "138950\n",
      "139000\n",
      "139050\n",
      "139100\n",
      "139150\n",
      "139200\n",
      "139250\n",
      "139300\n",
      "139350\n",
      "139400\n",
      "139450\n",
      "139500\n",
      "139550\n",
      "139600\n",
      "139650\n",
      "139700\n",
      "139750\n",
      "139800\n",
      "139850\n",
      "139900\n",
      "139950\n",
      "Epoch: 6/20...  Training Step: 22650...  Training loss: 1.4017...  0.0819 sec/batch\n",
      "140000\n",
      "140050\n",
      "140100\n",
      "140150\n",
      "140200\n",
      "140250\n",
      "140300\n",
      "140350\n",
      "140400\n",
      "140450\n",
      "140500\n",
      "140550\n",
      "140600\n",
      "140650\n",
      "140700\n",
      "140750\n",
      "140800\n",
      "140850\n",
      "140900\n",
      "140950\n",
      "141000\n",
      "141050\n",
      "141100\n",
      "141150\n",
      "141200\n",
      "141250\n",
      "141300\n",
      "141350\n",
      "141400\n",
      "141450\n",
      "141500\n",
      "141550\n",
      "141600\n",
      "141650\n",
      "141700\n",
      "141750\n",
      "141800\n",
      "141850\n",
      "141900\n",
      "141950\n",
      "142000\n",
      "142050\n",
      "142100\n",
      "142150\n",
      "142200\n",
      "142250\n",
      "142300\n",
      "142350\n",
      "142400\n",
      "142450\n",
      "Epoch: 6/20...  Training Step: 22700...  Training loss: 1.6592...  0.0859 sec/batch\n",
      "142500\n",
      "142550\n",
      "142600\n",
      "142650\n",
      "142700\n",
      "142750\n",
      "142800\n",
      "142850\n",
      "142900\n",
      "142950\n",
      "143000\n",
      "143050\n",
      "143100\n",
      "143150\n",
      "143200\n",
      "143250\n",
      "143300\n",
      "143350\n",
      "143400\n",
      "143450\n",
      "143500\n",
      "143550\n",
      "143600\n",
      "143650\n",
      "143700\n",
      "143750\n",
      "143800\n",
      "143850\n",
      "143900\n",
      "143950\n",
      "144000\n",
      "144050\n",
      "144100\n",
      "144150\n",
      "144200\n",
      "144250\n",
      "144300\n",
      "144350\n",
      "144400\n",
      "144450\n",
      "144500\n",
      "144550\n",
      "144600\n",
      "144650\n",
      "144700\n",
      "144750\n",
      "144800\n",
      "144850\n",
      "144900\n",
      "144950\n",
      "Epoch: 6/20...  Training Step: 22750...  Training loss: 1.4305...  0.0809 sec/batch\n",
      "145000\n",
      "145050\n",
      "145100\n",
      "145150\n",
      "145200\n",
      "145250\n",
      "145300\n",
      "145350\n",
      "145400\n",
      "145450\n",
      "145500\n",
      "145550\n",
      "145600\n",
      "145650\n",
      "145700\n",
      "145750\n",
      "145800\n",
      "145850\n",
      "145900\n",
      "145950\n",
      "146000\n",
      "146050\n",
      "146100\n",
      "146150\n",
      "146200\n",
      "146250\n",
      "146300\n",
      "146350\n",
      "146400\n",
      "146450\n",
      "146500\n",
      "146550\n",
      "146600\n",
      "146650\n",
      "146700\n",
      "146750\n",
      "146800\n",
      "146850\n",
      "146900\n",
      "146950\n",
      "147000\n",
      "147050\n",
      "147100\n",
      "147150\n",
      "147200\n",
      "147250\n",
      "147300\n",
      "147350\n",
      "147400\n",
      "147450\n",
      "Epoch: 6/20...  Training Step: 22800...  Training loss: 1.3799...  0.0788 sec/batch\n",
      "147500\n",
      "147550\n",
      "147600\n",
      "147650\n",
      "147700\n",
      "147750\n",
      "147800\n",
      "147850\n",
      "147900\n",
      "147950\n",
      "148000\n",
      "148050\n",
      "148100\n",
      "148150\n",
      "148200\n",
      "148250\n",
      "148300\n",
      "148350\n",
      "148400\n",
      "148450\n",
      "148500\n",
      "148550\n",
      "148600\n",
      "148650\n",
      "148700\n",
      "148750\n",
      "148800\n",
      "148850\n",
      "148900\n",
      "148950\n",
      "149000\n",
      "149050\n",
      "149100\n",
      "149150\n",
      "149200\n",
      "149250\n",
      "149300\n",
      "149350\n",
      "149400\n",
      "149450\n",
      "149500\n",
      "149550\n",
      "149600\n",
      "149650\n",
      "149700\n",
      "149750\n",
      "149800\n",
      "149850\n",
      "149900\n",
      "149950\n",
      "Epoch: 6/20...  Training Step: 22850...  Training loss: 1.5874...  0.0757 sec/batch\n",
      "150000\n",
      "150050\n",
      "150100\n",
      "150150\n",
      "150200\n",
      "150250\n",
      "150300\n",
      "150350\n",
      "150400\n",
      "150450\n",
      "150500\n",
      "150550\n",
      "150600\n",
      "150650\n",
      "150700\n",
      "150750\n",
      "150800\n",
      "150850\n",
      "150900\n",
      "150950\n",
      "151000\n",
      "151050\n",
      "151100\n",
      "151150\n",
      "151200\n",
      "151250\n",
      "151300\n",
      "151350\n",
      "151400\n",
      "151450\n",
      "151500\n",
      "151550\n",
      "151600\n",
      "151650\n",
      "151700\n",
      "151750\n",
      "151800\n",
      "151850\n",
      "151900\n",
      "151950\n",
      "152000\n",
      "152050\n",
      "152100\n",
      "152150\n",
      "152200\n",
      "152250\n",
      "152300\n",
      "152350\n",
      "152400\n",
      "152450\n",
      "Epoch: 6/20...  Training Step: 22900...  Training loss: 1.6464...  0.0811 sec/batch\n",
      "152500\n",
      "152550\n",
      "152600\n",
      "152650\n",
      "152700\n",
      "152750\n",
      "152800\n",
      "152850\n",
      "152900\n",
      "152950\n",
      "153000\n",
      "153050\n",
      "153100\n",
      "153150\n",
      "153200\n",
      "153250\n",
      "153300\n",
      "153350\n",
      "153400\n",
      "153450\n",
      "153500\n",
      "153550\n",
      "153600\n",
      "153650\n",
      "153700\n",
      "153750\n",
      "153800\n",
      "153850\n",
      "153900\n",
      "153950\n",
      "154000\n",
      "154050\n",
      "154100\n",
      "154150\n",
      "154200\n",
      "154250\n",
      "154300\n",
      "154350\n",
      "154400\n",
      "154450\n",
      "154500\n",
      "154550\n",
      "154600\n",
      "154650\n",
      "154700\n",
      "154750\n",
      "154800\n",
      "154850\n",
      "154900\n",
      "154950\n",
      "Epoch: 6/20...  Training Step: 22950...  Training loss: 1.4734...  0.0844 sec/batch\n",
      "155000\n",
      "155050\n",
      "155100\n",
      "155150\n",
      "155200\n",
      "155250\n",
      "155300\n",
      "155350\n",
      "155400\n",
      "155450\n",
      "155500\n",
      "155550\n",
      "155600\n",
      "155650\n",
      "155700\n",
      "155750\n",
      "155800\n",
      "155850\n",
      "155900\n",
      "155950\n",
      "156000\n",
      "156050\n",
      "156100\n",
      "156150\n",
      "156200\n",
      "156250\n",
      "156300\n",
      "156350\n",
      "156400\n",
      "156450\n",
      "156500\n",
      "156550\n",
      "156600\n",
      "156650\n",
      "156700\n",
      "156750\n",
      "156800\n",
      "156850\n",
      "156900\n",
      "156950\n",
      "157000\n",
      "157050\n",
      "157100\n",
      "157150\n",
      "157200\n",
      "157250\n",
      "157300\n",
      "157350\n",
      "157400\n",
      "157450\n",
      "Epoch: 6/20...  Training Step: 23000...  Training loss: 1.4653...  0.0847 sec/batch\n",
      "157500\n",
      "157550\n",
      "157600\n",
      "157650\n",
      "157700\n",
      "157750\n",
      "157800\n",
      "157850\n",
      "157900\n",
      "157950\n",
      "158000\n",
      "158050\n",
      "158100\n",
      "158150\n",
      "158200\n",
      "158250\n",
      "158300\n",
      "158350\n",
      "158400\n",
      "158450\n",
      "158500\n",
      "158550\n",
      "158600\n",
      "158650\n",
      "158700\n",
      "158750\n",
      "158800\n",
      "158850\n",
      "158900\n",
      "158950\n",
      "159000\n",
      "159050\n",
      "159100\n",
      "159150\n",
      "159200\n",
      "159250\n",
      "159300\n",
      "159350\n",
      "159400\n",
      "159450\n",
      "159500\n",
      "159550\n",
      "159600\n",
      "159650\n",
      "159700\n",
      "159750\n",
      "159800\n",
      "159850\n",
      "159900\n",
      "159950\n",
      "Epoch: 6/20...  Training Step: 23050...  Training loss: 1.6977...  0.0796 sec/batch\n",
      "160000\n",
      "160050\n",
      "160100\n",
      "160150\n",
      "160200\n",
      "160250\n",
      "160300\n",
      "160350\n",
      "160400\n",
      "160450\n",
      "160500\n",
      "160550\n",
      "160600\n",
      "160650\n",
      "160700\n",
      "160750\n",
      "160800\n",
      "160850\n",
      "160900\n",
      "160950\n",
      "161000\n",
      "161050\n",
      "161100\n",
      "161150\n",
      "161200\n",
      "161250\n",
      "161300\n",
      "161350\n",
      "161400\n",
      "161450\n",
      "161500\n",
      "161550\n",
      "161600\n",
      "161650\n",
      "161700\n",
      "161750\n",
      "161800\n",
      "161850\n",
      "161900\n",
      "161950\n",
      "162000\n",
      "162050\n",
      "162100\n",
      "162150\n",
      "162200\n",
      "162250\n",
      "162300\n",
      "162350\n",
      "162400\n",
      "162450\n",
      "Epoch: 6/20...  Training Step: 23100...  Training loss: 1.5756...  0.0761 sec/batch\n",
      "162500\n",
      "162550\n",
      "162600\n",
      "162650\n",
      "162700\n",
      "162750\n",
      "162800\n",
      "162850\n",
      "162900\n",
      "162950\n",
      "163000\n",
      "163050\n",
      "163100\n",
      "163150\n",
      "163200\n",
      "163250\n",
      "163300\n",
      "163350\n",
      "163400\n",
      "163450\n",
      "163500\n",
      "163550\n",
      "163600\n",
      "163650\n",
      "163700\n",
      "163750\n",
      "163800\n",
      "163850\n",
      "163900\n",
      "163950\n",
      "164000\n",
      "164050\n",
      "164100\n",
      "164150\n",
      "164200\n",
      "164250\n",
      "164300\n",
      "164350\n",
      "164400\n",
      "164450\n",
      "164500\n",
      "164550\n",
      "164600\n",
      "164650\n",
      "164700\n",
      "164750\n",
      "164800\n",
      "164850\n",
      "164900\n",
      "164950\n",
      "Epoch: 6/20...  Training Step: 23150...  Training loss: 1.5211...  0.0811 sec/batch\n",
      "165000\n",
      "165050\n",
      "165100\n",
      "165150\n",
      "165200\n",
      "165250\n",
      "165300\n",
      "165350\n",
      "165400\n",
      "165450\n",
      "165500\n",
      "165550\n",
      "165600\n",
      "165650\n",
      "165700\n",
      "165750\n",
      "165800\n",
      "165850\n",
      "165900\n",
      "165950\n",
      "166000\n",
      "166050\n",
      "166100\n",
      "166150\n",
      "166200\n",
      "166250\n",
      "166300\n",
      "166350\n",
      "166400\n",
      "166450\n",
      "166500\n",
      "166550\n",
      "166600\n",
      "166650\n",
      "166700\n",
      "166750\n",
      "166800\n",
      "166850\n",
      "166900\n",
      "166950\n",
      "167000\n",
      "167050\n",
      "167100\n",
      "167150\n",
      "167200\n",
      "167250\n",
      "167300\n",
      "167350\n",
      "167400\n",
      "167450\n",
      "Epoch: 6/20...  Training Step: 23200...  Training loss: 1.4582...  0.0756 sec/batch\n",
      "167500\n",
      "167550\n",
      "167600\n",
      "167650\n",
      "167700\n",
      "167750\n",
      "167800\n",
      "167850\n",
      "167900\n",
      "167950\n",
      "168000\n",
      "168050\n",
      "168100\n",
      "168150\n",
      "168200\n",
      "168250\n",
      "168300\n",
      "168350\n",
      "168400\n",
      "168450\n",
      "168500\n",
      "168550\n",
      "168600\n",
      "168650\n",
      "168700\n",
      "168750\n",
      "168800\n",
      "168850\n",
      "168900\n",
      "168950\n",
      "169000\n",
      "169050\n",
      "169100\n",
      "169150\n",
      "169200\n",
      "169250\n",
      "169300\n",
      "169350\n",
      "169400\n",
      "169450\n",
      "169500\n",
      "169550\n",
      "169600\n",
      "169650\n",
      "169700\n",
      "169750\n",
      "169800\n",
      "169850\n",
      "169900\n",
      "169950\n",
      "Epoch: 6/20...  Training Step: 23250...  Training loss: 1.5288...  0.0812 sec/batch\n",
      "170000\n",
      "170050\n",
      "170100\n",
      "170150\n",
      "170200\n",
      "170250\n",
      "170300\n",
      "170350\n",
      "170400\n",
      "170450\n",
      "170500\n",
      "170550\n",
      "170600\n",
      "170650\n",
      "170700\n",
      "170750\n",
      "170800\n",
      "170850\n",
      "170900\n",
      "170950\n",
      "171000\n",
      "171050\n",
      "171100\n",
      "171150\n",
      "171200\n",
      "171250\n",
      "171300\n",
      "171350\n",
      "171400\n",
      "171450\n",
      "171500\n",
      "171550\n",
      "171600\n",
      "171650\n",
      "171700\n",
      "171750\n",
      "171800\n",
      "171850\n",
      "171900\n",
      "171950\n",
      "172000\n",
      "172050\n",
      "172100\n",
      "172150\n",
      "172200\n",
      "172250\n",
      "172300\n",
      "172350\n",
      "172400\n",
      "172450\n",
      "Epoch: 6/20...  Training Step: 23300...  Training loss: 1.5460...  0.0995 sec/batch\n",
      "172500\n",
      "172550\n",
      "172600\n",
      "172650\n",
      "172700\n",
      "172750\n",
      "172800\n",
      "172850\n",
      "172900\n",
      "172950\n",
      "173000\n",
      "173050\n",
      "173100\n",
      "173150\n",
      "173200\n",
      "173250\n",
      "173300\n",
      "173350\n",
      "173400\n",
      "173450\n",
      "173500\n",
      "173550\n",
      "173600\n",
      "173650\n",
      "173700\n",
      "173750\n",
      "173800\n",
      "173850\n",
      "173900\n",
      "173950\n",
      "174000\n",
      "174050\n",
      "174100\n",
      "174150\n",
      "174200\n",
      "174250\n",
      "174300\n",
      "174350\n",
      "174400\n",
      "174450\n",
      "174500\n",
      "174550\n",
      "174600\n",
      "174650\n",
      "174700\n",
      "174750\n",
      "174800\n",
      "174850\n",
      "174900\n",
      "174950\n",
      "Epoch: 6/20...  Training Step: 23350...  Training loss: 1.5042...  0.1228 sec/batch\n",
      "175000\n",
      "175050\n",
      "175100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175150\n",
      "175200\n",
      "175250\n",
      "175300\n",
      "175350\n",
      "175400\n",
      "175450\n",
      "175500\n",
      "175550\n",
      "175600\n",
      "175650\n",
      "175700\n",
      "175750\n",
      "175800\n",
      "175850\n",
      "175900\n",
      "175950\n",
      "176000\n",
      "176050\n",
      "176100\n",
      "176150\n",
      "176200\n",
      "176250\n",
      "176300\n",
      "176350\n",
      "176400\n",
      "176450\n",
      "176500\n",
      "176550\n",
      "176600\n",
      "176650\n",
      "176700\n",
      "176750\n",
      "176800\n",
      "176850\n",
      "176900\n",
      "176950\n",
      "177000\n",
      "177050\n",
      "177100\n",
      "177150\n",
      "177200\n",
      "177250\n",
      "177300\n",
      "177350\n",
      "177400\n",
      "177450\n",
      "Epoch: 6/20...  Training Step: 23400...  Training loss: 1.6051...  0.0799 sec/batch\n",
      "177500\n",
      "177550\n",
      "177600\n",
      "177650\n",
      "177700\n",
      "177750\n",
      "177800\n",
      "177850\n",
      "177900\n",
      "177950\n",
      "178000\n",
      "178050\n",
      "178100\n",
      "178150\n",
      "178200\n",
      "178250\n",
      "178300\n",
      "178350\n",
      "178400\n",
      "178450\n",
      "178500\n",
      "178550\n",
      "178600\n",
      "178650\n",
      "178700\n",
      "178750\n",
      "178800\n",
      "178850\n",
      "178900\n",
      "178950\n",
      "179000\n",
      "179050\n",
      "179100\n",
      "179150\n",
      "179200\n",
      "179250\n",
      "179300\n",
      "179350\n",
      "179400\n",
      "179450\n",
      "179500\n",
      "179550\n",
      "179600\n",
      "179650\n",
      "179700\n",
      "179750\n",
      "179800\n",
      "179850\n",
      "179900\n",
      "179950\n",
      "Epoch: 6/20...  Training Step: 23450...  Training loss: 1.5843...  0.0796 sec/batch\n",
      "180000\n",
      "180050\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "# Print losses every N interations\n",
    "print_every_n = 50\n",
    "\n",
    "# Save every N iterations\n",
    "save_every_n = 200\n",
    "\n",
    "model = CharRNN(len(vocab), batch_size=batch_size, num_steps=num_steps,\n",
    "                lstm_size=lstm_size, num_layers=num_layers, \n",
    "                learning_rate=learning_rate)\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=100)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Use the line below to load a checkpoint and resume training\n",
    "    #saver.restore(sess, 'checkpoints/______.ckpt')\n",
    "    counter = 0\n",
    "    for e in range(epochs):\n",
    "        # Train network\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        loss = 0\n",
    "        for x, y in get_batches(encoded, batch_size, num_steps):\n",
    "            counter += 1\n",
    "            start = time.time()\n",
    "            feed = {model.inputs: x,\n",
    "                    model.targets: y,\n",
    "                    model.keep_prob: keep_prob,\n",
    "                    model.initial_state: new_state}\n",
    "            batch_loss, new_state, _ = sess.run([model.loss, \n",
    "                                                 model.final_state, \n",
    "                                                 model.optimizer], \n",
    "                                                 feed_dict=feed)\n",
    "            if (counter % print_every_n == 0):\n",
    "                end = time.time()\n",
    "                print('Epoch: {}/{}... '.format(e+1, epochs),\n",
    "                      'Training Step: {}... '.format(counter),\n",
    "                      'Training loss: {:.4f}... '.format(batch_loss),\n",
    "                      '{:.4f} sec/batch'.format((end-start)))\n",
    "        \n",
    "            if (counter % save_every_n == 0):\n",
    "                saver.save(sess, \"checkpoints/i{}_l{}.ckpt\".format(counter, lstm_size))\n",
    "    \n",
    "    saver.save(sess, \"checkpoints/i{}_l{}.ckpt\".format(counter, lstm_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saved checkpoints\n",
    "\n",
    "Read up on saving and loading checkpoints here: https://www.tensorflow.org/programmers_guide/variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.train.get_checkpoint_state('checkpoints')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling\n",
    "\n",
    "Now that the network is trained, we'll can use it to generate new text. The idea is that we pass in a character, then the network will predict the next character. We can use the new one, to predict the next one. And we keep doing this to generate all new text. I also included some functionality to prime the network with some text by passing in a string and building up a state from that.\n",
    "\n",
    "The network gives us predictions for each character. To reduce noise and make things a little less random, I'm going to only choose a new character from the top N most likely characters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pick_top_n(preds, vocab_size, top_n=5):\n",
    "    p = np.squeeze(preds)\n",
    "    p[np.argsort(p)[:-top_n]] = 0\n",
    "    p = p / np.sum(p)\n",
    "    c = np.random.choice(vocab_size, 1, p=p)[0]\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(checkpoint, n_samples, lstm_size, vocab_size, prime=\"The \"):\n",
    "    samples = [c for c in prime]\n",
    "    model = CharRNN(len(vocab), lstm_size=lstm_size, sampling=True)\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, checkpoint)\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        for c in prime:\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0,0] = vocab_to_int[c]\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.prediction, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "        c = pick_top_n(preds, len(vocab))\n",
    "        samples.append(int_to_vocab[c])\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            x[0,0] = c\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.prediction, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "            c = pick_top_n(preds, len(vocab))\n",
    "            samples.append(int_to_vocab[c])\n",
    "        \n",
    "    return ''.join(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, pass in the path to a checkpoint and sample from the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.train.latest_checkpoint('checkpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "checkpoint = tf.train.latest_checkpoint('checkpoints')\n",
    "samp = sample(checkpoint, 2000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "checkpoint = 'checkpoints/i200_l512.ckpt'\n",
    "samp = sample(checkpoint, 1000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "checkpoint = 'checkpoints/i600_l512.ckpt'\n",
    "samp = sample(checkpoint, 1000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "checkpoint = 'checkpoints/i1200_l512.ckpt'\n",
    "samp = sample(checkpoint, 1000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
