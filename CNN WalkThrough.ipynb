{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 14s 1us/step\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "(X_train,y_train),(X_test,y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Playing with shape\n",
    "#x_temp=X_train\n",
    "#x_temp.reshape(-1,60000).shape\n",
    "x_temp = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAEYCAYAAACgDKohAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nNAAA3mgaAABv/wdxeI71a9p/ugAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x18296f0f60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(0,6):\n",
    "    plt.subplot(2,3,i+1)\n",
    "    plt.imshow(X_train[i],cmap=\"gray\")\n",
    "    plt.title(str(y_train[i]))\n",
    "    plt.axis(\"off\")\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABHEAAADBCAYAAABIbSwnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n4gAAAADUgCEOAAAAQA0Y4gAAAADUwP8Fc1l4oyGRvWAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1820d6fd30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(20,20))\n",
    "for i in range(6):\n",
    "    ax = fig.add_subplot(1,6,i+1,xticks=[],yticks=[])\n",
    "    ax.imshow(X_train[i],cmap=\"gray\")\n",
    "    ax.set_title(str(y_train[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#rescale [0,255] -> [0,1]\n",
    "X_train = X_train.astype(\"float32\")/255\n",
    "X_test = X_test.astype(\"float32\")/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OnehotLabel\n",
    "import pandas as pd\n",
    "y_train=np.array(pd.get_dummies(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Reshaping X_train\n",
    "X_train_matrix = X_train.reshape(60000,-1)\n",
    "X_train_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 32)                25120     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                330       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 25,450\n",
      "Trainable params: 25,450\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense,Activation\n",
    "model = Sequential()\n",
    "model.add(Dense(32,input_dim=X_train_matrix.shape[1]))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation(\"softmax\"))\n",
    "model.compile(loss=\"categorical_crossentropy\",optimizer=\"adam\",metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 2s 29us/step\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train_matrix,y_train,epochs=200,batch_size=124,verbose=0)\n",
    "score = model.evaluate(X_train_matrix,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_flatten = X_test.reshape(X_test.shape[0],-1)\n",
    "y_test_one_hot = np.array(pd.get_dummies(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 31us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.96430000000000005"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_test = model.evaluate(X_test_flatten,y_test_one_hot)\n",
    "score_test[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model in Lecture\n",
    "from keras.layers import Dropout\n",
    "model1 = Sequential()\n",
    "model1.add(Dense(512,input_dim=X_train_matrix.shape[1]))\n",
    "model1.add(Dropout(0.5))\n",
    "model1.add(Dense(512))\n",
    "model1.add(Dropout(0.7))\n",
    "model1.add(Dense(10,activation=\"softmax\"))\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 139us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9.9199999999999999"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.compile(loss=\"categorical_crossentropy\",optimizer=\"rmsprop\",metrics=[\"accuracy\"])\n",
    "score_1 = model1.evaluate(X_test_flatten,y_test_one_hot)\n",
    "100*score_1[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/6\n",
      "47872/48000 [============================>.] - ETA: 0s - loss: 0.6034 - acc: 0.8286\n",
      "Epoch 00001: val_loss improved from inf to 0.31437, saving model to ./best_weight_mnist.hdf5\n",
      "48000/48000 [==============================] - 10s 217us/step - loss: 0.6025 - acc: 0.8289 - val_loss: 0.3144 - val_acc: 0.9157\n",
      "Epoch 2/6\n",
      "47744/48000 [============================>.] - ETA: 0s - loss: 0.4850 - acc: 0.8694- ETA - ETA: 6s - - ETA: 4s - loss - ETA: 2s - l - ETA: 0s - loss: 0.4873 - acc: \n",
      "Epoch 00002: val_loss did not improve\n",
      "48000/48000 [==============================] - 10s 209us/step - loss: 0.4851 - acc: 0.8693 - val_loss: 0.3626 - val_acc: 0.9019\n",
      "Epoch 3/6\n",
      "47744/48000 [============================>.] - ETA: 0s - loss: 0.4497 - acc: 0.8770- ETA: 3s - loss: 0.4563 - acc:  - ETA: 0s - loss: 0.4510 - acc:\n",
      "Epoch 00003: val_loss did not improve\n",
      "48000/48000 [==============================] - 9s 191us/step - loss: 0.4492 - acc: 0.8770 - val_loss: 0.3312 - val_acc: 0.9093\n",
      "Epoch 4/6\n",
      "47744/48000 [============================>.] - ETA: 0s - loss: 0.4246 - acc: 0.8829\n",
      "Epoch 00004: val_loss did not improve\n",
      "48000/48000 [==============================] - 10s 203us/step - loss: 0.4243 - acc: 0.8830 - val_loss: 0.3156 - val_acc: 0.9154\n",
      "Epoch 5/6\n",
      "47744/48000 [============================>.] - ETA: 0s - loss: 0.4079 - acc: 0.8864\n",
      "Epoch 00005: val_loss did not improve\n",
      "48000/48000 [==============================] - 10s 212us/step - loss: 0.4083 - acc: 0.8862 - val_loss: 0.3377 - val_acc: 0.9016\n",
      "Epoch 6/6\n",
      "47744/48000 [============================>.] - ETA: 0s - loss: 0.4061 - acc: 0.8873- ETA: 0s - loss: 0.4054 - acc: \n",
      "Epoch 00006: val_loss improved from 0.31437 to 0.30520, saving model to ./best_weight_mnist.hdf5\n",
      "48000/48000 [==============================] - 10s 213us/step - loss: 0.4059 - acc: 0.8873 - val_loss: 0.3052 - val_acc: 0.9177\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "checkpointer = ModelCheckpoint(filepath=\"./best_weight_mnist.hdf5\",verbose=1,\n",
    "                              save_best_only=True)\n",
    "hist = model1.fit(X_train_matrix,y_train,batch_size=128,epochs=6,validation_split=0.2,\n",
    "                 callbacks=[checkpointer],verbose=1,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 117us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "91.459999999999994"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_1 = model1.evaluate(X_test_flatten,y_test_one_hot)\n",
    "100*score_1[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolution Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##OK WITH Lossing DATA then PADDING = \"VALID\"\n",
    "# IF NOT OK WITH LOSSING THE DATA THEN PADDING = \"SAME\"\n",
    "#Depends on the stride of our filter\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D\n",
    "Conv2D(filters,kernel_size,strides,padding,activation=\"relu\",input_shape)\n",
    "#filters - the number of filter\n",
    "#Kernel_size - number specifying both height and width of (square)\n",
    "#convolution window.\n",
    "#activation-typically relu.if not specify anything then no activation function\n",
    "#strongly encouraged to specify relu in activation \n",
    "#strides-the stride of convolution , if not specified then default is 1.\n",
    "#padding - one of \"valid\" or \"same\" if nothing is not specified then by default is set to \"valid\"\n",
    "#NOTE : it is possible to reperesent both kernel_size and strides as either a number or a tuple\n",
    "#When using your convolution layer as the first layer (appearing after the input layer)in a model,we must provide a input shape\n",
    "#And donot specify a input shape if its not the first layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.convolutional.Conv2D at 0x181d4befd0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Say I'm constructing a CNN and my input layer accepts a grayscale images that are \n",
    "#200by200 pixels(corresponding to a 3D array with height 200, width 200 depth 1).Then\n",
    "#say I'd like the next layer to be a convolution layer with 16 filters,each with width and \n",
    "#height of 3. When performing the convoultion, I'd like the filter to jump two pixels\n",
    "#at a time. I also don't want the filter to extend outside of the image bondaries \n",
    "#in other words,I don't want to pad the images with zeros. Then, to construct this convolution \n",
    "#layer, I would use the following line of command\n",
    "\n",
    "\n",
    "\n",
    "Conv2D(filters=16,kernel_size=2,strides=2,activation=\"relu\",input_shape=(200,200,1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.convolutional.Conv2D at 0x181f76c5f8>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Say I'd like the next layer in my CNN to be a convolution layer that takes \n",
    "#the layer constructed in Example 1 as input. Say I'd like my new layer to have 32 \n",
    "#filters,each with a height and width of 3.When performing convolution, I'd like the filter \n",
    "#to jump 1 pixels at a time. I want the convolution layer to see all regions of previous layer, and \n",
    "#so I don't mind if the filter hangs over the edge of previous layer when its performing\n",
    "#the convolution. Then, to construct this convolution layer I would use following\n",
    "#line of code\n",
    "Conv2D(filters=32,kernel_size=3,padding=\"same\",activation=\"relu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example #3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.convolutional.Conv2D at 0x181f76c908>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#In this case, there are 64 filters and the size of filter is (2x2), and the \n",
    "#layer has a Relu activation function.The other arguments in the layer use the\n",
    "#default values, so this convolution uses a stride of 1 and the padding is set to\n",
    "#\"valid\"(so no padding)\n",
    "Conv2D(64,(2,2),activation=\"relu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_12 (Conv2D)           (None, 100, 100, 16)      80        \n",
      "=================================================================\n",
      "Total params: 80\n",
      "Trainable params: 80\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters=16,kernel_size=2,strides=2,padding=\"valid\",activation=\"relu\",\n",
    "                input_shape = (200,200,1)))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Take note how the number of parameter in the convolution layer changes\n",
    "## Params # 80 \n",
    "## also notice the shape of the convolution layer changes . This corresponds\n",
    "# to value under Output shape in the printed output.\n",
    "# None corresponds to the batch size, and the convolution layer has. a height of 100,\n",
    "# width of 100 and depth of 16\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formula: Number of Parameter in a convolution Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of parameter in a covolution layer depends on the supplied values of **filters**, **kernel_size** and **input_shape**.\n",
    "K - the number of filter in a convolution layer\n",
    "F -  the height and width of convolution filters\n",
    "D_in - the depth of previous layer\n",
    "Notice that K = filters and F=kernel_size . likewise D_in the last value in the input_shape tuple.\n",
    "Since there are (**F x F x D_in**) weights per filter, and the convolution layer is composed of K filters, the total number of weights in convolution layer is equal to **K x F x F x D_in** and there is a bias term per filter thus the number of parameters in convolution layer is give by **(K x F x F x D_in + K)**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2*2*16*1 + 16\n",
    "## Above example number of parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formula: Shape of a Convolution Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape of a convoution layer depends on the supplied values of **kernel_size, input_shape, padding** and **stride**.\n",
    "* K - the number of filter in the convolution layer \n",
    "* F - the height and width of convolution filters\n",
    "* S - the stride of convolution filters\n",
    "* H_in - the height of the previous layer\n",
    "* W_in - the width of the previous layer\n",
    "Notice that K = filters , F = kernel_size and S = stride and H_in and W_in is equal to first and second value of input_shape respectively.\n",
    "* The depth of convolution layer will be always be equal to **K**(no of filter)\n",
    "IF **padding = \"same\"**, then spatial dimension of the convolution layer are the following\n",
    "* height = ceil(float(H_in)/float(S))\n",
    "* width = ceil(float(W_in)/float(S))\n",
    "If **padding = \"valid\"**, then spatial dimension of the convolution layer are the following:\n",
    "* height = ceil(float(H_in - F +1)/float(S))\n",
    "* width = ceil(float(W_in - F +1)/float(S))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quiz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_17 (Conv2D)           (None, 64, 64, 32)        896       \n",
      "=================================================================\n",
      "Total params: 896\n",
      "Trainable params: 896\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters=32,kernel_size=3,strides=2,padding=\"same\",\n",
    "                activation = \"relu\",input_shape=(128,128,3)))\n",
    "model.summary()\n",
    "#Depth of convolution layer is equal to number of filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polling of layers!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Max Pooling layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# So if want to find a complex pattern in our data then we will have many filters \n",
    "#and dimentionalty we will increase thus it will overfit our training data\n",
    "#THAT is the reason we introduce polling layer.\n",
    "#first we will discuss about max polling layer \n",
    "#Max Pooling Layer\n",
    "#Window Size: 2*2\n",
    "#Strides 2\n",
    "# So it reduces the dimension of convolution layer (Half of original size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global average Pooling Layer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A global average pooling layer takes a 3D array and turns it into a vector.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Max Pooling Layers in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.pooling.MaxPooling2D at 0x181e5d7780>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import MaxPooling2D\n",
    "MaxPooling2D(pool_size=2,strides=2,padding=\"valid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arguments\n",
    "* pool_size - Number specifying the height and width of the pooling window\n",
    "* strides - The vertical and horizontal stride. IF don't specify anything,strides will default to pool_size\n",
    "* padding - One of the \"valid\" or \"same\". If don't specify anything padding is set to \"valid\"\n",
    "Input could be number or tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example \n",
    "Say I'm constructing a CNN, and I'd like to reduce the dimentionality of a convolution layer by following it with a max pooling layer.Say the convolution layer has size **(100,100,15)** and I'd like the max pooling layer to have a size **(50,50,15)**. I can do this by using 2 x 2 window in my max pooling layer, with a stride of 2,which could be constructed in the following line of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.pooling.MaxPooling2D at 0x181e5ea2e8>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MaxPooling2D(pool_size=2,strides=2)\n",
    "# IF you'd like to use a stride of 1, but still keep the size of window at \n",
    "#2 x 2 then we could use\n",
    "MaxPooling2D(pool_size=2,strides=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the Dimentionality of Max Pooling Layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "max_pooling2d_5 (MaxPooling2 (None, 50, 50, 15)        0         \n",
      "=================================================================\n",
      "Total params: 0\n",
      "Trainable params: 0\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import MaxPooling2D\n",
    "from keras.models import Sequential\n",
    "model = Sequential()\n",
    "model.add(MaxPooling2D(pool_size=2,strides=2,input_shape=(100,100,15)))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_18 (Conv2D)           (None, 32, 32, 16)        208       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 16, 16, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 16, 16, 32)        2080      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 8, 8, 64)          8256      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 500)               512500    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                5010      \n",
      "=================================================================\n",
      "Total params: 528,054\n",
      "Trainable params: 528,054\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters=16,kernel_size=2,padding=\"same\",activation=\"relu\",\n",
    "                input_shape=(32,32,3)))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Conv2D(filters=32,kernel_size=2,padding=\"same\",activation=\"relu\",))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Conv2D(filters=64,kernel_size=2,padding=\"same\",activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(500,activation=\"relu\"))\n",
    "model.add(Dense(10,activation=\"softmax\"))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Things to Remember\n",
    "* Always add a ReLU activation to Conv2D layer in your CNN. With the exception of the final layer in the network, Dense layers should also have a ReLU activation function\n",
    "* When constructing a network for classification, the final layer in the network should be a dense layer with a Softmax activation function. The number of nodes in the final layer should be equal the total number of classes in the dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR-10 --CNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      " 89333760/170498071 [==============>...............] - ETA: 8:23"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-f8cd962f0a62>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# LOAD THE PRE -SHUFFLED TRAIN AND TEST DATA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcifar10\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/datasets/cifar10.py\u001b[0m in \u001b[0;36mload_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mdirname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'cifar-10-batches-py'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0morigin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morigin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morigin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muntar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mnum_train_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget_file\u001b[0;34m(fname, origin, untar, md5_hash, file_hash, cache_subdir, hash_algorithm, extract, archive_format, cache_dir)\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m                 \u001b[0murlretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morigin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdl_progress\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mURLError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morigin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrno\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36murlretrieve\u001b[0;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m                 \u001b[0mblock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    447\u001b[0m             \u001b[0;31m# Amount is given, implement using readinto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    491\u001b[0m         \u001b[0;31m# connection, and the user is reading more bytes than will be provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;31m# (for example, reading in 1k chunks)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0;31m# Ideally, we would raise IncompleteRead if the content-length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1007\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1008\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1009\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1010\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1011\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m    869\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Read on closed or unwrapped SSL socket.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 871\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    872\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mSSLError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mSSL_ERROR_EOF\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuppress_ragged_eofs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m    629\u001b[0m         \"\"\"\n\u001b[1;32m    630\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m             \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import keras \n",
    "from keras.datasets import cifar10\n",
    "\n",
    "# LOAD THE PRE -SHUFFLED TRAIN AND TEST DATA\n",
    "(x_train,y_train),(x_test,y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "fig = plt.figure(figsize=(20,5))\n",
    "for i in range(36):\n",
    "    ax = fig.add_subplot(3,12,i+1,xticks=[],yticks=[])\n",
    "    ax.imshow(np.squeeze(x_train[i])) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
