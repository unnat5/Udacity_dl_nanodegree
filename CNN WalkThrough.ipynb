{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "(X_train,y_train),(X_test,y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Playing with shape\n",
    "#x_temp=X_train\n",
    "#x_temp.reshape(-1,60000).shape\n",
    "x_temp = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,6):\n",
    "    plt.subplot(2,3,i+1)\n",
    "    plt.imshow(X_train[i],cmap=\"gray\")\n",
    "    plt.title(str(y_train[i]))\n",
    "    plt.axis(\"off\")\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,20))\n",
    "for i in range(6):\n",
    "    ax = fig.add_subplot(1,6,i+1,xticks=[],yticks=[])\n",
    "    ax.imshow(X_train[i],cmap=\"gray\")\n",
    "    ax.set_title(str(y_train[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rescale [0,255] -> [0,1]\n",
    "X_train = X_train.astype(\"float32\")/255\n",
    "X_test = X_test.astype(\"float32\")/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OnehotLabel\n",
    "import pandas as pd\n",
    "y_train=np.array(pd.get_dummies(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Reshaping X_train\n",
    "X_train_matrix = X_train.reshape(60000,-1)\n",
    "X_train_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense,Activation\n",
    "model = Sequential()\n",
    "model.add(Dense(32,input_dim=X_train_matrix.shape[1]))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation(\"softmax\"))\n",
    "model.compile(loss=\"categorical_crossentropy\",optimizer=\"adam\",metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train_matrix,y_train,epochs=200,batch_size=124,verbose=1)\n",
    "score = model.evaluate(X_train_matrix,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_flatten = X_test.reshape(X_test.shape[0],-1)\n",
    "y_test_one_hot = np.array(pd.get_dummies(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_test = model.evaluate(X_test_flatten,y_test_one_hot)\n",
    "score_test[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model in Lecture\n",
    "from keras.layers import Dropout\n",
    "model1 = Sequential()\n",
    "model1.add(Dense(512,input_dim=X_train_matrix.shape[1]))\n",
    "model1.add(Dropout(0.5))\n",
    "model1.add(Dense(512))\n",
    "model1.add(Dropout(0.7))\n",
    "model1.add(Dense(10,activation=\"softmax\"))\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.compile(loss=\"categorical_crossentropy\",optimizer=\"rmsprop\",metrics=[\"accuracy\"])\n",
    "score_1 = model1.evaluate(X_test_flatten,y_test_one_hot)\n",
    "100*score_1[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "checkpointer = ModelCheckpoint(filepath=\"./best_weight_mnist.hdf5\",verbose=1,\n",
    "                              save_best_only=True)\n",
    "hist = model1.fit(X_train_matrix,y_train,batch_size=128,epochs=6,validation_split=0.2,\n",
    "                 callbacks=[checkpointer],verbose=1,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_1 = model1.evaluate(X_test_flatten,y_test_one_hot)\n",
    "100*score_1[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolution Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##OK WITH Lossing DATA then PADDING = \"VALID\"\n",
    "# IF NOT OK WITH LOSSING THE DATA THEN PADDING = \"SAME\"\n",
    "#Depends on the stride of our filter\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D\n",
    "Conv2D(filters,kernel_size,strides,padding,activation=\"relu\",input_shape)\n",
    "#filters - the number of filter\n",
    "#Kernel_size - number specifying both height and width of (square)\n",
    "#convolution window.\n",
    "#activation-typically relu.if not specify anything then no activation function\n",
    "#strongly encouraged to specify relu in activation \n",
    "#strides-the stride of convolution , if not specified then default is 1.\n",
    "#padding - one of \"valid\" or \"same\" if nothing is not specified then by default is set to \"valid\"\n",
    "#NOTE : it is possible to reperesent both kernel_size and strides as either a number or a tuple\n",
    "#When using your convolution layer as the first layer (appearing after the input layer)in a model,we must provide a input shape\n",
    "#And donot specify a input shape if its not the first layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Say I'm constructing a CNN and my input layer accepts a grayscale images that are \n",
    "#200by200 pixels(corresponding to a 3D array with height 200, width 200 depth 1).Then\n",
    "#say I'd like the next layer to be a convolution layer with 16 filters,each with width and \n",
    "#height of 3. When performing the convoultion, I'd like the filter to jump two pixels\n",
    "#at a time. I also don't want the filter to extend outside of the image bondaries \n",
    "#in other words,I don't want to pad the images with zeros. Then, to construct this convolution \n",
    "#layer, I would use the following line of command\n",
    "\n",
    "\n",
    "\n",
    "Conv2D(filters=16,kernel_size=2,strides=2,activation=\"relu\",input_shape=(200,200,1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Say I'd like the next layer in my CNN to be a convolution layer that takes \n",
    "#the layer constructed in Example 1 as input. Say I'd like my new layer to have 32 \n",
    "#filters,each with a height and width of 3.When performing convolution, I'd like the filter \n",
    "#to jump 1 pixels at a time. I want the convolution layer to see all regions of previous layer, and \n",
    "#so I don't mind if the filter hangs over the edge of previous layer when its performing\n",
    "#the convolution. Then, to construct this convolution layer I would use following\n",
    "#line of code\n",
    "Conv2D(filters=32,kernel_size=3,padding=\"same\",activation=\"relu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example #3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In this case, there are 64 filters and the size of filter is (2x2), and the \n",
    "#layer has a Relu activation function.The other arguments in the layer use the\n",
    "#default values, so this convolution uses a stride of 1 and the padding is set to\n",
    "#\"valid\"(so no padding)\n",
    "Conv2D(64,(2,2),activation=\"relu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters=16,kernel_size=2,strides=2,padding=\"valid\",activation=\"relu\",\n",
    "                input_shape = (200,200,1)))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Take note how the number of parameter in the convolution layer changes\n",
    "## Params # 80 \n",
    "## also notice the shape of the convolution layer changes . This corresponds\n",
    "# to value under Output shape in the printed output.\n",
    "# None corresponds to the batch size, and the convolution layer has. a height of 100,\n",
    "# width of 100 and depth of 16\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formula: Number of Parameter in a convolution Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of parameter in a covolution layer depends on the supplied values of **filters**, **kernel_size** and **input_shape**.\n",
    "K - the number of filter in a convolution layer\n",
    "F -  the height and width of convolution filters\n",
    "D_in - the depth of previous layer\n",
    "Notice that K = filters and F=kernel_size . likewise D_in the last value in the input_shape tuple.\n",
    "Since there are (**F x F x D_in**) weights per filter, and the convolution layer is composed of K filters, the total number of weights in convolution layer is equal to **K x F x F x D_in** and there is a bias term per filter thus the number of parameters in convolution layer is give by **(K x F x F x D_in + K)**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2*2*16*1 + 16\n",
    "## Above example number of parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formula: Shape of a Convolution Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape of a convoution layer depends on the supplied values of **kernel_size, input_shape, padding** and **stride**.\n",
    "* K - the number of filter in the convolution layer \n",
    "* F - the height and width of convolution filters\n",
    "* S - the stride of convolution filters\n",
    "* H_in - the height of the previous layer\n",
    "* W_in - the width of the previous layer\n",
    "Notice that K = filters , F = kernel_size and S = stride and H_in and W_in is equal to first and second value of input_shape respectively.\n",
    "* The depth of convolution layer will be always be equal to **K**(no of filter)\n",
    "IF **padding = \"same\"**, then spatial dimension of the convolution layer are the following\n",
    "* height = ceil(float(H_in)/float(S))\n",
    "* width = ceil(float(W_in)/float(S))\n",
    "If **padding = \"valid\"**, then spatial dimension of the convolution layer are the following:\n",
    "* height = ceil(float(H_in - F +1)/float(S))\n",
    "* width = ceil(float(W_in - F +1)/float(S))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quiz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters=32,kernel_size=3,strides=2,padding=\"same\",\n",
    "                activation = \"relu\",input_shape=(128,128,3)))\n",
    "model.summary()\n",
    "#Depth of convolution layer is equal to number of filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polling of layers!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Max Pooling layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So if want to find a complex pattern in our data then we will have many filters \n",
    "#and dimentionalty we will increase thus it will overfit our training data\n",
    "#THAT is the reason we introduce polling layer.\n",
    "#first we will discuss about max polling layer \n",
    "#Max Pooling Layer\n",
    "#Window Size: 2*2\n",
    "#Strides 2\n",
    "# So it reduces the dimension of convolution layer (Half of original size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global average Pooling Layer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A global average pooling layer takes a 3D array and turns it into a vector.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Max Pooling Layers in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import MaxPooling2D\n",
    "MaxPooling2D(pool_size=2,strides=2,padding=\"valid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arguments\n",
    "* pool_size - Number specifying the height and width of the pooling window\n",
    "* strides - The vertical and horizontal stride. IF don't specify anything,strides will default to pool_size\n",
    "* padding - One of the \"valid\" or \"same\". If don't specify anything padding is set to \"valid\"\n",
    "Input could be number or tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example \n",
    "Say I'm constructing a CNN, and I'd like to reduce the dimentionality of a convolution layer by following it with a max pooling layer.Say the convolution layer has size **(100,100,15)** and I'd like the max pooling layer to have a size **(50,50,15)**. I can do this by using 2 x 2 window in my max pooling layer, with a stride of 2,which could be constructed in the following line of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MaxPooling2D(pool_size=2,strides=2)\n",
    "# IF you'd like to use a stride of 1, but still keep the size of window at \n",
    "#2 x 2 then we could use\n",
    "MaxPooling2D(pool_size=2,strides=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the Dimentionality of Max Pooling Layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import MaxPooling2D\n",
    "from keras.models import Sequential\n",
    "model = Sequential()\n",
    "model.add(MaxPooling2D(pool_size=2,strides=2,input_shape=(100,100,15)))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters=16,kernel_size=2,padding=\"same\",activation=\"relu\",\n",
    "                input_shape=(32,32,3)))\n",
    "model.add(Conv2D(filters=16,kernel_size=2,padding=\"same\",activation=\"relu\",))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Conv2D(filters=32,kernel_size=2,padding=\"same\",activation=\"relu\",))\n",
    "model.add(Conv2D(filters=32,kernel_size=2,padding=\"same\",activation=\"relu\",))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Conv2D(filters=64,kernel_size=2,padding=\"same\",activation=\"relu\"))\n",
    "model.add(Conv2D(filters=64,kernel_size=2,padding=\"same\",activation=\"relu\",))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(500,activation=\"relu\"))\n",
    "model.add(Dense(10,activation=\"softmax\"))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Things to Remember\n",
    "* Always add a ReLU activation to Conv2D layer in your CNN. With the exception of the final layer in the network, Dense layers should also have a ReLU activation function\n",
    "* When constructing a network for classification, the final layer in the network should be a dense layer with a Softmax activation function. The number of nodes in the final layer should be equal the total number of classes in the dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR-10 --CNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras \n",
    "from keras.datasets import cifar10\n",
    "\n",
    "# LOAD THE PRE -SHUFFLED TRAIN AND TEST DATA\n",
    "(x_train,y_train),(x_test,y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "fig = plt.figure(figsize=(20,5))\n",
    "for i in range(36):\n",
    "    ax = fig.add_subplot(3,12,i+1,xticks=[],yticks=[])\n",
    "    ax.imshow(np.squeeze(x_train[i])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalizing the data\n",
    "x_train = x_train.astype(\"float32\")/255\n",
    "x_test=x_test.astype(\"float32\")/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "import pandas as pd\n",
    "#one hot encoding in keras\n",
    "num_classes = len(np.unique(y_train))\n",
    "y_train = keras.utils.to_categorical(y_train,num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#break data into training and validation set\n",
    "(x_train,x_valid)=x_train[5000:],x_train[:5000]\n",
    "(y_train,y_valid)=y_train[5000:],y_train[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(x_train.shape)\n",
    "#print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Infereing\n",
    "print(x_train.shape)\n",
    "print(y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print number of training, validation ,and test images\n",
    "print(x_train.shape[0],\"training samples\")\n",
    "print(x_test.shape[0],\"testing samples\")\n",
    "print(x_valid.shape[0],\"validating samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Model-- Regular Vanilla Neural Network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#falttening of image \n",
    "x_train_vanilla = x_train.reshape(x_train.shape[0],-1)\n",
    "x_valid_vanilla = x_valid.reshape(x_valid.shape[0],-1)\n",
    "x_test_vanilla = x_test.reshape(x_test.shape[0],-1)\n",
    "print(x_train_vanilla.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vanilla Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout\n",
    "model = Sequential()\n",
    "model.add(Dense(1000,activation=\"relu\",input_dim=x_train_vanilla.shape[1]))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(512,activation=\"relu\"))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(num_classes,activation=\"softmax\"))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile model\n",
    "model.compile(loss=\"categorical_crossentropy\",optimizer=\"rmsprop\",\n",
    "             metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training the model!!\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "#train model\n",
    "checkpointer = ModelCheckpoint(filepath=\"MLP.weights.best.hdf5\",verbose=1,\n",
    "                              save_best_only=True)\n",
    "hist = model.fit(x_train_vanilla,y_train,batch_size=32,epochs=20,\n",
    "                validation_data=(x_valid_vanilla,y_valid),callbacks=[checkpointer],\n",
    "                verbose=2,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the weights that yeilds the best calidation accuracy\n",
    "model.load_weights(\"MLP.weights.best.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Checking the accuracy\n",
    "accuracy=model.evaluate(x_test_vanilla,y_test)\n",
    "print(100*accuracy[1],\"Testing accuracy of the data in vanilla model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second Model --CNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D,MaxPooling2D,Flatten,Dense,Dropout\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters=16,activation=\"relu\",padding=\"same\",kernel_size=2,\n",
    "                input_shape=(x_train.shape[1],x_train.shape[2],x_train.shape[3])))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Conv2D(filters=32,activation=\"relu\",padding=\"same\",kernel_size=2))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Conv2D(filters=64,activation=\"relu\",padding=\"same\",kernel_size=2))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024,activation=\"relu\"))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(510,activation=\"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes,activation=\"softmax\"))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compling the model\n",
    "from keras import optimizers\n",
    "adam = optimizers.Adam()\n",
    "model.compile(loss=\"categorical_crossentropy\",optimizer=adam,\n",
    "             metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "checkpointer_cnn = ModelCheckpoint(filepath=\"CNN.best.weights.hdf5\",\n",
    "                                  verbose=1,save_best_only=True)\n",
    "model.fit(x_train,y_train,verbose=2,batch_size=124,epochs=10,\n",
    "          callbacks=[checkpointer_cnn],validation_data=(x_valid,y_valid),\n",
    "         shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"CNN.best.weights.hdf5\")\n",
    "score = model.evaluate(x_test,y_test)\n",
    "print(100*score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(100*score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Agumentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "checkpoint = ModelCheckpoint(filepath=\"CNN.aug_best.weights.hdf5\",save_best_only=True,verbose=2)\n",
    "datagen = ImageDataGenerator(featurewise_center=True,featurewise_std_normalization=True,\n",
    "                            rotation_range=20,width_shift_range=0.2,height_shift_range=0.2,\n",
    "                            horizontal_flip=True)\n",
    "datagen.fit(x_train)\n",
    "hist = model.fit_generator(datagen.flow(x_train,y_train,batch_size=124),steps_per_epoch=len(x_train)/124,epochs=100,\n",
    "                          callbacks=[checkpoint],validation_data = (x_valid,y_valid),verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"CNN.aug_best.weights.hdf5\")\n",
    "score = model.evaluate(x_test,y_test)\n",
    "print(100*score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(100*score[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and Config Agumented Image Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage\n",
    "from skimage import transform\n",
    "x_train[1].shape\n",
    "x_temp_128_18 = transform.resize(x_train[1],(128,18,3))\n",
    "x_train = np.array([transform.resize(train,(128,18,3)) for train in x_train])\n",
    "x_train.shape\n",
    "x_valid = np.array([transform.resize(valid,(128,18,3)) for valid in x_valid])\n",
    "x_test = np.array([transform.resize(test,(128,18,3)) for test in x_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape\n",
    "x_valid.shape\n",
    "plt.imshow(x_test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "datagen_train = ImageDataGenerator(width_shift_range=0.1,\n",
    "                                  height_shift_range=0.1,\n",
    "                                  horizontal_flip = True)\n",
    "datagen_train.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#take a subset of training data\n",
    "x_train_subset = x_train[:12]\n",
    "\n",
    "#Visualize subset of taining data\n",
    "fig = plt.figure(figsize=(20,2))\n",
    "for i in range(0,len(x_train_subset)):\n",
    "    ax = fig.add_subplot(1,12,i+1,xticks=[],yticks=[])\n",
    "    ax.imshow(x_train_subset[i])\n",
    "fig.suptitle(\"Subset of Original Training Images\",fontsize=20)\n",
    "plt.show()\n",
    "\n",
    "# Visualize agumented images\n",
    "fig = plt.figure(figsize=(20,2))\n",
    "for x_batch in datagen_train.flow(x_train_subset,batch_size=12):\n",
    "    for i in range(0,12):\n",
    "        ax = fig.add_subplot(1,12,i+1,xticks=[],yticks=[])\n",
    "        ax.imshow(x_batch[i])\n",
    "    fig.suptitle(\"Agumented Images\",fontsize=20)\n",
    "    plt.show()\n",
    "    break;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D,MaxPooling2D,Flatten,Dense,Dropout\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters=16,activation=\"relu\",padding=\"same\",kernel_size=(8,2),\n",
    "                input_shape=(x_train.shape[1],x_train.shape[2],x_train.shape[3])))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Conv2D(filters=32,activation=\"relu\",padding=\"same\",kernel_size=(8,2)))\n",
    "model.add(MaxPooling2D(pool_size=(2)))\n",
    "model.add(Conv2D(filters=64,activation=\"relu\",padding=\"same\",kernel_size=(8,2)))\n",
    "model.add(MaxPooling2D(pool_size=(2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(500,activation=\"relu\"))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(num_classes,activation=\"softmax\"))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\",optimizer=\"adam\",\n",
    "             metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "batch_size=32\n",
    "epochs =100\n",
    "checkpointer=ModelCheckpoint(filepath=\"CNN.aug_best.weights.hdf5\",\n",
    "                            verbose=1,save_best_only=True)\n",
    "model.fit_generator(datagen_train.flow(x_train,y_train,batch_size=batch_size),\n",
    "                                      epochs=epochs,verbose=2,callbacks=[checkpointer],\n",
    "                                      validation_data=(x_valid,y_valid),\n",
    "                                      validation_steps=x_valid.shape[0]//batch_size,\n",
    "                                      steps_per_epoch=x_train.shape[0]//batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing CNNs\n",
    "* So some CNNs works and others they don't!\n",
    "* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import applications\n",
    "#build the VGG16 network\n",
    "modle = applications.VGG16(include_top=False,\n",
    "                          weights=\"imagenet\")\n",
    "#get the symbolic outputs of each \"key\" layer (we gave them unique name).\n",
    "layer_dict = dict([(layer.name,layer) for layer in model.layers])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "layer_name = \"block5_conv3\"\n",
    "filter_index = 0 # can be any value form 0 to 511, as there are 512 filters in that layer \n",
    "#build a loss function that maximizes the activation\n",
    "# of the nth filter of the layer considered\n",
    "layer_output = layer_dict[layer_name].output\n",
    "loss = K.mean(layer_output[:,:,:,filter_index])\n",
    "#compute the gradient of the input picture wrt this loss\n",
    "grads = K.gradients(loss,input_img)[0]\n",
    "grads /= (K.sqrt(K.mean(K.square(grads)))+1e-5)\n",
    "#this function returns the loss and grads given the input picture \n",
    "iterate = K.function([input_img],[loss,grads])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AlexNet\n",
    "* Pioneered the use of the ReLu activation function and dropout as a technique for avoiding overfitting\n",
    "## VGG Architecture \n",
    "* Came from visual geometry group by Oxford University\n",
    "* VGG 16\n",
    "* VGG 19\n",
    "* 3X3 convolutions\n",
    "* Broken up by 2x2 pooling layers\n",
    "* Finshed with 3 fully connected layers\n",
    "* Pioneered the exclusive use of small 3x3 convolution windows(filter)\n",
    "* AlexNet had 11x11 windows(which was way bigger than VGG Architecture)\n",
    "## ResNet Archictecture \n",
    "* ResNet is kind of like VGG, but and not the same structure is repeated again and again for layer after layers like VGG\n",
    "* The largest having a groundbreaking 152 layers.\n",
    "* Previous researcher tried to have similar deeper layers but as performance increased for inital iterations but after some time it fell(performance declined) -- vanishing gradient problem.\n",
    "* ResNet team added connections to their very deep CNN that skipped layers so that gradient signal has a shorter route to travel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing CNNs\n",
    "* Each layer in the grid represents a pattern that causes the neuron in the first layer to activate-in other words, they are patterns that the first layer recognizes.\n",
    "* Second layer in Cnn pick up more complex ideas like circles and stripes. \n",
    "* The Cnns learns to do this on its own.\n",
    "* Third layer picks up complex combination of previous features that were picked up by previous layer.\n",
    "* Last layer picks up most complex features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning\n",
    "* Basic architecture of CNN - Input - CONV - CONV - POOL - CONV - CONV - POOL - CONV -CONV - POOL - DENSE -DENSE - DENSE \n",
    "* Why transfer learning work -- because the intial layer in Cnn detects features like edges and more deeper layer detect stuffs like circle square (shapes) and more deeper layer detect features which complex combination of intial features that is the reason **TRANSFER LEARNING** works!!\n",
    "* Normal procedure for transfer learning is to remove the dense layers which are specific to the particular dataset and add new dense layer and train them(**only the dense layer**)!!\n",
    "* For instance, the technique will work well if our data set is relatively small and very similar to ImageNet.\n",
    "* Transfer Learning was used to develop a CNN to diagnose skin cancer\n",
    "* The CNN classifies lesions as either benign or malignant and ** and achieves better performance than dermatologists for diagnosing some form of skin cancer**\n",
    "* He used a transfer learning approach with the inception architecture pre-trained on ImageNet database.\n",
    "* He removed the final densely connected classification layer and added a new fully connected layer.\n",
    "* This layer had far fewer categories,one for each type of disease class that he wanted to detect.\n",
    "* As of all the other layers in the network \n",
    "* Randomly initialize the weights in the new fully connected layer\n",
    "* Intialize the rest of the weights using pre trained weights \n",
    "* Retrain the entire neural network\n",
    "* SO model got a head start that it was given from pre-training on ImageNet.\n",
    "* This approach with fine tuning the parameters in a network with different final classification layer,**works best if your data set is quite large and very different from ImageNet database**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning -- notes\n",
    "* Transfer learning involves taking pre trained neural network and adapting the neural network to a new, different data set.\n",
    "* Depending to both:\n",
    "* the size of the new data set, and \n",
    "* the similarity of the new data to the original data set\n",
    "* the approach for usinf transfer learning will be different, There are four case:\n",
    "    1. new data set is small, new data is similar to original training data set.\n",
    "    2. new data set is small, new data is different from original training data\n",
    "    3. new data set is large, new data is similar to original training data set.\n",
    "    4. new data set is large, new data is different from original training dataset\n",
    "\n",
    "\n",
    "* A large dataset -- one million image\n",
    "* A small dataset -- two thousand images (matter of perspective)\n",
    "* Overfitting is a concern when using transfer learning with a small dataset.\n",
    "* images of dog and wolves are considered to be similar but image of dog and flower are not consider to be similar.\n",
    "* Each of the four transfer learning cases has its own approach.\n",
    "* So our example network contains three convolution layer and three dense fully connected layers.\n",
    "* Generalized overview of what the convolution neural network does:\n",
    "    * the first layer will detect edges in the image\n",
    "    * the second layer will detect shapes\n",
    "    * the third convolution layer detect higher level featutes \n",
    "* Each transfer learning case will use the pre trained convolution nn in a different way.\n",
    "\n",
    "\n",
    "### Case 1 : **Small Data Set, Similar Data**\n",
    "    * slice off the end the neural network\n",
    "    * add a new fully connected layer that matches the number of classes in the new data set \n",
    "    * randomize the weights of the new fully connected layer; freeze all the weights from pre-trained network.\n",
    "    * train the network to update the weights of the new fully connected layer.\n",
    "* **to avoid overfitting on the small data set, the weights of the original network will be held constant then re-training the weights.**\n",
    "\n",
    "\n",
    "### Case 2: ** Small Data Set, Different Data **\n",
    "* If the new data set is small and different from the original training data:\n",
    "    * slice off most of the pre-trained layers near the beginning of the network \n",
    "    * and to the remaining pre-trained layers a new fully connected layer that matches the number of classes in the new data set.\n",
    "    * randomize the weights of the new fully conected layer; freeze all the weights from the pre-trained network.\n",
    "    * train the network to update the weights of the new fully connected layer.\n",
    "* Because the data set is small, overfitting is still a concern. To combat overfitting, the weights of the original neural network will be held constant, like in the first case,\n",
    "* But the original training set and the new data set do not share higher level features.In this case, the network will only use the layers containing lower level features.\n",
    "\n",
    "\n",
    "### Case 3: ** Large Data Set, Similar Data**\n",
    "* if the new data set is large and similar to the orginal training data:\n",
    "    * remove the last fully conncected layer and replace with a layer matching the number of classes in the new data set\n",
    "    * randomly intialize the weights in the new fully connected layer \n",
    "    * initialize the rest of the weights using the pre-trained weights \n",
    "    * re -train the entire neural network\n",
    "* Overfitting is not as much a concern when training a large data set; we can re-train all the weights\n",
    "* Because the original training set and the new data set share higher level features, the entire neural network is used as well.\n",
    "\n",
    "\n",
    "### Case 4: ** Large Data Set, Differernt Data**\n",
    "* if the new data set is large and different from the orginial training data:\n",
    "    * remove the last fully connected layer and replace with a layer matching the number of the classes in the new data set\n",
    "    * retain the network from the scratch with randomly initilazied weights \n",
    "    * alternatively, you could just use the same strategy as the \"large and similar\" data case\n",
    "\n",
    "* Even though the data set is different from taining data , initializing the weights from the pre-trained network might take training faster. So this case is exactly the same as the case with a large, similar data set.\n",
    "* If using the pre-trained network as a starting point does not produce a successful model, another option is to randomly initialize the convolution neural network weights and train the network from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning in Keras!!\n",
    "* In CNN project -- the goal is to create an algorithm that can predict breed just from dog's picture!!\n",
    "* We will be using the **VGG-16** model pre-trained on the ImageNet data set.\n",
    "* The dog data set is relatively small and it has signficant overlap with a subset of ImageNet categories.\n",
    "* SO this condition is -- similar data & small Data Set so we have to keep in mind not overfit while doing the transfer learning so we will remove last fully connected dense layer and randomize its weight and train it and freeze other weights so it dosent overfit!!\n",
    "* So we will use keras..\n",
    "    ## (224,224,3) --> VGG-16 --> (7,7,512) \n",
    "    ##          (Conv and pooling layer)\n",
    "* then we can save the output of this as a new dataset \n",
    "* and we will make our own fully connected dense layer according to our output calssification problem!!\n",
    "* need to remember this model.add(Flatten(input_shape=(7,7,512)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load and Preprocess Sample Images\n",
    "\n",
    "We will be loading the pre-trained weights from ImageNet of VGG16 architecture and the pre-process step are as following.\n",
    "* We have imported a very small dataset of 8 images and stored the preprocessed image input as img_input. Note that the dimensionality of this array is (8,224,224,3).In this case, each of the 8 images is a 3D tensor, with shape (224,224,3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.preprocessing import image \n",
    "import numpy as np \n",
    "import glob\n",
    "img_paths =glob.glob(\"images/*.jpg\")\n",
    "def path_to_tenser(img_path):\n",
    "    #load RBG image as PIL.Image type \n",
    "    img = image.load_img(img_path,target_size = (224,224))\n",
    "    # convert PIL.image.image type to 3D tensor with shape (224,224,3)\n",
    "    x = image.img_to_array(img)\n",
    "    # convert 3D tensor to 4D tensor with shape (1,224,224,3)\n",
    "    return np.expand_dims(x,axis=0)\n",
    "def paths_to_tensor(img_paths):\n",
    "    list_of_tensor = [path_to_tensor(img_path) for img_path in img_paths]\n",
    "    return np.vstack(list_of_tensor)\n",
    "# calculate the image input. you will learn more about how this works in project!\n",
    "#img_input = preprocess_input(paths_to_tensor(img_paths))\n",
    "#print(img_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from keras.applications.vgg16 import VGG16\n",
    "#model = VGG16()\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow -- Convolution Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# output depth \n",
    "k_output = 64\n",
    "#image dimensions\n",
    "image_width = 10\n",
    "image_height = 10\n",
    "color_channels = 3\n",
    "# convolution filter dimension\n",
    "filter_size_width = 5\n",
    "filter_size_height = 5\n",
    "#input/image\n",
    "input = tf.placeholder(tf.float32,shape=[None,image_height,image_width,color_channels])\n",
    "# weights and bias \n",
    "weight = tf.Variable(tf.truncated_normal([filter_size_height,\n",
    "                                         filter_size_width,color_channels,\n",
    "                                         k_output]))\n",
    "bias = tf.Variable(tf.zeros(k_output))\n",
    "\n",
    "#apply convolution\n",
    "conv_layer = tf.nn.conv2d(input,weight,strides=[1,2,2,1],padding=\"SAME\")\n",
    "## add bias \n",
    "conv_layer = tf.nn.bias_add(conv_layer,bias)\n",
    "# apply activation function \n",
    "conv_layer = tf.nn.relu(conv_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above use the **tf.nn.conv2d()** function ato compute the convolution with weight as the **filters** and [1,2,2,1] for the strides\n",
    "* TensorFlow **uses a stride for each input dimension,** [batch,input_height,input_width,input_channels]\n",
    "* we generally always set the strides for batch and input_channels(i.e. the first and fourth element in the strides array) to be 1. this ensure that the model uses all the batch and input_channels. *(Its good practice to remove the batches or channnels you want to skip from the data set rather use a stride to skip them)*\n",
    "* we will focus on changing input_height and input_width strides are for striding the filter over input.\n",
    "    * IF using strides of two then \n",
    "    * **tf.nn.conv(x,W,strides=[1,2,2,1]**\n",
    "    * better to use a tf.nn.bias_add()\n",
    "    \n",
    "    \n",
    "* **Things to remember are:**\n",
    "    * the shape of weight(filter) is [filter_height,filter_width,depth_of_input,number_feeatures_you_want_to_detect]\n",
    "    * [5,5,3,64] so this a filter for first conv layer whose input is **rbg** data set and we want depth of conv layer to be 64(64 pattern to detect)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Convolution Layers in TensorFlow\n",
    "Let's now build a convoultion layer in TensorFlow. In the below exercis , you'll be asked to set up dimensions of convolution filters, weights, and the biases. This is in many ways the trickiest part using CNNsin TensorFlow. Once you have a sense of how to set up the dimensions of these attributes, applying CNNs will be far more straightforward.\n",
    "\n",
    "## Review \n",
    "You should go over the TensorFlow documentation for 2D convolutions. Most of the documentation is straightforward, except perhaps the padding argument. The padding might differ depending on wheteher you pass \"VALID\" and \"SAME\".\n",
    "\n",
    "Here are a few more things worth reviewing:\n",
    "   1. Introduction of TensorFlow\n",
    "   2. How to **determine the dimensions of the output based on the input size and the filter size!**\n",
    "       * new_height = (input_height - filter_height + 2 x P)/S +1\n",
    "       * new_width = (input_width - filter +2 x P )/S +1\n",
    "       * \"VALID\" **--NO PADDING**\n",
    "       * \"SAME\" **--PADDING**\n",
    "       * out_height = **ceil(float(in_height-filter_height+1)/float(strides[1]))**\n",
    "       * out_width = **ceil(float(in_width - filter_Width+1)/float(strides[2]))**\n",
    "       \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Max Pooling Layers in TensorFlow\n",
    "* Function -- **tf.nn.max_pool()**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_layer = tf.nn.conv2d(input,weight,strides=[1,2,2,1],padding=\"SAME\")\n",
    "conv_layer = tf.nn.bias_add(conv_layer,bias)\n",
    "conv_layer = tf.nn.relu(conv_layer)\n",
    "# apply max pooling layer\n",
    "conv_layer = tf.nn.max_pool(conv_layer,ksize=[1,2,2,1],\n",
    "                           strides = [1,2,2,1],padding=\"SAME\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Function -- **tf.nn.max_pool()**\n",
    "* Arguments\n",
    "    * ksize\n",
    "    * strides\n",
    "    * format -- [batch,height,width,channels]\n",
    "    * for both ksize and strides the batch and channel dimension are typically set to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolution Neural Networks in TensorFlow\n",
    "* Basic architecture of \"ALEXNET\"\n",
    "* Image -> Convolution -> max_pooling -> Convolution -> max_pooling -> fully_connected -> fully_connected -> calssifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\".\",one_hot =True,reshape=False)\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# parameters\n",
    "learning_rate = 0.00001\n",
    "epochs = 10\n",
    "batch_size = 128 \n",
    "\n",
    "# number of samples to calculate validation and accuracy\n",
    "# decrease this if you're running out of memory\n",
    "test_valdi_size = 256\n",
    "\n",
    "# network Parameters\n",
    "n_classes = 10 # MNIST total calsses (0-9 digits)\n",
    "dropout =0.75 # dropout (probability to keep units)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store weights and biases\n",
    "\n",
    "weight = {\n",
    "    \"wc1\":tf.Variable(tf.random_normal([5,5,1,32])),#because its black and weight so the 3rd position is 1\"\n",
    "    \"wc2\":tf.Variable(tf.random_normal([5,5,32,64])),\n",
    "    \"wd1\":tf.Variable(tf.random_normal([7*7*64,1024])),\n",
    "    \"out\": tf.Variable(tf.random_normal([1024,n_classes]))\n",
    "}\n",
    "biases = {\n",
    "    \"bc1\":tf.Variable(tf.random_normal([32])),\n",
    "    \"bc2\":tf.Variable(tf.random_normal([64])),\n",
    "    \"bd1\":tf.Variable(tf.random_normal([1024])),\n",
    "    \"out\":tf.Variable(tf.random_normal([n_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolution Layer\n",
    "* **tf.nn.conv2d()**\n",
    "* **tf.nn.bias_add()**\n",
    "* **tf.nn.relu()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(x,W,b,strides=1):\n",
    "    x = tf.nn.conv2d(x,W,strides = [1,strides,strides,1],padding =\"same\")\n",
    "    x = tf.nn.bias_add(x,b)\n",
    "    return tf.nn.relu(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Max Pooling Layers\n",
    "* **tf.nn.max_pool()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maxpool2d(x,k=2):\n",
    "    return tf.nn.max_pool(x,ksize=[1,k,k,1],strides=[1,k,k,1],\n",
    "                         padding = \"same\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "* The transformation of each layer to new dimensions is shown in the comments, the first layer shapes the images from 28x28x1 to 28x28x32 in the convolution step . The next step applies max pooling, turning each sample into 14x14x32.All layer are applied from conv1 to output, producing 10 class prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_net(x,weights,biases,dropout):\n",
    "    # Layer 1 - 28x28x1 to 14x14x32\n",
    "    conv1 = conv2d(x,weights[\"wc1\"],biases[\"bc1\"])\n",
    "    conv1 = maxpool2d(conv2,k=2)\n",
    "    \n",
    "    # Layer 2 - 14x14x32 to 7x7x64\n",
    "    conv2 = conv2d(conv1,weights[\"wc2\"],baises[\"bc2\"])\n",
    "    conv2 = maxpool2d(conv2,k=2)\n",
    "    \n",
    "    # Fully Connected layer - 7*7*64 to 1024\n",
    "    fc1 = tf.reshape(conv2,[-1,weights[\"wd1\"].get_shape().as_list()[0]])\n",
    "    fc1 = tf.add(tf.matmul(fc1,weights[\"wd1\"]),biases[\"bd1\"])\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "    fc1 = tf.nn.dropout(fc1,dropout)\n",
    "    \n",
    "    # Output Layer - class prediction - 1024 to 10\n",
    "    out - tf.add(tf.matmul(fc1,weights[\"out\"]),biases[\"out\"])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf Graph input \n",
    "x = tf.placeholder(tf.float32,[None,28,28,1])\n",
    "y = tf.placeholder(tf.float32,[None,n_classes])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "#Model \n",
    "logits = conv_net(x,weights,biases,keep_prob)\n",
    "\n",
    "# Define Loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits,\n",
    "                                                             labels=y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits,1),tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred,tf.float32))\n",
    "\n",
    "# Initializing the variables \n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "## Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for batch in range(mnist.train.num_examples//batch_size):\n",
    "            batch_x,batch_y = mnist.train.next_batch(batch_size)\n",
    "            sess.run(optimizer,feed_dict = {x:batch_x,\n",
    "                                           y:batch_y,\n",
    "                                           keep_prob:dropout})\n",
    "            # Calculate batch loss and accuracy\n",
    "            loss = sess.run(cost,feed_dict = {x:batch_x,\n",
    "                                             y:batch_y,\n",
    "                                             keep_porb:1.0})\n",
    "            valid_acc = sess.run(accuracy, feed_dict = { \n",
    "            x:mnist.validation.images[:test_valid_size],\n",
    "            y:mnist.validation.labels[:test_valid_size],\n",
    "            keep_prob = 1.0})\n",
    "            \n",
    "            print(\"Epoch {:>2}, Batch {:>3} -\"\n",
    "                 \"loss: {:10.4f} Validation Accuracy: {:.6f}\".format(\n",
    "                 epoch+1,\n",
    "                 batch+1,\n",
    "                 loss,\n",
    "                 valid_acc))\n",
    "            \n",
    "            #Calculate Test Accuracy \n",
    "            test_acc = sess.run(accuracy,feed_dict={\n",
    "                x:mnist.test.images[:test_valid_size],\n",
    "                y:mnist.test.labels[:test_valid_size],\n",
    "                keep_prob =1.0\n",
    "            })\n",
    "            print(\"Testing Accuracy: {}\".format(test_acc))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weight Initializing Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general rule for setting the weights in a neural network is to be  close to zero without being too small. A good practice is to start your weight in the range [-y,y] where y = 1/sqrt(n) (n is the number of input to a given neuron).\n",
    "* **tf.random_normal()**\n",
    "* **tf.random_uniform()**\n",
    "* so truncated normal distribution is when we put thershold limit to our distribution beyond that limit no random number should be generated.\n",
    "* **tf.truncated_normal(shape,mean=0,std=1.0)**\n",
    "* So this function generate normal distributed values and drop any |values| which are greater than 2.(by default)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These models are used to compress data, as well as image denoising, which we'll be implementing in this lesson. The idea here is we'll build a network that tries to generate it's input data,but with a narrow hidden layer that serves as a compressed representation of the input data.\n",
    "* This is a type of network architecture to perform **data compression**, where the compression and decompression **functions are learned from the data itself**, not hand-engineered by humans.\n",
    "* So **encoder** and **decoder** are build with data itself.\n",
    "* The whole network is trained by **minimizing** the difference between the **input and output**.\n",
    "* STRUCTURE OF AUTOENCODER\n",
    "* **\n",
    "* **[10] -> [5] -> [2][BOTTEL NECK HIDDEN LAYER) -> (5) -> (10)**\n",
    "* **\n",
    "* Bottel neck hidden layer is the compressed data form which we can create our original data.\n",
    "* There will be some loss of information!!(But very less)\n",
    "* So autoencoder are **not good(WORST)** in :\n",
    "    1. **COMPRESSION**(compared to JEPGs,MP3s and MPEgs)\n",
    "    2. **GENERALIZING TO DATASETS**(other than what they were trained on)\n",
    "* Application of AutoEncoder:\n",
    "    1. **IMAGE DENOISING**\n",
    "    2. **DIMENSIONALITY REDUCTION**\n",
    "* so now we will compress a image(using autoencoder) then sice its a image data we'll improve it by using convolutional layers    \n",
    "* We will learn how to code autoencoder on tensorflow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoEncoder with tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Importing data \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data\",validation_size=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = mnist.train.images[2]\n",
    "plt.imshow(img.reshape((28,28)),cmap =\"gray\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist.train.images[0].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden = 392\n",
    "n_hidden_2 = 32\n",
    "weight = {\n",
    "    \"w1\":tf.Variable(tf.truncated_normal([mnist.train.images.shape[1],n_hidden])),\n",
    "    \"w2\":tf.Variable(tf.truncated_normal([n_hidden,n_hidden_2])),\n",
    "    \"w3\":tf.Variable(tf.truncated_normal([n_hidden_2,n_hidden])),\n",
    "    \"w4\":tf.Variable(tf.truncated_normal([n_hidden,mnist.train.images.shape[1]]))\n",
    "}\n",
    "biases = {\n",
    "    \"b1\":tf.Variable(tf.zeros([n_hidden])),\n",
    "    \"b2\":tf.Variable(tf.zeros([n_hidden_2])),\n",
    "    \"b3\":tf.Variable(tf.zeros([n_hidden])),\n",
    "    \"b4\":tf.Variable(tf.zeros([mnist.train.images.shape[1]]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_path(x,weight,biases):\n",
    "    pass_1 = tf.add(tf.matmul(x,weight[\"w1\"]),biases[\"b1\"])\n",
    "    pass_1 = tf.nn.relu(pass_1)\n",
    "    pass_2 = tf.add(tf.matmul(pass_1,weight[\"w2\"]),biases[\"b2\"])\n",
    "    pass_2 = tf.nn.relu(pass_2)\n",
    "    encoded = pass_2\n",
    "    pass_3 = tf.add(tf.matmul(pass_2,weight[\"w3\"]),biases[\"b3\"])\n",
    "    pass_3 = tf.nn.relu(pass_3)\n",
    "    pass_4 = tf.add(tf.matmul(pass_1,weight[\"w4\"]),biases[\"b4\"])\n",
    "    return pass_4,encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating placeholders\n",
    "batch_size = 128\n",
    "epochs = 30\n",
    "n_classes = mnist.train.images.shape[1]\n",
    "x =tf.placeholder(tf.float32,[None,n_classes])\n",
    "y = tf.placeholder(tf.float32,[None,n_classes])\n",
    "\n",
    "logits,encoded = forward_path(x,weight,biases)\n",
    "\n",
    "cost = tf.nn.sigmoid_cross_entropy_with_logits(labels=y,logits=logits)\n",
    "cost = tf.reduce_mean(cost)\n",
    "decoded = tf.nn.sigmoid(logits)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cost)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "sess.run(init)\n",
    "for epoch in range(epochs):\n",
    "    for batch in range(mnist.train.num_examples//batch_size):\n",
    "        batch_x = mnist.train.next_batch(batch_size)\n",
    "        _,loss=sess.run([optimizer,cost],feed_dict = {x:batch_x[0],y:batch_x[0]})\n",
    "            #loss = sess.run(cost,feed_dict = {x:batch_x[0]})\n",
    "            \n",
    "            \n",
    "    print(\"Epoch {:>2}, Batch {:>3} -\"\n",
    "                \"loss: {:10.4f}\".format(\n",
    "                 epoch+1,\n",
    "                 batch+1,\n",
    "                 loss))\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, axes = plt.subplots(nrows=2, ncols=10, sharex=True, sharey=True, figsize=(20,4))\n",
    "in_imgs = mnist.test.images[:10]\n",
    "reconstructed, compressed = sess.run([decoded, encoded], feed_dict={x: in_imgs})\n",
    "\n",
    "for images, row in zip([in_imgs, reconstructed], axes):\n",
    "    for img, ax in zip(images, row):\n",
    "        ax.imshow(img.reshape((28, 28)), cmap='Greys_r')\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "\n",
    "fig.tight_layout(pad=0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
